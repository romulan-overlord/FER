{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9418102,"sourceType":"datasetVersion","datasetId":5714598}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe==0.10.14 --no-deps  # Skip conflicting dependencies\n!pip install libreface --no-deps","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:19.749967Z","iopub.execute_input":"2025-02-11T08:25:19.750306Z","iopub.status.idle":"2025-02-11T08:25:24.493159Z","shell.execute_reply.started":"2025-02-11T08:25:19.750281Z","shell.execute_reply":"2025-02-11T08:25:24.492048Z"}},"outputs":[{"name":"stdout","text":"Collecting mediapipe==0.10.14\n  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\nDownloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: mediapipe\nSuccessfully installed mediapipe-0.10.14\nCollecting libreface\n  Downloading libreface-0.1.0-py3-none-any.whl.metadata (11 kB)\nDownloading libreface-0.1.0-py3-none-any.whl (33 kB)\nInstalling collected packages: libreface\nSuccessfully installed libreface-0.1.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# !git clone https://github.com/ihp-lab/LibreFace.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:24:28.626699Z","iopub.execute_input":"2025-02-06T05:24:28.627070Z","iopub.status.idle":"2025-02-06T05:24:34.687691Z","shell.execute_reply.started":"2025-02-06T05:24:28.627038Z","shell.execute_reply":"2025-02-06T05:24:34.686456Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LibreFace'...\nremote: Enumerating objects: 634, done.\u001b[K\nremote: Counting objects: 100% (441/441), done.\u001b[K\nremote: Compressing objects: 100% (237/237), done.\u001b[K\nremote: Total 634 (delta 322), reused 308 (delta 201), pack-reused 193 (from 1)\u001b[K\nReceiving objects: 100% (634/634), 123.93 MiB | 36.49 MiB/s, done.\nResolving deltas: 100% (377/377), done.\nUpdating files: 100% (105/105), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport os\nfrom tqdm import tqdm\nimport warnings\nimport numpy as np\nimport scipy.ndimage\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nwarnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n\nimport sys\nsys.path.insert(0, '/kaggle/working/LibreFace')\n\nfrom libreface.detect_mediapipe_image import *\nfrom libreface.AU_Detection.inference import detect_action_units, detect_action_units_video\nfrom libreface.AU_Recognition.inference import *\nfrom libreface.Facial_Expression_Recognition.inference import get_facial_expression, get_facial_expression_video\nfrom libreface.utils import get_frames_from_video_ffmpeg, uniquify_file, check_file_type\nfrom libreface.AU_Recognition.solver_inference_image import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:27.256533Z","iopub.execute_input":"2025-02-11T08:25:27.256872Z","iopub.status.idle":"2025-02-11T08:25:30.353110Z","shell.execute_reply.started":"2025-02-11T08:25:27.256842Z","shell.execute_reply":"2025-02-11T08:25:30.352414Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Preprocessor with MediaPipe","metadata":{}},{"cell_type":"code","source":"def image_align(img, face_landmarks, output_size=256,\n        transform_size=512, enable_padding=True, x_scale=1,\n        y_scale=1, em_scale=0.1, alpha=False, pad_mode='const'):\n\n  lm = np.array(face_landmarks)\n  lm[:,0] *= img.size[0]\n  lm[:,1] *= img.size[1]\n\n  lm_eye_right      = lm[0:16]  \n  lm_eye_left     = lm[16:32]  \n  lm_mouth_outer   = lm[32:]  \n  # lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n  lm_mouth_outer_x = lm_mouth_outer[:,0].tolist()\n  left_index = lm_mouth_outer_x.index(min(lm_mouth_outer_x))\n  right_index = lm_mouth_outer_x.index(max(lm_mouth_outer_x))\n  # print(left_index,right_index)\n  # Calculate auxiliary vectors.\n  eye_left     = np.mean(lm_eye_left, axis=0)\n  # eye_left[[0,1]] = eye_left[[1,0]]\n  eye_right    = np.mean(lm_eye_right, axis=0)\n  # eye_right[[0,1]] = eye_right[[1,0]]\n  eye_avg      = (eye_left + eye_right) * 0.5\n  eye_to_eye   = eye_right - eye_left\n  # print(lm_mouth_outer)s\n  mouth_avg    = (lm_mouth_outer[left_index,:] + lm_mouth_outer[right_index,:])/2.0\n  # mouth_avg[[0,1]] = mouth_avg[[1,0]]\n  \n  eye_to_mouth = mouth_avg - eye_avg\n  # Choose oriented crop rectangle.\n  x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n  x /= np.hypot(*x)\n  x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n  x *= x_scale\n  y = np.flipud(x) * [-y_scale, y_scale]\n  c = eye_avg + eye_to_mouth * em_scale\n  quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n  qsize = np.hypot(*x) * 2\n\n  # Shrink.\n  shrink = int(np.floor(qsize / output_size * 0.5))\n  if shrink > 1:\n    rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n    # img = img.resize(rsize, Image.ANTIALIAS)\n    img = img.resize(rsize, Image.LANCZOS)  # ✅ Correct\n    quad /= shrink\n    qsize /= shrink\n\n  # Crop.\n  border = max(int(np.rint(qsize * 0.1)), 3)\n  crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n  crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n  if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n    img = img.crop(crop)\n    quad -= crop[0:2]\n\n  # Pad.\n  pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n  pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n  if enable_padding and max(pad) > border - 4:\n    pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n    if pad_mode == 'const':\n      img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'constant', constant_values=0)\n    else:\n      img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n    h, w, _ = img.shape\n    y, x, _ = np.ogrid[:h, :w, :1]\n    mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n    blur = qsize * 0.02\n    img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n    img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n    img = np.uint8(np.clip(np.rint(img), 0, 255))\n    if alpha:\n      mask = 1-np.clip(3.0 * mask, 0.0, 1.0)\n      mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))\n      img = np.concatenate((img, mask), axis=2)\n      img = Image.fromarray(img, 'RGBA')\n    else:\n      img = Image.fromarray(img, 'RGB')\n    quad += pad[:2]\n\n  img = img.transform((transform_size, transform_size), Image.Transform.QUAD,\n            (quad + 0.5).flatten(), Image.Resampling.BILINEAR)\n\n  out_image = img.resize((output_size, output_size), Image.Resampling.LANCZOS)\n  out_image = img\n\n  return out_image\n\ndef image_align_fallback(img, output_size=256, transform_size=512, enable_padding=True, pad_mode='const'):\n    \"\"\"\n    Performs alignment transformation with padding and edge blurring when landmarks are not available.\n    \"\"\"\n    # Convert to NumPy array if not already\n    img = np.array(img)\n    \n    # Compute padding values\n    border = int(output_size * 0.1)\n    pad = [border, border, border, border]  # (top, bottom, left, right)\n    \n    # Apply padding\n    if enable_padding:\n        if pad_mode == 'const':\n            img = np.pad(img, ((pad[0], pad[1]), (pad[2], pad[3]), (0, 0)), 'constant', constant_values=0)\n        else:\n            img = np.pad(img, ((pad[0], pad[1]), (pad[2], pad[3]), (0, 0)), 'reflect')\n    \n    # Apply edge blur to blend padding\n    h, w, _ = img.shape\n    y, x, _ = np.ogrid[:h, :w, :1]\n    mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[2], np.float32(w-1-x) / pad[3]),\n                      1.0 - np.minimum(np.float32(y) / pad[0], np.float32(h-1-y) / pad[1]))\n    blur = output_size * 0.02\n    img = img.astype(np.float32)\n    img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n    # img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n    img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n    img = np.uint8(np.clip(np.rint(img), 0, 255))\n    \n    # Convert back to PIL Image and resize\n    img = Image.fromarray(img, 'RGB')\n    img = img.resize((transform_size, transform_size), Image.LANCZOS)\n    \n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:33.442659Z","iopub.execute_input":"2025-02-11T08:25:33.443123Z","iopub.status.idle":"2025-02-11T08:25:33.471772Z","shell.execute_reply.started":"2025-02-11T08:25:33.443081Z","shell.execute_reply":"2025-02-11T08:25:33.470667Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_aligned_image(image_path, temp_dir = \"./tmp\", verbose=False):\n  os.makedirs(temp_dir, exist_ok=True)\n  image_name = \".\".join(image_path.split(\"/\")[-1].split(\".\")[:-1])\n  img_path = image_path\n  land_save_path = uniquify_file(os.path.join(temp_dir, f\"{image_name}_landmark.npy\"))\n  aligned_img_save_path = uniquify_file(os.path.join(temp_dir, f\"{image_name}_aligned.png\"))\n  annotated_image_save_path = uniquify_file(os.path.join(temp_dir, f\"{image_name}_annotated.png\"))\n\n  image = cv2.imread(img_path)\n  mp_face_mesh = mp.solutions.face_mesh\n\n  FACEMESH_LIPS = [(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n                            (17, 314), (314, 405), (405, 321), (321, 375),\n                            (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n                            (37, 0), (0, 267), (267, 269), (269, 270), (270, 409), (409, 291),\n                            (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n                            (14, 317), (317, 402), (402, 318), (318, 324),\n                            (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n                            (82, 13), (13, 312), (312, 311), (311, 310),\n                            (310, 415), (415, 308)]\n    \n  FACEMESH_LEFT_EYE = [(263, 249), (249, 390), (390, 373), (373, 374),\n                                (374, 380), (380, 381), (381, 382), (382, 362),\n                                (263, 466), (466, 388), (388, 387), (387, 386),\n                                (386, 385), (385, 384), (384, 398), (398, 362)]\n\n  FACEMESH_RIGHT_EYE = [(33, 7), (7, 163), (163, 144), (144, 145),\n                                  (145, 153), (153, 154), (154, 155), (155, 133),\n                                  (33, 246), (246, 161), (161, 160), (160, 159),\n                                  (159, 158), (158, 157), (157, 173), (173, 133)]\n  Left_eye = []\n  Right_eye = []\n  Lips = []\n  for (x,y) in FACEMESH_LEFT_EYE:\n    if x not in Left_eye:\n      Left_eye.append(x)\n    if y not in Left_eye:\n      Left_eye.append(y)\n\n  for (x,y) in FACEMESH_RIGHT_EYE:\n    if x not in Right_eye:\n      Right_eye.append(x)\n    if y not in Right_eye:\n      Right_eye.append(y)\n\n  for (x,y) in FACEMESH_LIPS:\n    if x not in Lips:\n      Lips.append(x)\n    if y not in Lips:\n      Lips.append(y)\n\n  landmark_dict = {}\n\n  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n  with mp_face_mesh.FaceMesh(\n      static_image_mode=True,\n      refine_landmarks=True,\n      max_num_faces=2,\n      min_detection_confidence=0.5) as face_mesh:\n      results = face_mesh.process(image_rgb)\n      if results == None:\n        print(\"Processing face mesh had some issue...\")\n      if not results.multi_face_landmarks:\n        # print(\"Processing landmarks did not result on anything...\")\n        # temp = pad_image(image_rgb)\n        temp = image_align_fallback(image_rgb)\n        return temp\n      img_h, img_w, img_c = image.shape\n      face_3d = []\n      face_2d = []\n      \n      for face_landmarks in results.multi_face_landmarks:\n        \n        landmark_dict = restructure_landmark_mediapipe(face_landmarks.landmark)\n\n        for idx, lm in enumerate(face_landmarks.landmark):\n          if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n\n              x, y = int(lm.x * img_w), int(lm.y * img_h)\n\n              # Get the 2D Coordinates\n              face_2d.append([x, y])\n\n              # Get the 3D Coordinates\n              face_3d.append([x, y, lm.z])       \n          \n        # Convert it to the NumPy array\n        face_2d = np.array(face_2d, dtype=np.float64)\n\n        # Convert it to the NumPy array\n        face_3d = np.array(face_3d, dtype=np.float64)\n\n        # The camera matrix\n        focal_length = 1 * img_w\n\n        cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n                                [0, focal_length, img_w / 2],\n                                [0, 0, 1]])\n\n        # The distortion parameters\n        dist_matrix = np.zeros((4, 1), dtype=np.float64)\n\n        # Solve PnP\n        success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n\n        # Get rotational matrix\n        rmat, jac = cv2.Rodrigues(rot_vec)\n\n        # Get angles\n        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n\n        # Get the y rotation degree\n        x = angles[0] * 360\n        y = angles[1] * 360\n        z = angles[2] * 360\n\n        pitch = x\n        yaw = y\n        roll = z\n\n        head_pose = f\"pitch:{pitch:.1f}, yaw:{yaw:.1f}, roll:{roll:.1f}\"\n\n        lm_left_eye_x = []\n        lm_left_eye_y = []\n        lm_right_eye_x = []\n        lm_right_eye_y = []\n        lm_lips_x = []\n        lm_lips_y = []\n        for i in Left_eye:\n          lm_left_eye_x.append(face_landmarks.landmark[i].x)\n          lm_left_eye_y.append(face_landmarks.landmark[i].y)\n        for i in Right_eye:\n          lm_right_eye_x.append(face_landmarks.landmark[i].x)\n          lm_right_eye_y.append(face_landmarks.landmark[i].y)\n        for i in Lips:\n          lm_lips_x.append(face_landmarks.landmark[i].x)\n          lm_lips_y.append(face_landmarks.landmark[i].y)\n        lm_x = lm_left_eye_x + lm_right_eye_x + lm_lips_x\n        lm_y = lm_left_eye_y + lm_right_eye_y + lm_lips_y\n        landmark = np.array([lm_x,lm_y]).T\n          \n  aligned_image = image_align(Image.fromarray(image_rgb), landmark)\n  # aligned_image.save(aligned_img_save_path)\n\n  head_pose = {\"pitch\":pitch,\n               \"yaw\":yaw,\n               \"roll\":roll}\n  \n  return aligned_image#, head_pose, landmark_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:36.640661Z","iopub.execute_input":"2025-02-11T08:25:36.641044Z","iopub.status.idle":"2025-02-11T08:25:36.658380Z","shell.execute_reply.started":"2025-02-11T08:25:36.641011Z","shell.execute_reply":"2025-02-11T08:25:36.657406Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Facial Action Unit Extractor","metadata":{}},{"cell_type":"code","source":"class solver_inference_image(nn.Module):\n    def __init__(self, config):\n        super(solver_inference_image, self).__init__()\n        self.config = config\n\n        # Setup number of labels\n        self.config.num_labels = 12\n        self.num_labels = self.config.num_labels\n\n        self.image_transform = image_test(img_size=config.image_size, crop_size=config.crop_size)\n        self.device = config.device\n\n        # Initiate the networks\n        if config.model_name == \"resnet\":\n            self.model = ResNet18(config).to(self.device)\n        elif config.model_name == \"emotionnet_mae\":\n            self.model = MaskedAutoEncoder(config).to(self.device)\n        else:\n            raise NotImplementedError\n\n        if self.config.half_precision:\n            print(\"Use Half Precision.\")\n        \n        # Setup AU index\n        self.aus = [1,2,4,5,6,9,12,15,17,20,25,26]\n\n    def pil_loader(self, path):\n        with open(path, 'rb') as f:\n            with Image.open(f) as img:\n                return img.convert('RGB')\n\n    def transform_image_inference(self, image):\n        # image = self.pil_loader(aligned_image_path)\n        image = self.image_transform(image)\n        return image\n\n    def image_inference(self, transformed_images):\n        with torch.no_grad():\n            self.eval()\n            input_images = transformed_images.to(self.device)\n            if self.config.half_precision:\n                input_images = input_images.half()\n                self.model = self.model.half()\n            labels_pred = self.model(input_images)\n            if self.config.half_precision:\n                labels_pred = labels_pred.float()\n            labels_pred = torch.clamp(labels_pred * 5.0, min=0.0, max=5.0)\n            return labels_pred\n\n    def load_best_ckpt(self):\n        download_weights(self.config.weights_download_id, self.config.ckpt_path)\n        checkpoints = torch.load(self.config.ckpt_path, map_location=self.device, weights_only=True)['model']\n        self.model.load_state_dict(checkpoints, strict=True)\n\n    def run(self, aligned_images):\n        if \"cuda\" in self.device:\n            torch.backends.cudnn.benchmark = True\n\n        self.load_best_ckpt()\n        \n        if isinstance(aligned_images, str):  # If a single image is passed, process normally\n            transformed_image = self.transform_image_inference(aligned_images)\n            pred_labels = self.image_inference(transformed_image.unsqueeze(0))\n            return dict(zip(self.aus, pred_labels.squeeze().tolist()))\n        \n        # If multiple images are passed, process in batch\n        transformed_images = torch.stack([self.transform_image_inference(img) for img in aligned_images])\n        pred_labels = self.image_inference(transformed_images)\n        \n        return [dict(zip(self.aus, pred.tolist())) for pred in pred_labels]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:41.152404Z","iopub.execute_input":"2025-02-11T08:25:41.152701Z","iopub.status.idle":"2025-02-11T08:25:41.162548Z","shell.execute_reply.started":"2025-02-11T08:25:41.152679Z","shell.execute_reply":"2025-02-11T08:25:41.161774Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def get_au_intensities(images, device=\"cpu\",\n                        weights_download_dir = \"./weights_libreface\"):\n    opts = ConfigObject({'seed': 0,\n                        'data_root': '',\n                        'ckpt_path': f'{weights_download_dir}/AU_Recognition/weights/resnet.pt',\n                        'weights_download_id': '14qEnWRew2snhdMdOVyqKFJ5rq5VZrfAX',\n                        'data': 'DISFA',\n                        'fold': 'all',\n                        'image_size': 256,\n                        'crop_size': 224,\n                        'num_labels': 12,\n                        'sigma': 10.0,\n                        'model_name': 'resnet',\n                        'dropout': 0.1,\n                        'hidden_dim': 128,\n                        'half_precision': False,\n                        'num_epochs': 30,\n                        'interval': 500,\n                        'threshold': 0,\n                        'batch_size': 256,\n                        'learning_rate': '3e-5',\n                        'weight_decay': '1e-4',\n                        'loss': 'unweighted',\n                        'clip': 1.0,\n                        'when': 10,\n                        'patience': 5,\n                        'fm_distillation': False,\n                        'device': 'cpu'})\n\n    #set seed\n    set_seed(opts.seed)\n    opts.device = device\n    solver = solver_inference_image(opts).to(device)\n\n    au_intensities = solver.run(images)\n    return au_intensities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:25:43.479500Z","iopub.execute_input":"2025-02-11T08:25:43.479929Z","iopub.status.idle":"2025-02-11T08:25:43.486416Z","shell.execute_reply.started":"2025-02-11T08:25:43.479884Z","shell.execute_reply":"2025-02-11T08:25:43.485331Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Setting up mapping for textual action units","metadata":{}},{"cell_type":"code","source":"au_map = {\n    1: 'Inner brow raiser',\n    2: 'Outer brow raiser',\n    4: 'Brow lowerer',\n    5: 'Upper lid raiser',\n    6: 'Cheek raiser',\n    7: 'Lid tightener',\n    9: 'Nose wrinkler',\n    10: 'Upper lip raiser',\n    12: 'Lip corner puller',\n    14: 'Dimpler',\n    15: 'Lip corner depressor',\n    17: 'Chin raiser',\n    20: 'Lip stretcher',\n    23: 'Lip tightener',\n    24: 'Lip pressor',\n    25: 'Lips part',\n    26: 'Jaw drop'\n}\n\ndef generate_prompt(intensities):\n    prompt = 'Question: Please play the role of a facial action describer. Objectively describe the detailed facial actions of the person in the image while considering the specified Facial Action Units. A higher value indicates the action is performed with high intensity. The positive AUs are: '\n    for key in intensities.keys():\n        prompt += f'{au_map[key]}: {intensities[key]}, '\n    prompt += \". The possible expression classes are: Happiness, Sadness, Anger, Surprise, Disgust, Neutral and Fear. Ensure that your description is expression focused. Answer:\"\n    return prompt\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:50:30.662504Z","iopub.execute_input":"2025-02-11T08:50:30.662839Z","iopub.status.idle":"2025-02-11T08:50:30.668194Z","shell.execute_reply.started":"2025-02-11T08:50:30.662814Z","shell.execute_reply":"2025-02-11T08:50:30.667177Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"prompt = ['Surprise',\n'Fear',\n'Disgust',\n'Happiness',\n'Sadness',\n'Anger',\n'Neutral']","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Blip2 for AU analysis","metadata":{}},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport torch\nfrom PIL import Image\n\n# Load model and processor from Hugging Face\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\",\n    torch_dtype=torch.float16,  # Efficient memory usage\n    device_map=\"auto\"  # Automatically selects GPU if available\n)\n# Ensure the model runs on CUDA if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:19:51.955855Z","iopub.execute_input":"2025-02-11T08:19:51.956175Z","iopub.status.idle":"2025-02-11T08:22:31.522855Z","shell.execute_reply.started":"2025-02-11T08:19:51.956150Z","shell.execute_reply":"2025-02-11T08:22:31.522038Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e4271a9cc14e25a08822175e2e6ecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bafc04cfc24baeae95e339d729fc24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ddf32127804c2caf20b17769c802e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb3d932aab1481aadd1bb861b96e355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da32e1803124307900d6f8063d2f632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6a938de39e4b999614604f7d6b645a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56154cd8232471f81c0f764dffbdb5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa27caa063174605b85d8281c7854140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11662dec5071410095a3d675f68ad0cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38a19a9f261488fbb73fece15cd1a96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e1de14ba8c475984571badf079d38a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb3f8efc69b481eb38fd7d539809236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ce68999a7a4f88b2a290b8ba1728f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d157a5e0d8cd41089cafe0906d1092c8"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"Blip2ForConditionalGeneration(\n  (vision_model): Blip2VisionModel(\n    (embeddings): Blip2VisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n    )\n    (encoder): Blip2Encoder(\n      (layers): ModuleList(\n        (0-38): 39 x Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n  )\n  (qformer): Blip2QFormerModel(\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (encoder): Blip2QFormerEncoder(\n      (layer): ModuleList(\n        (0): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (language_projection): Linear(in_features=768, out_features=2048, bias=True)\n  (language_model): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 2048)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 2048)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n                (relative_attention_bias): Embedding(32, 32)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 2048)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n                (relative_attention_bias): Embedding(32, 32)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Load an image\nimage_path = \"/kaggle/input/rafdb-dg/aligned/aligned/test_0007_aligned.jpg\"  # Change this to your image path\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Define your prompt\n# prompt = \"Question: Describe the facial expression in detail. Answer:\"\nprompt = generate_prompt(get_au_intensities([image,])[0])\n\n\n# Process the image and prompt\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n\n# Generate a textual description\n# output = model.generate(**inputs)\noutput = model.generate(\n    **inputs,\n    max_length=100,  # Increase max tokens\n    min_length=30,  # Ensure a longer output\n    do_sample=True,  # Enables stochastic generation\n    temperature=1.0,  # Higher values → more diverse responses\n    top_p=0.95,  # Nucleus sampling for diversity\n    num_beams=5,  # Use beam search for better results\n    # repetition_penalty=1.2  # Reduce repeated phrases\n)\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\n# Print the result\nprint(\"Generated Description:\", generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:51:12.745233Z","iopub.execute_input":"2025-02-11T08:51:12.745554Z","iopub.status.idle":"2025-02-11T08:51:15.104335Z","shell.execute_reply.started":"2025-02-11T08:51:12.745532Z","shell.execute_reply":"2025-02-11T08:51:15.103594Z"}},"outputs":[{"name":"stdout","text":"Generated Description: a man with a beard and a mustache is looking at the camera with a smirk on his face and a smile on his face\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:43:29.174057Z","iopub.execute_input":"2025-02-11T08:43:29.174388Z","iopub.status.idle":"2025-02-11T08:43:29.184425Z","shell.execute_reply.started":"2025-02-11T08:43:29.174362Z","shell.execute_reply":"2025-02-11T08:43:29.183717Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([[    0,     3,     9,   385,  3202,    19,  2213,    53,    44,     8,\n          1861,    28,   160,  2053,  1148,   539,    11,   160,  1268,  6756,\n           223,    16,     3,     9, 26190,  5756,     1]], device='cuda:0')"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"get_au_intensities([image,])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T08:27:19.556575Z","iopub.execute_input":"2025-02-11T08:27:19.556895Z","iopub.status.idle":"2025-02-11T08:27:19.824801Z","shell.execute_reply.started":"2025-02-11T08:27:19.556871Z","shell.execute_reply":"2025-02-11T08:27:19.823861Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{1: 2.3929967880249023,\n  2: 2.1396665573120117,\n  4: 0.7750539779663086,\n  5: 0.12001913040876389,\n  6: 0.4018769860267639,\n  9: 0.15539467334747314,\n  12: 0.30806055665016174,\n  15: 0.9314776062965393,\n  17: 1.7802917957305908,\n  20: 0.40010425448417664,\n  25: 0.14131002128124237,\n  26: 0.11967062950134277}]"},"metadata":{}}],"execution_count":17}]}