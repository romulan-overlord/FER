{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9418102,"sourceType":"datasetVersion","datasetId":5714598},{"sourceId":9468042,"sourceType":"datasetVersion","datasetId":5757223}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riturajpradhan/resnet18-on-rafdb?scriptVersionId=198632204\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image, ImageEnhance, ImageOps, ImageDraw\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\nimport pandas as pd\nimport time\nimport copy\nfrom collections import Counter\nimport ast\nimport random\nimport argparse\nimport math\nimport random\n\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.init as init\n\nfrom transformers import SwinForImageClassification, SwinConfig\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n\nfrom sklearn.model_selection import KFold\n\nfrom timm.scheduler.scheduler import Scheduler\nfrom timm.loss import LabelSmoothingCrossEntropy\nimport timm\n\nfrom torchvision import transforms\nimport torchvision\n\nimport cv2\n# import insightface\n# from insightface.app import FaceAnalysis\n# from insightface.data import get_image as ins_get_image\nimport matplotlib.pyplot as plt\n\nimport sys\nsys.path.insert(0, '/kaggle/working/CFERNet')\nfrom auto_augment import AutoAugment","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:26:34.767188Z","iopub.execute_input":"2024-09-28T10:26:34.767657Z","iopub.status.idle":"2024-09-28T10:27:01.747883Z","shell.execute_reply.started":"2024-09-28T10:26:34.767608Z","shell.execute_reply":"2024-09-28T10:27:01.746699Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Trying to import resnet18 model","metadata":{}},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, end2end=True):\n        self.inplanes = 64\n        self.end2end = end2end\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        \n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512,num_classes)#2048 fr 50\n        #self.classifier =  nn.Linear(512,num_classes)#2048 fr 50\n        \n        #self.threedmm_layer = threeDMM_model(alfa,threed_model_data)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n       \n        bs = x.size(0)\n        f = x\n\n        f = self.conv1(f)\n        f = self.bn1(f)\n        f = self.relu(f)\n        f = self.maxpool(f)\n        \n        f = self.layer1(f)\n        #print('layer1: ',f.size())\n        f = self.layer2(f)\n        #print('layer2: ',f.size())\n       \n        f = self.layer3(f)\n        feature = f.view(bs, -1)\n        #print('layer4: ',f.size())\n        f = self.layer4(f)\n        #print('layer4: ',f.size())\n        f = self.avgpool(f)\n        \n        f = f.squeeze(3).squeeze(2)\n        pred = self.fc(f)\n        #pred = self.classifier(f)\n        #print('fc: ',f.size())\n        #return f#pred\n        return  pred\n\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n        print('Loading pretrained imagenet model')\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:27:18.510839Z","iopub.execute_input":"2024-09-28T10:27:18.511269Z","iopub.status.idle":"2024-09-28T10:27:18.54133Z","shell.execute_reply.started":"2024-09-28T10:27:18.511228Z","shell.execute_reply":"2024-09-28T10:27:18.539793Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"landmarks_cache = {}\n\n# Open the file and read all lines first to calculate total length for progress bar\nwith open('/kaggle/input/ferplus/FERPlus_Train_kps.txt', 'r') as file:\n    lines = file.readlines()\n\n# Iterate over lines with progress bar\nfor line in tqdm(lines, desc=\"Processing landmarks\", unit=\"lines\"):\n    # Split at the first occurrence of the colon\n    file_name, coordinates = line.split(\":\", 1)\n\n    # Remove whitespace around the file name\n    file_name = file_name.strip()\n\n    # Convert the string representation of the list of tuples to an actual list of tuples\n    coordinates = ast.literal_eval(coordinates.strip())\n\n    # Add to the dictionary\n    landmarks_cache[file_name] = coordinates","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:27:22.829273Z","iopub.execute_input":"2024-09-28T10:27:22.829757Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing landmarks:   0%|          | 0/30996 [00:00<?, ?lines/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc1efbc4a80748519a63de026a45ecab"}},"metadata":{}}]},{"cell_type":"markdown","source":"# RafDB DATA Loader","metadata":{}},{"cell_type":"code","source":"def PIL_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            return Image.open(f).convert('RGB')\n    except IOError:\n        print('Cannot load image ' + path)\n        \ndef change_emotion_label_same_as_affectnet(emo_to_return):\n        \"\"\"\n        Parse labels to make them compatible with AffectNet.  \n        #https://github.com/siqueira-hc/Efficient-Facial-Feature-Learning-with-Wide-Ensemble-based-Convolutional-Neural-Networks/blob/master/model/utils/udata.py\n        \"\"\"\n\n        if emo_to_return == 0:\n            emo_to_return = 3\n        elif emo_to_return == 1:\n            emo_to_return = 4\n        elif emo_to_return == 2:\n            emo_to_return = 5\n        elif emo_to_return == 3:\n            emo_to_return = 1\n        elif emo_to_return == 4:\n            emo_to_return = 2\n        elif emo_to_return == 5:\n            emo_to_return = 6\n        elif emo_to_return == 6:\n            emo_to_return = 0\n\n        return emo_to_return\n\ndef default_reader(fileList):\n    #print(fileList)\n    counter_loaded_images_per_label = [0 for _ in range(7)]\n\n    num_per_cls_dict = dict()\n    for i in range(0, 7):\n        num_per_cls_dict[i] = 0\n\n    imgList = []\n    if fileList.find('occlusion_list.txt') > -1:\n        fp = open(fileList,'r')\n        for names in fp.readlines():\n            image_path, target, _  = names.split(' ')  #Eg. test_0025_aligned 3 3 #name, Ist target, 2nd target\n            image_path = image_path.strip()+'.jpg'\n            target = int(target) \n            target = change_emotion_label_same_as_affectnet(target)\n            num_per_cls_dict[target] = num_per_cls_dict[target] + 1 \n            imgList.append((image_path, target))           \n        return imgList ,    num_per_cls_dict\n             \n    elif  fileList.find('pose') > -1:\n        fp = open(fileList,'r')\n        for names in fp.readlines():\n            target, image_path  = names.split('/')  #Eg. for each entry before underscore lable and afterwards name in 1/fer0034656.jpg\n            image_path = image_path.strip()\n            #print(target,image_path)\n            target = int(target) \n            target = change_emotion_label_same_as_affectnet(target)\n            num_per_cls_dict[target] = num_per_cls_dict[target] + 1 \n            imgList.append((image_path, target))\n        return imgList ,    num_per_cls_dict \n    else:#test/train/validation.csv\n \n        fp = open(fileList,'r')\n\n        for names in fp.readlines():\n            image_path, target  = names.split(' ')  #Eg. for each entry before underscore lable and afterwards name in 1_fer0034656.png 8 0, 2_fer0033878.png 8 0\n\n            name,ext = image_path.strip().split('.')                #imagename is name.jpg  --->  name_algined.jpg\n\n            image_path  = name + '_aligned.' + ext\n\n            target  =  int(target) - 1 #labels are from 1-7\n\n            target = change_emotion_label_same_as_affectnet(target)\n\n            counter_loaded_images_per_label[target] += 1 \n\n            num_per_cls_dict[target] = num_per_cls_dict[target] + 1 \n\n            imgList.append((image_path, int(target)))\n\n        fp.close()\n\n        return imgList, num_per_cls_dict\n\nclass ImageList(Dataset):\n    def __init__(self, root, fileList,  transform=None, list_reader=default_reader, loader=PIL_loader, step = None):\n        self.root = root\n        self.cls_num = 7\n        self.imgList, self.num_per_cls_dict = list_reader(fileList)\n        self.transform = transform\n        self.loader = loader\n        self.is_save = True\n        self.totensor = transforms.ToTensor()\n        self.landmark_cache = landmarks_cache\n        self.step = step\n        \n    def draw_squares_on_landmarks(self, img, landmarks, n):\n    # Resize the input image to 224x224\n        dimg = img.resize((224, 224))\n\n        if n == 0:\n            return dimg\n        half_n = n // 2\n        draw = ImageDraw.Draw(dimg)\n\n        for kp in landmarks:\n            # Keypoints are already in the 224x224 scale\n            kp_x, kp_y = kp[0], kp[1]\n\n            # Ensure coordinates are integers\n            top_left = (int(kp_x - half_n), int(kp_y - half_n))\n            bottom_right = (int(kp_x + half_n), int(kp_y + half_n))\n\n            # Draw black rectangle on the keypoint\n            draw.rectangle([top_left, bottom_right], fill=(0, 0, 0))  # fill the rectangle with black\n\n        return dimg\n    def __getitem__(self, index):\n        imgPath, target_expression = self.imgList[index]\n#         print(imgPath, target_expression)\n        img1 = self.loader(os.path.join(self.root, imgPath))\n            \n#         if imgPath in self.landmark_cache and self.step:\n# #             print('found landmarks:')\n#             img2 = self.draw_squares_on_landmarks(img1, self.landmark_cache[imgPath], self.step)\n#         else:\n#             img2 = img1.copy()\n        img2 = img1.copy()\n\n\n        if random.random() > 0.5:\n            img2 = img2.transpose(Image.FLIP_LEFT_RIGHT)\n        else:\n            img2 = img2\n        \n        if self.transform is not None:\n#             print(f'performing transformations. shape before: {img1.size}')\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n#             print(f'new stuff: {type(img1)}, {img1.shape}')\n        return img1,img2, target_expression\n\n    def __len__(self):\n        return len(self.imgList)\n\n    def get_cls_num_list(self):\n        cls_num_list = []\n        for i in range(self.cls_num):\n            cls_num_list.append(self.num_per_cls_dict[i])\n        return cls_num_list","metadata":{"execution":{"iopub.status.busy":"2024-09-23T08:17:01.933158Z","iopub.execute_input":"2024-09-23T08:17:01.933903Z","iopub.status.idle":"2024-09-23T08:17:01.958918Z","shell.execute_reply.started":"2024-09-23T08:17:01.933859Z","shell.execute_reply":"2024-09-23T08:17:01.958042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FERPlus DataLoader","metadata":{}},{"cell_type":"code","source":"def PIL_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            return Image.open(f).convert('RGB')\n    except IOError:\n        print('Cannot load image ' + path)\n\n\ndef make_emotion_compatible_to_affectnet(emo_to_return):\n        \"\"\"\n        Parse labels to make them compatible with AffectNet.\n        :param idx:\n        :return:\n        \"\"\"\n\n        if emo_to_return == 2:\n            emo_to_return = 3\n        elif emo_to_return == 3:\n            emo_to_return = 2\n        elif emo_to_return == 4:\n            emo_to_return = 6\n        elif emo_to_return == 6:\n            emo_to_return = 4\n\n        return emo_to_return\n\n\n\ndef default_reader(fileList, mode, num_expression=8):\n    imgList = []\n    i = 0\n    fp = open(fileList,'r')\n\n    for names in fp.readlines():\n        image_path, target  = names.split(' ')  #Eg. for each entry before underscore lable and afterwards name in 1_fer0034656.png 8 0, 2_fer0033878.png 8 0\n\n        target  =  int(target) #labels are from 1-7\n\n#             target = make_emotion_compatible_to_affectnet(target)\n\n#         counter_loaded_images_per_label[target] += 1 \n\n#         num_per_cls_dict[target] = num_per_cls_dict[target] + 1 \n\n        imgList.append((image_path, int(target)))\n\n    fp.close()\n\n    return imgList\n\nclass ImageList(data.Dataset):\n    def __init__(self, root, fileList, transform=None, num_expressions = 8, list_reader=default_reader, loader = PIL_loader, mode = 'majority', step = 0):\n        self.root = root\n        self.transform = transform\n        self.loader = loader\n        self.fileList  = fileList\n        self.training_mode = mode\n        self.landmark_cache = landmarks_cache\n        self.num_expression =  num_expressions\n        self.step = step\n\n        self.imgList = list_reader(fileList, self.training_mode, self.num_expression)\n        self.num_per_cls_dict = self.get_class_wise_count()\n#         print('checking class wise list: ', self.num_per_cls_dict)\n\n    def get_class_wise_count(self):\n        num_per_cls_dict = dict()\n        for i in range(0, self.num_expression):\n            num_per_cls_dict[i] = 0\n\n        for _,target in self.imgList:\n            target_int = target\n            target_int = make_emotion_compatible_to_affectnet(target_int)\n\n            num_per_cls_dict[target_int] = num_per_cls_dict[target_int] + 1 \n        return num_per_cls_dict\n            \n    def _process_target(self, target):\n        '''\n        Based on https://arxiv.org/abs/1608.01041 the target depend on the training mode.\n        Majority or crossentropy: return the probability distribution generated by \"_process_data\"\n        Probability: pick one emotion based on the probability distribtuion.\n        Multi-target:\n        '''\n        if self.training_mode == 'majority' or self.training_mode == 'crossentropy':\n            target = np.argmax(target)\n            return target\n        elif self.training_mode == 'probability':            \n            idx = np.random.choice(len(target), p=target)\n            return idx\n        \n    def draw_squares_on_landmarks(self, img, landmarks, n):\n    # Resize the input image to 224x224\n        dimg = img.resize((224, 224))\n\n        if n == 0:\n            return dimg\n        half_n = n // 2\n        draw = ImageDraw.Draw(dimg)\n\n        for kp in landmarks:\n            # Keypoints are already in the 224x224 scale\n            kp_x, kp_y = kp[0], kp[1]\n\n            # Ensure coordinates are integers\n            top_left = (int(kp_x - half_n), int(kp_y - half_n))\n            bottom_right = (int(kp_x + half_n), int(kp_y + half_n))\n\n            # Draw black rectangle on the keypoint\n            draw.rectangle([top_left, bottom_right], fill=(0, 0, 0))  # fill the rectangle with black\n\n        return dimg\n        \n    def __getitem__(self, index):\n\n        imgPath, target_expression = self.imgList[index]\n#         print(os.path.join(self.root, imgPath))\n        img1 = self.loader(os.path.join(self.root, imgPath + '.png'))\n    \n#         print(imgPath)\n        \n        if imgPath + '.png' in self.landmark_cache and self.step and random.random() > 0.5:\n#             print('found landmarks:')\n            img2 = self.draw_squares_on_landmarks(img1, self.landmark_cache[imgPath + '.png'], self.step)\n        else:\n            img2 = img1.copy()\n\n#         img2 = img1.copy()\n\n        if random.random() > 0.5:\n            img2 = img2.transpose(Image.FLIP_LEFT_RIGHT)\n        \n        if self.transform is not None:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n\n\n        target_expression = make_emotion_compatible_to_affectnet(target_expression) #converts into emtion category same as that of affectnet\n\n        return img1,img2, target_expression\n\n    def __len__(self):\n        return len(self.imgList)\n\n    def get_cls_num_list(self):\n        cls_num_list = []\n        for i in range(self.num_expression):\n            cls_num_list.append(self.num_per_cls_dict[i])\n        return cls_num_list","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:18:20.771583Z","iopub.execute_input":"2024-09-26T08:18:20.771979Z","iopub.status.idle":"2024-09-26T08:18:20.798774Z","shell.execute_reply.started":"2024-09-26T08:18:20.771943Z","shell.execute_reply":"2024-09-26T08:18:20.797424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([                      \n#         AutoAugment(),\n        transforms.Resize((224,224)),            \n        transforms.ToTensor(),\n    ])\n\ntrain_dataset = ImageList(root='/kaggle/input/ferplus/FER2013TrainValid/FER2013TrainValid', fileList='/kaggle/input/ferplus/ferplus_trainvalid_list.txt',\n                  transform=train_transform, step = 5)\ncount = 0\nfor a,b,c in train_dataset:\n#     to_pil = transforms.ToPILImage()\n#     input1_img = to_pil(b)  # Convert only the first image in the batch to PIL\n#     display(input1_img)\n    break\nto_pil = transforms.ToPILImage()\ninput1_img = to_pil(a)  # Convert only the first image in the batch to PIL\ndisplay(input1_img)  # Display the PIL image","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:18:35.139721Z","iopub.execute_input":"2024-09-26T08:18:35.14041Z","iopub.status.idle":"2024-09-26T08:18:35.216902Z","shell.execute_reply.started":"2024-09-26T08:18:35.140358Z","shell.execute_reply":"2024-09-26T08:18:35.215597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, basemodel, criterion, epoch, scheduler, writer= None, run = 0):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    overall_loss = AverageMeter()\n    top1 = AverageMeter()\n    mode = 'Testing'\n\n    all_preds = []\n    all_targets = []\n    \n    avg_loss = 0\n    avg_acc = 0\n    count = 0\n\n    # Ensure model is on the correct device\n    basemodel.to(device)\n\n    # Switch to evaluate mode\n    basemodel.eval()\n\n    end = time.time()\n\n    with torch.no_grad():\n        # Use tqdm for the progress bar\n        with tqdm(total=len(val_loader), desc=f\"Validation Epoch {epoch}\", unit=\"batch\") as pbar:\n            for i, (input1, input2, target) in enumerate(val_loader):\n                # Measure data loading time\n                data_time.update(time.time() - end)\n\n                # Move inputs to device\n                input1 = input1.to(device)\n                target = target.to(device)\n\n                # Compute predictions\n                region_preds = basemodel(input1)\n\n                # Compute loss\n                loss = criterion(region_preds, target)\n                overall_loss.update(loss.item(), input1.size(0))\n\n                # Calculate accuracy\n                avg_prec = accuracy(region_preds, target, topk=(1,))\n                top1.update(avg_prec[0], input1.size(0))\n\n                # Store predictions and targets for metrics calculation\n                preds = torch.argmax(region_preds, dim=1).cpu().numpy()\n                all_preds.extend(preds)\n                all_targets.extend(target.cpu().numpy())\n\n                # Measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                # Update progress bar\n                pbar.set_postfix(\n                    batch_time=batch_time.avg,\n                    data_time=data_time.avg,\n                    overall_loss=overall_loss.avg,\n                    Prec1=top1.avg,\n                )\n                pbar.update(1)\n\n                # Log the metrics\n#                 if writer:\n                avg_loss += loss.item()\n                avg_acc += top1.avg\n                count += 1\n#                     writer.add_scalar(f'Validation_{run}/Loss', loss.item(), epoch * len(val_loader) + i)\n#                     writer.add_scalar(f'Validation_{run}/Accuracy', top1.avg, epoch * len(val_loader) + i)\n\n        # Calculate precision, recall, F1 score\n        precision = precision_score(all_targets, all_preds, average='weighted')\n        recall = recall_score(all_targets, all_preds, average='weighted')\n        f1 = f1_score(all_targets, all_preds, average='weighted')\n\n        # Log precision, recall, and F1 score\n        if writer:\n            writer.add_scalar(f'Validation_{run}/Precision', precision, epoch)\n            writer.add_scalar(f'Validation_{run}/Recall', recall, epoch)\n            writer.add_scalar(f'Validation_{run}/F1_Score', f1, epoch)\n            writer.add_scalar(f'Validation_{run}/Loss', avg_loss/count, epoch)\n            writer.add_scalar(f'Validation_{run}/Accuracy', avg_acc/count, epoch)\n\n\n        print(f'\\n{mode} [{epoch}/{len(val_loader)}]\\t'\n              f'overall_loss ({overall_loss.avg:.4f})\\t'\n              f'Prec@1  ({top1.avg:.4f})\\t')\n\n        # Step the scheduler\n        scheduler.step(overall_loss.avg)\n\n    return top1.avg\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:18:47.816754Z","iopub.execute_input":"2024-09-26T08:18:47.817176Z","iopub.status.idle":"2024-09-26T08:18:47.841802Z","shell.execute_reply.started":"2024-09-26T08:18:47.817134Z","shell.execute_reply":"2024-09-26T08:18:47.840585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_transform = transforms.Compose([                      \n#         AutoAugment(),\n        transforms.Resize((224,224)),            \n        transforms.ToTensor(),\n    ])\n\nvalid_transform = transforms.Compose([\n        transforms.Resize((224,224)),            \n        transforms.ToTensor(),\n\n    ])\n\ntrain_dataset = ImageList(root='/kaggle/input/ferplus/FER2013TrainValid/FER2013TrainValid', fileList='/kaggle/input/ferplus/ferplus_trainvalid_list.txt',\n                  transform=train_transform)\ncls_num_list = train_dataset.get_cls_num_list()\n# print(cls_num_list)\n\n# Using Reweight training rule because idk what sir used\ntrain_sampler = None\nbeta = 0.9999                 #0:normal weighting\neffective_num = 1.0 - np.power(beta, cls_num_list)\n# print(effective_num)\nper_cls_weights = (1.0 - beta) / np.array(effective_num)\nper_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\nper_cls_weights = torch.FloatTensor(per_cls_weights).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:18:51.406682Z","iopub.execute_input":"2024-09-26T08:18:51.407074Z","iopub.status.idle":"2024-09-26T08:18:51.677091Z","shell.execute_reply.started":"2024-09-26T08:18:51.407037Z","shell.execute_reply":"2024-09-26T08:18:51.675622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basemodel = resnet18(pretrained= False)\nbasemodel.fc = nn.Linear(512, 8)\n  \n# basemodel = torch.nn.DataParallel(basemodel).to(device)\ncheckpoint = torch.load('/kaggle/input/rafdb-dg/res18_naive.pth_MSceleb')\npretrained_state_dict = checkpoint['state_dict']\nnew_state_dict = {}\nfor key in pretrained_state_dict.keys():\n    new_key = key.replace('module.', '')  # Remove 'module.' prefix\n    new_state_dict[new_key] = pretrained_state_dict[key]\n    \nmodel_state_dict = basemodel.state_dict()\n\nfor key in new_state_dict:\n    if  ((key=='fc.weight')|(key=='fc.bias')|(key == 'feature.weight')|(key == 'feature.bias')):\n        pass\n    else:\n        model_state_dict[key] = new_state_dict[key]\n\nbasemodel.load_state_dict(model_state_dict, strict = True)\nbasemodel = basemodel.to(device)\nprint('Model loaded from Msceleb pretrained')   \n# print('\\nNumber of trainable parameters: {}\\n'.format(count_parameters(basemodel)))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:18:54.708544Z","iopub.execute_input":"2024-09-26T08:18:54.709241Z","iopub.status.idle":"2024-09-26T08:18:58.168156Z","shell.execute_reply.started":"2024-09-26T08:18:54.709198Z","shell.execute_reply":"2024-09-26T08:18:58.167027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\ncriterion = nn.CrossEntropyLoss(weight=per_cls_weights).to(device)\noptimizer = torch.optim.Adamax(basemodel.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=4e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)    \n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=(train_sampler is None), pin_memory=True, sampler=train_sampler)    \n    \ntest_data = ImageList('/kaggle/input/ferplus/FER2013Test/FER2013Test', fileList='/kaggle/input/ferplus/ferplus_test.txt',\n              transform=valid_transform)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n                     shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:19:01.141903Z","iopub.execute_input":"2024-09-26T08:19:01.14278Z","iopub.status.idle":"2024-09-26T08:19:01.169256Z","shell.execute_reply.started":"2024-09-26T08:19:01.142733Z","shell.execute_reply":"2024-09-26T08:19:01.168126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 0\nprec1 = validate(test_loader, basemodel, criterion,  epoch, scheduler)\nprint(\"Epoch: {}   Test Acc: {}\".format(epoch, prec1))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:19:02.53508Z","iopub.execute_input":"2024-09-26T08:19:02.535542Z","iopub.status.idle":"2024-09-26T08:19:28.428014Z","shell.execute_reply.started":"2024-09-26T08:19:02.535485Z","shell.execute_reply":"2024-09-26T08:19:28.42675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, basemodel, criterion, optimizer, num_epochs, epoch, writer, run):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    overall_loss = AverageMeter()\n    top1 = AverageMeter()\n    \n#     step = max(0, 5 - epoch // 8)                  # decreasing patch size\n\n    if epoch < 10:\n        step = 0\n    else:\n        step = (epoch - 10) // 8 + 1\n        \n    print(f'Epoch {epoch}; step = {step}')\n    \n    \n    train_dataset = ImageList(root='/kaggle/input/ferplus/FER2013TrainValid/FER2013TrainValid', fileList='/kaggle/input/ferplus/ferplus_trainvalid_list.txt',\n                  transform=train_transform, step = step)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=(train_sampler is None), pin_memory=True, sampler=train_sampler) \n    \n    all_preds = []\n    all_targets = []\n\n    end = time.time()\n    \n    flag = True\n    \n    avg_loss = 0\n    avg_acc = 0\n    count = 0\n    \n    # Wrap the train_loader with tqdm for the progress bar\n    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch}\", unit=\"batch\") as pbar:\n#         i am swapping names of input1 and input2. now. input1 has augmented data\n        for i, (input2, input1, target) in enumerate(train_loader):\n            # Measure data loading time\n            if flag:\n                # Convert the tensor to a PIL image\n                to_pil = transforms.ToPILImage()\n\n                # Convert the first image in the batch to PIL images\n                input1_img = to_pil(input1[0])\n#                 input2_img = to_pil(input2[0])\n\n#                 # Create a new blank image with double the width to hold both images\n#                 grid_width = input1_img.width + input2_img.width\n#                 grid_height = max(input1_img.height, input2_img.height)\n\n#                 grid_img = Image.new('RGB', (grid_width, grid_height))\n\n#                 # Paste input1_img and input2_img side by side\n#                 grid_img.paste(input1_img, (0, 0))  # Paste input1 at (0, 0)\n#                 grid_img.paste(input2_img, (input1_img.width, 0))  # Paste input2 next to input1\n\n                # Display the grid image with both images\n                display(input1_img)\n\n                flag = False\n\n            data_time.update(time.time() - end)\n            \n#             if step > 0:\n#                 input1 = torch.cat([input1, input2], dim=0)\n#                 target = torch.cat([target, target], dim = 0)\n\n            input1 = input1.to(device)\n            target = target.to(device)\n\n            # Compute output\n            region_preds = basemodel(input1)\n            loss = criterion(region_preds, target)\n            overall_loss.update(loss.item(), input1.size(0))\n\n            # Calculate accuracy\n            avg_prec = accuracy(region_preds, target, topk=(1,))\n            top1.update(avg_prec[0], input1.size(0))\n\n            # Store predictions and targets for metrics calculation\n            preds = torch.argmax(region_preds, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(target.cpu().numpy())\n\n            # Compute gradient and update\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # Update the progress bar\n            pbar.set_postfix(\n                batch_time=batch_time.avg,\n                data_time=data_time.avg,\n                overall_loss=overall_loss.avg,\n                Prec1=top1.avg,\n            )\n            pbar.update(1)\n\n            # Log the metrics\n            avg_loss += loss.item()\n            avg_acc += top1.avg\n            count += 1\n#             writer.add_scalar(f'Train_{run}/Loss', loss.item(), epoch * len(train_loader) + i)\n#             writer.add_scalar(f'Train_{run}/Accuracy', top1.avg, epoch * len(train_loader) + i)\n\n    # Calculate precision, recall, F1 score\n    precision = precision_score(all_targets, all_preds, average='weighted')\n    recall = recall_score(all_targets, all_preds, average='weighted')\n    f1 = f1_score(all_targets, all_preds, average='weighted')\n    ls = avg_loss/count\n    acc = avg_acc/count\n\n    # Log precision, recall, and F1 score\n    writer.add_scalar(f'Train_{run}/Precision', precision, epoch)\n    writer.add_scalar(f'Train_{run}/Recall', recall, epoch)\n    writer.add_scalar(f'Train_{run}/F1_Score', f1, epoch)\n    writer.add_scalar(f'Train_{run}/Loss', ls, epoch)\n    writer.add_scalar(f'Train_{run}/Accuracy', acc, epoch)\n    writer.add_scalar(f'Train_{run}/Step', step, epoch)\n\n\n\n    # Log the learning rate\n    for param_group in optimizer.param_groups:\n        writer.add_scalar(f'Train_{run}/Learning_Rate', param_group['lr'], epoch)\n\n    return top1.avg\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:19:42.874503Z","iopub.execute_input":"2024-09-26T08:19:42.874943Z","iopub.status.idle":"2024-09-26T08:19:42.898499Z","shell.execute_reply.started":"2024-09-26T08:19:42.8749Z","shell.execute_reply":"2024-09-26T08:19:42.89733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# add summaryWriter to log base training\n# train for 50 epochs\n# implement FiFA and observe changes","metadata":{}},{"cell_type":"code","source":"os.remove('/kaggle/working/run_7/events.out.tfevents.1727075140.dd049e723a8f.36.5')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:07:43.411534Z","iopub.execute_input":"2024-09-23T07:07:43.41193Z","iopub.status.idle":"2024-09-23T07:07:43.416948Z","shell.execute_reply.started":"2024-09-23T07:07:43.411891Z","shell.execute_reply":"2024-09-23T07:07:43.415728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nbest_prec1 = 0.\n# run 1: decreasing patch size with AutoAugment\n# run 2: increasing patch size with AutoAugment\n# run 2.1: rerun of increasing patch size with AutoAugment\n# run 2.2: rerun of increasing patch size with AutoAugment, patch size increases by 1 every 10 steps\n\n# run 0.1: no augment\n# run 3: only FiFA augment, increasing patch size\n# run 4: only FiFA augment, decreasing patch size\n# run 5: FiFA_ + AutoAugment, and only AutoAugment\n\n# run 6: noAug till 10, FiFA increase by 1 every 8 steps and randomFLip\n# run 7: RandomFlip + FiFA decreasing patch size, step = 8\n# run 8: randomFlip only\n\n# run f0: resnet18 on FERplus with only randomflip\n# run f1: resnet18 on fer+ with randomflip + FiFA increasing patch size, starts at 10, +1 every 8 steps\n# run f2: resnet18 on fer+ with randomflip + FiFA increasing patch size, starts at 15, +1 every 10 steps\n# fun f3: resnet18 on fer+ with randomflip + FiFA increasing patch size, starts at 25, +1 every 5 steps\n# run f4: resnet18 on fer+ with randomflip + FiFA decreasing patch size, starts at size 5, -1 every 8 steps\nrun = 'f5'\nwriter = SummaryWriter(f'run_{run}')\nprint('\\nTraining starting:\\n')\nfor epoch in range(0, epochs):\n\n    # train for one epoch        \n    train(train_loader, basemodel,  criterion, optimizer,epochs, epoch, writer, run)\n    prec1 = validate(test_loader, basemodel, criterion,  epoch, scheduler, writer, run)\n    print(\"Epoch: {}   Test Acc: {}\".format(epoch, prec1))\n    print(\"=================================================================\\n\")\n\n    # remember best prec@1 and save checkpoint\n    is_best = prec1.item() > best_prec1\n    best_prec1 = max(prec1.to(device).item(), best_prec1)\n\n    if is_best:\n        print('So far best epoch, acc',epoch, best_prec1)\n        torch.save(  {\n                       'epoch': epoch + 1,\n                       'base_state_dict': basemodel.state_dict(),\n                       'best_prec1': best_prec1,\n                       'optimizer' : optimizer.state_dict(),\n                     }, os.path.join('/kaggle/working/trained_model', f'run_{run}_best_checkpoint_rafdb_resnet18_pre_trained_msceleb.pth.tar'))\n# writer.close()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T08:19:56.925492Z","iopub.execute_input":"2024-09-26T08:19:56.926006Z","iopub.status.idle":"2024-09-26T10:52:28.771953Z","shell.execute_reply.started":"2024-09-26T08:19:56.925957Z","shell.execute_reply":"2024-09-26T10:52:28.770767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation for ABAW","metadata":{}},{"cell_type":"code","source":"def get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0):\n\n    dataset_train = RAFDBDataset(choose=\"train\",\n        data_path=data_path,\n        label_path=label_path,\n        app = None,\n        transform = None,\n        img_size = 224,\n        num_epochs = num_epochs,\n        epochs = epochs\n                                 \n    )\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        batch_size=batch_size,\n        drop_last=True,\n    )\n    return data_loader_train, dataset_train.length, dataset_train\n\n\ndef get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0):\n\n    dataset_val = RAFDBDataset(choose=\"test\",\n        data_path=data_path,\n        label_path=label_path,\n        app = None,        \n        transform = None,\n        img_size = 224,\n        num_epochs = num_epochs,\n        epochs = epochs\n    )\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False\n    )\n    return data_loader_val, dataset_val.length, dataset_val","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:36:18.354064Z","iopub.execute_input":"2024-09-16T08:36:18.354425Z","iopub.status.idle":"2024-09-16T08:36:18.365889Z","shell.execute_reply.started":"2024-09-16T08:36:18.354391Z","shell.execute_reply":"2024-09-16T08:36:18.364633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landmarks_cache = {}\n\n# Open the file and read all lines first to calculate total length for progress bar\nwith open('/kaggle/input/raf-db-trial/keypoints.txt', 'r') as file:\n    lines = file.readlines()\n\n# Iterate over lines with progress bar\nfor line in tqdm(lines, desc=\"Processing landmarks\", unit=\"lines\"):\n    # Split at the first occurrence of the colon\n    file_name, coordinates = line.split(\":\", 1)\n\n    # Remove whitespace around the file name\n    file_name = file_name.strip()\n\n    # Convert the string representation of the list of tuples to an actual list of tuples\n    coordinates = ast.literal_eval(coordinates.strip())\n\n    # Add to the dictionary\n    landmarks_cache[file_name] = coordinates","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:45:38.516778Z","iopub.execute_input":"2024-09-15T06:45:38.517334Z","iopub.status.idle":"2024-09-15T06:45:58.192184Z","shell.execute_reply.started":"2024-09-15T06:45:38.517268Z","shell.execute_reply":"2024-09-15T06:45:58.190901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/input/')\n# /kaggle/input/raf-db-trial/keypoints.txt","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:45:58.194424Z","iopub.execute_input":"2024-09-15T06:45:58.195268Z","iopub.status.idle":"2024-09-15T06:45:58.204214Z","shell.execute_reply.started":"2024-09-15T06:45:58.195211Z","shell.execute_reply":"2024-09-15T06:45:58.203105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RandomMasking:\n    def __init__(self, mask_size=32, num_masks=5):\n        \"\"\"\n        Initialize the masking transformation.\n        mask_size: Size of the square mask (in pixels).\n        num_masks: Number of random masks to apply.\n        \"\"\"\n        self.mask_size = mask_size\n        self.num_masks = num_masks\n\n    def __call__(self, img):\n        \"\"\"\n        Apply random masks to the image.\n        img: Tensor image (C, H, W)\n        \"\"\"\n        # Get image dimensions\n        _, h, w = img.shape\n        \n        # Mask the image `num_masks` times\n        for _ in range(self.num_masks):\n            # Randomly choose top-left corner of the mask\n            x = random.randint(0, w - self.mask_size)\n            y = random.randint(0, h - self.mask_size)\n            \n            # Apply the mask (zero out the pixels in the selected region)\n            img[:, y:y+self.mask_size, x:x+self.mask_size] = 0\n        \n        return img","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:46:02.454157Z","iopub.execute_input":"2024-09-15T06:46:02.454575Z","iopub.status.idle":"2024-09-15T06:46:02.463298Z","shell.execute_reply.started":"2024-09-15T06:46:02.454538Z","shell.execute_reply":"2024-09-15T06:46:02.46214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RAFDBDataset(Dataset):\n    def __init__(self, choose, data_path, label_path, app, transform=None, img_size=224, num_epochs = 20, epochs = 0):\n        self.image_paths = []\n        self.labels = []\n        self.data_path = data_path\n        self.label_path = label_path\n        self.app = app\n        self.landmarks_cache = landmarks_cache  # Cache to store landmark coordinates\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.num_epoch = num_epochs\n        self.epochs = epochs\n\n        if transform:\n            self.transform = transform\n        else:\n            self.transform = T.Compose([\n                T.Resize(256),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n            ])\n\n        self.train = True if choose == \"train\" else False\n\n        if choose == \"train\" or choose == \"test\":\n            with open(self.label_path, \"r\") as f:\n                data = f.readlines()\n\n            for i in range(0, len(data)):\n                line = data[i].strip('\\n').split(\" \")\n\n                image_name = line[0]\n                sample_temp = image_name.split(\"_\")[0]\n\n                if self.train and sample_temp == \"train\":\n                    image_path = os.path.join(self.data_path, image_name)\n                    self.image_paths.append(image_path)\n                    self.labels.append(int(line[1]) - 1)\n\n                elif not self.train and sample_temp == \"test\":\n                    image_path = os.path.join(self.data_path, image_name)\n                    self.image_paths.append(image_path)\n                    self.labels.append(int(line[1]) - 1)\n                    \n        self.length = len(self.labels)\n        self.labels = np.asarray(self.labels)\n\n    def draw_squares_on_landmarks(self, img, landmarks, n):\n        dimg = img.copy()\n        if n == 0:\n            return dimg\n        half_n = n // 2\n        for kp in landmarks:\n            # Ensure coordinates are integers\n            top_left = (int(kp[0] - half_n), int(kp[1] - half_n))\n            bottom_right = (int(kp[0] + half_n), int(kp[1] + half_n))\n            cv2.rectangle(dimg, top_left, bottom_right, (0, 0, 0), -1)  # -1 fills the rectangle\n        return dimg\n\n    def get_landmarks(self, img_path):\n        if img_path not in self.landmarks_cache:\n            return None\n            img = cv2.imread(img_path)\n            nimg = cv2.resize(img, (224,224))\n            k = 100\n            padded_image = cv2.copyMakeBorder(\n                nimg,\n                k,\n                k,\n                k,\n                k,\n                cv2.BORDER_CONSTANT,  # Border type\n                value=[0, 0, 0]       # Padding color (black in this case)\n            )\n#             padded_image = padded_image.to(self.device)  # Ensure the image is on the right device\n            out = self.app.get(padded_image)\n            \n            if len(out) == 0:  # Check if any faces were detected\n                return None\n            \n            landmarks = out[0].landmark_2d_106.astype(np.int64)\n            self.landmarks_cache[img_path] = landmarks\n        return self.landmarks_cache[img_path]\n\n    def augment(self, img_path):\n        img = cv2.imread(img_path)\n        nimg = cv2.resize(img, (224, 224))\n        landmarks = self.get_landmarks(img_path.split('/')[-1])\n#         print(landmarks)\n        if landmarks is None:  # Skip if no faces are detected\n            return nimg, nimg\n        \n        # Use landmarks to draw squares on the padded image\n        aug = self.draw_squares_on_landmarks(nimg, landmarks, n=max(0, self.epochs))\n#         plt.imshow(aug)\n#         plt.axis('off')  # Optional: turn off axes\n#         plt.show()\n        return nimg, aug\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img1, img2 = self.augment(img_path)\n        \n        if self.transform:\n            img1 = self.transform(Image.fromarray(img1))\n            img2 = self.transform(Image.fromarray(img2))\n            \n#         img1 = img1.unsqueeze(1)\n#         img2 = img2.unsqueeze(1)\n        label = torch.tensor(self.labels[idx])\n        return img1, img2, label\n\n    def __len__(self):\n        return self.length","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:46:05.397333Z","iopub.execute_input":"2024-09-15T06:46:05.397777Z","iopub.status.idle":"2024-09-15T06:46:05.424674Z","shell.execute_reply.started":"2024-09-15T06:46:05.397711Z","shell.execute_reply":"2024-09-15T06:46:05.42323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '//kaggle/input/raf-db-trial/92_86/92/dataset/RAF'\nlabel_path = '/kaggle/input/raf-db-trial/92_86/92/dataset/list_patition_label.txt'\nbatch_size = 128\n\n# data_loader_train, train_len, d = get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 4)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:46:31.363371Z","iopub.execute_input":"2024-09-15T06:46:31.36384Z","iopub.status.idle":"2024-09-15T06:46:31.369456Z","shell.execute_reply.started":"2024-09-15T06:46:31.363796Z","shell.execute_reply":"2024-09-15T06:46:31.367954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = d.augment('/kaggle/input/92_86/92/dataset/RAF/train_00016.jpg')\n\nfor a,b,c in data_loader_train:\n    break\n\nprint(a.shape)\nprint(b.shape)\nprint(t[1].shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:54:59.774004Z","iopub.execute_input":"2024-09-14T08:54:59.775098Z","iopub.status.idle":"2024-09-14T08:55:00.602805Z","shell.execute_reply.started":"2024-09-14T08:54:59.775058Z","shell.execute_reply":"2024-09-14T08:55:00.601859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(a[0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:30:43.013498Z","iopub.execute_input":"2024-09-14T08:30:43.014005Z","iopub.status.idle":"2024-09-14T08:30:43.021682Z","shell.execute_reply.started":"2024-09-14T08:30:43.013959Z","shell.execute_reply":"2024-09-14T08:30:43.020231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = model(a)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:34:03.11644Z","iopub.execute_input":"2024-09-14T08:34:03.117084Z","iopub.status.idle":"2024-09-14T08:34:05.042912Z","shell.execute_reply.started":"2024-09-14T08:34:03.117024Z","shell.execute_reply":"2024-09-14T08:34:05.041638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:34:11.317277Z","iopub.execute_input":"2024-09-14T08:34:11.317821Z","iopub.status.idle":"2024-09-14T08:34:11.32646Z","shell.execute_reply.started":"2024-09-14T08:34:11.317774Z","shell.execute_reply":"2024-09-14T08:34:11.325105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(a[0].permute(1,2,0).numpy()[:, :, ::-1] )\nplt.axis('off')  # Optional: turn off axes\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:41:01.191893Z","iopub.execute_input":"2024-09-14T08:41:01.195391Z","iopub.status.idle":"2024-09-14T08:41:01.394617Z","shell.execute_reply.started":"2024-09-14T08:41:01.195315Z","shell.execute_reply":"2024-09-14T08:41:01.393323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t[1].shape","metadata":{"execution":{"iopub.status.busy":"2024-09-11T04:56:21.456292Z","iopub.execute_input":"2024-09-11T04:56:21.456733Z","iopub.status.idle":"2024-09-11T04:56:21.465095Z","shell.execute_reply.started":"2024-09-11T04:56:21.456699Z","shell.execute_reply":"2024-09-11T04:56:21.463441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code below is for ABAW, code above is for RAFDB","metadata":{}},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model = models.resnet50(pretrained=False)\ncheckpoint = torch.load('/kaggle/input/resnet18-checkpoint-real/res18_naive.pth_MSceleb', map_location=torch.device('cpu'))\n\nstate_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\nnew_state_dict = {}\nfor k, v in state_dict.items():\n    new_key = k.replace('module.', '')  # Remove 'module.' prefix\n    new_state_dict[new_key] = v\n\n# Load the modified state_dict into the model\nmodel.load_state_dict(new_state_dict, strict = False)\n# Assuming the checkpoint contains only the state_dict\n# model.load_state_dict(checkpoint['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:53:32.431923Z","iopub.execute_input":"2024-09-15T06:53:32.432378Z","iopub.status.idle":"2024-09-15T06:53:33.279768Z","shell.execute_reply.started":"2024-09-15T06:53:32.432329Z","shell.execute_reply":"2024-09-15T06:53:33.277809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint['state_dict'].keys()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T06:49:50.376427Z","iopub.execute_input":"2024-09-15T06:49:50.376875Z","iopub.status.idle":"2024-09-15T06:49:50.384766Z","shell.execute_reply.started":"2024-09-15T06:49:50.376833Z","shell.execute_reply":"2024-09-15T06:49:50.383448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet50(pretrained=True)\n\n# Freeze all the layers except the last few (usually fc and some conv blocks)\nfor name, param in model.named_parameters():\n    if \"layer4\" not in name and \"fc\" not in name:  # Only layer4 and fc will be unfrozen\n        param.requires_grad = False\n\n# Modify the final fully connected layer for 7 classes\nnum_classes = 7\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Define a transformation pipeline for the input images\ntransform = T.Compose([\n    T.Resize((224, 224)),  # ResNet input size\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)\n\nrun = 1\nlearning_rate = 0.001\n# learning_rate = 0.01\n\n# labels_np = np.array(train_labels)\n# class_counts = np.bincount(labels_np)          # Generate class counts\n# class_weights = 1.0 / class_counts             # Calculate inverse of class counts\n# class_weights = class_weights / class_weights.sum()            # Normalize the weights to sum to 1 (optional)\n# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n# criterion = LabelSmoothingCrossEntropy()\ncriterion = nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)\nwriter = SummaryWriter(f'run_{run}')","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:42:37.46088Z","iopub.execute_input":"2024-09-14T15:42:37.461637Z","iopub.status.idle":"2024-09-14T15:42:38.851885Z","shell.execute_reply.started":"2024-09-14T15:42:37.461595Z","shell.execute_reply":"2024-09-14T15:42:38.850855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:42:40.729939Z","iopub.execute_input":"2024-09-14T15:42:40.730387Z","iopub.status.idle":"2024-09-14T15:42:40.738389Z","shell.execute_reply.started":"2024-09-14T15:42:40.730347Z","shell.execute_reply":"2024-09-14T15:42:40.737399Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device, writer, run, num_epochs=20, app=None):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    step = 0\n    best_f1 = 0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n        flag = True\n        if epoch >= 6 and epoch < 20:\n            # Increase step after every 3 epochs starting from epoch 6\n            step = (epoch - 6) // 3 + 1\n            \n        data_loader_train, train_len, dataset_train = get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = step)\n        data_loader_val, val_len, dataset_val = get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = step)\n\n        dataloaders = {\n            \"train\": data_loader_train,\n            \"test\": data_loader_val\n        }\n\n        dataset_sizes = {\n            \"train\": train_len,\n            \"test\": val_len\n        }\n        \n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            all_preds = []\n            all_labels = []\n            \n            for input1, input2, labels in tqdm(dataloaders[phase]):\n                if phase == 'train':\n#                     inputs = torch.cat((input1, input2), dim = 0)\n#                     labels = torch.cat((labels, labels), dim = 0)\n#                     training with no augmentations\n                    inputs = input1\n#                     if flag:\n#                         fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n#                         # Display the original image (input1[0]) in the first subplot\n#                         axes[0].imshow(input1[0].permute(1, 2, 0).numpy()[:, :, ::-1])  # Assuming input1[0] is in (C, H, W) format\n#                         axes[0].set_title('Original')  # Add heading\n#                         axes[0].axis('off')  # Turn off the axes\n#                         # Display the augmented image (input2[0]) in the second subplot\n#                         axes[1].imshow(input2[0].permute(1, 2, 0).numpy()[:, :, ::-1])  # Assuming input2[0] is in (C, H, W) format\n#                         axes[1].set_title('Augmented')  # Add heading\n#                         axes[1].axis('off')  # Turn off the axes\n#                         # Show the figure with both images\n#                         plt.tight_layout()\n#                         plt.show()\n# #                         plt.imshow(input2[0].permute(1,2,3,0).squeeze(0))\n# #                         plt.axis('off')  # Optional: turn off axes\n# #                         plt.show()\n#                         flag = False\n                else:\n                    inputs = input1\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n            \n#             if phase == 'train' and (epoch+1) % 5 == 0:\n#                 scheduler.step()\n#                 torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase6_freezing_epoch_{epoch}.pth')\n            epoch_loss = running_loss / (dataset_sizes[phase] * (1 if phase == 'train' else 1))\n            epoch_acc = running_corrects.double() / (dataset_sizes[phase] * (1 if phase == 'train' else 1))\n            # Calculate F1 score, precision, and recall\n            epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n            \n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            \n            # Log the metrics\n            writer.add_scalar(f'{phase}{run}/Loss', epoch_loss, epoch)\n            writer.add_scalar(f'{phase}{run}/Accuracy', epoch_acc, epoch)\n            writer.add_scalar(f'{phase}{run}/F1', epoch_f1, epoch)\n            writer.add_scalar(f'{phase}{run}/Precision', epoch_precision, epoch)\n            writer.add_scalar(f'{phase}{run}/Recall', epoch_recall, epoch)\n            \n            # Log the learning rate as a scalar\n            current_lr = optimizer.param_groups[0]['lr']\n            writer.add_scalar(f'{phase}{run}/Learning_Rate', current_lr, epoch)\n            \n            if phase == 'test' and epoch_f1 > best_f1:\n                best_f1 = epoch_f1\n                torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase_{run}_best.pth')  # Save the model\n\n        \n        print()\n        exp_lr_scheduler.step()\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n#     torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase{run}freezing_final_epoch_{num_epochs}.pth')  # Save the model\n#     best_model_wts = copy.deepcopy(model.state_dict())\n#     model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:43:17.303822Z","iopub.execute_input":"2024-09-14T15:43:17.304223Z","iopub.status.idle":"2024-09-14T15:43:17.329982Z","shell.execute_reply.started":"2024-09-14T15:43:17.304187Z","shell.execute_reply":"2024-09-14T15:43:17.328742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Increase the message rate limit for printing training progress\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager().update('notebook', {\n    \"NotebookApp\": {\n        \"iopub_msg_rate_limit\": 10000,  # Increase to 10000 messages/sec\n        \"rate_limit_window\": 10.0,      # Increase the rate limit window to 10 seconds\n    }\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:43:18.435669Z","iopub.execute_input":"2024-09-14T15:43:18.436071Z","iopub.status.idle":"2024-09-14T15:43:18.456743Z","shell.execute_reply.started":"2024-09-14T15:43:18.436034Z","shell.execute_reply":"2024-09-14T15:43:18.455828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\n\ndescription = (\n    f\"Training parameters for RAFDB on Resnet50:\\n\"\n    f\"Model: {model.__class__.__name__}\\n\"\n    f\"Criterion: {criterion.__class__.__name__}\\n\"\n    f\"Optimizer: {optimizer.__class__.__name__}\\n\"\n    f\"Scheduler: {exp_lr_scheduler.__class__.__name__}\\n\"\n    f\"Device: {device}\\n\"\n    f\"Number of epochs: {num_epochs}\\n\"\n    f\"Batch Size: {batch_size}\\n\"\n#     f\"Stride: {stride}\\n\"\n#     f\"Sequence Length: {sequence_length}\"\n)\nwriter.add_text(f\"Desc_{run}/Training Parameters\", description, global_step=0)\nmodel_ft = train_model(model,criterion, optimizer, exp_lr_scheduler, dataloaders = None, dataset_sizes = None, device = device, writer = writer, run = run, num_epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:43:39.678229Z","iopub.execute_input":"2024-09-14T15:43:39.678999Z","iopub.status.idle":"2024-09-14T16:08:20.980979Z","shell.execute_reply.started":"2024-09-14T15:43:39.678957Z","shell.execute_reply":"2024-09-14T16:08:20.979438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\n\ntrain_loader, train_len, dataset_train = get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = 0)\nval_loader, val_len, dataset_val = get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = 0)\n\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nwriter = SummaryWriter(log_dir='experiment_1')\nnum_epochs = 20\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Initialize tqdm progress bar for training loop\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", unit=\"batch\")\n    \n    # Track metrics for training\n    all_train_preds = []\n    all_train_labels = []\n    \n    for inputs1, inputs2, labels in train_loader_tqdm:\n        inputs = inputs1\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Store predictions and labels\n        _, predicted = torch.max(outputs.data, 1)\n        all_train_preds.extend(predicted.cpu().numpy())\n        all_train_labels.extend(labels.cpu().numpy())\n\n        # Update tqdm description\n        train_loader_tqdm.set_postfix(loss=f\"{running_loss / len(train_loader):.4f}\")\n\n    # Calculate training metrics\n    train_accuracy = 100 * sum(np.array(all_train_preds) == np.array(all_train_labels)) / len(all_train_labels)\n    train_precision = precision_score(all_train_labels, all_train_preds, average='macro')\n    train_recall = recall_score(all_train_labels, all_train_preds, average='macro')\n    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n\n    # Log training metrics\n    writer.add_scalar('Loss/train', running_loss/len(train_loader), epoch)\n    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n    writer.add_scalar('Precision/train', train_precision, epoch)\n    writer.add_scalar('Recall/train', train_recall, epoch)\n    writer.add_scalar('F1 Score/train', train_f1, epoch)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n\n    # Validation phase\n    model.eval()\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    # Initialize tqdm progress bar for validation loop\n    val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\", unit=\"batch\")\n    \n    with torch.no_grad():\n        for inputs1, inputs2, labels in val_loader_tqdm:\n            inputs = inputs1\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # Collect all predictions and labels for metric calculation\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate validation metrics\n    val_accuracy = 100 * correct / total\n    val_precision = precision_score(all_labels, all_preds, average='macro')\n    val_recall = recall_score(all_labels, all_preds, average='macro')\n    val_f1 = f1_score(all_labels, all_preds, average='macro')\n#     current_lr = exp_lr_scheduler.get_last_lr()\n\n    # Log validation metrics\n    writer.add_scalar('Loss/val', running_loss/len(train_loader), epoch)\n    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n    writer.add_scalar('Precision/val', val_precision, epoch)\n    writer.add_scalar('Recall/val', val_recall, epoch)\n    writer.add_scalar('F1 Score/val', val_f1, epoch)\n\n    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n    print(f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")\n\n# Close the SummaryWriter after training is done\nwriter.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:39:35.048813Z","iopub.execute_input":"2024-09-14T15:39:35.049785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" torch.save(model.state_dict(), f'/kaggle/working/trained_model/rafDB_Fiducial_14_epochs.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:25:22.467915Z","iopub.execute_input":"2024-09-10T11:25:22.468582Z","iopub.status.idle":"2024-09-10T11:25:22.987661Z","shell.execute_reply.started":"2024-09-10T11:25:22.468549Z","shell.execute_reply":"2024-09-10T11:25:22.986872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = torch.cat((a, b), dim = 0)\nlabels = torch.cat((c, c), dim = 0)\n\ninputs.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:49:56.937475Z","iopub.execute_input":"2024-09-03T10:49:56.937859Z","iopub.status.idle":"2024-09-03T10:49:56.951805Z","shell.execute_reply.started":"2024-09-03T10:49:56.937826Z","shell.execute_reply":"2024-09-03T10:49:56.950919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\na.unsqueeze(0).shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:41:58.118878Z","iopub.execute_input":"2024-09-03T10:41:58.119701Z","iopub.status.idle":"2024-09-03T10:41:58.125761Z","shell.execute_reply.started":"2024-09-03T10:41:58.119666Z","shell.execute_reply":"2024-09-03T10:41:58.124814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = inputs.to(device)\nt = model(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:50:01.431168Z","iopub.execute_input":"2024-09-03T10:50:01.431639Z","iopub.status.idle":"2024-09-03T10:50:01.674753Z","shell.execute_reply.started":"2024-09-03T10:50:01.431606Z","shell.execute_reply":"2024-09-03T10:50:01.674013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:50:05.470063Z","iopub.execute_input":"2024-09-03T10:50:05.470455Z","iopub.status.idle":"2024-09-03T10:50:05.476437Z","shell.execute_reply.started":"2024-09-03T10:50:05.470417Z","shell.execute_reply":"2024-09-03T10:50:05.475523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Independent Validation","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:13:35.136848Z","iopub.execute_input":"2024-09-10T09:13:35.137967Z","iopub.status.idle":"2024-09-10T09:13:35.644144Z","shell.execute_reply.started":"2024-09-10T09:13:35.13792Z","shell.execute_reply":"2024-09-10T09:13:35.643106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_seqs, val_labels = sequence_extractor(\n    data_dict = data_builder(val_label_path, train = False),\n    data_path = data_path,\n    min_stride = 1,\n    sequence_length = 5,\n    train = False\n)\n\nval_dataset = ABAWFeatureDataset(val_seqs, val_labels, transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n    ])\n)\n\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle = True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:05.277394Z","iopub.execute_input":"2024-08-19T09:28:05.277779Z","iopub.status.idle":"2024-08-19T09:28:07.581753Z","shell.execute_reply.started":"2024-08-19T09:28:05.277749Z","shell.execute_reply":"2024-08-19T09:28:07.580948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"running_loss = 0.0\nrunning_corrects = 0.0\nall_preds = []\nall_labels = []\n            \n    \nfor inputs, labels in tqdm(data_loader_val):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n#     optimizer.zero_grad()\n\n    with torch.no_grad():\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n\n    running_loss += loss.item() * inputs.size(0)\n    running_corrects += torch.sum(preds == labels.data)\n\n    all_preds.extend(preds.cpu().numpy())\n    all_labels.extend(labels.cpu().numpy())\n\nepoch_loss = running_loss / dataset_sizes['val']\nepoch_acc = running_corrects.double() / dataset_sizes['val']\n\n# Calculate F1 score, precision, and recall\nepoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n\nprint(f\"{'val'} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:11.406099Z","iopub.execute_input":"2024-08-19T09:28:11.40643Z","iopub.status.idle":"2024-08-19T09:30:41.886717Z","shell.execute_reply.started":"2024-08-19T09:28:11.406405Z","shell.execute_reply":"2024-08-19T09:30:41.885835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Example list of image paths\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nimage_paths = [os.path.join(data_path, x) for x in train_seqs[0]]\nlabels = [train_labels[0] for x in train_seqs[0]]\n# Number of images\nn_images = len(image_paths)\n\n# Calculate grid size (assuming a square grid)\ngrid_size = int(n_images**0.5) + (n_images**0.5 != int(n_images**0.5))\n\n# Create a figure with a grid of subplots\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n\n# Flatten the axes array for easy iteration\naxes = axes.flatten()\n\n# Iterate over images and plot them with labels\nfor i, (image_path, label) in enumerate(zip(image_paths, labels)):\n    img = mpimg.imread(image_path)\n    axes[i].imshow(img)\n    axes[i].axis('off')  # Hide axes\n    axes[i].set_title(label, fontsize=12)  # Add label\n\n# Hide any remaining empty subplots\nfor j in range(i+1, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:12:32.517175Z","iopub.execute_input":"2024-08-14T10:12:32.518098Z","iopub.status.idle":"2024-08-14T10:12:33.938161Z","shell.execute_reply.started":"2024-08-14T10:12:32.518062Z","shell.execute_reply":"2024-08-14T10:12:33.937147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Facial Landmark detector on cuda","metadata":{}},{"cell_type":"code","source":"# app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n# app.prepare(ctx_id=0, det_size=(640, 640))  # Set detection size, though not used directly for cropped images\n\n# Example usage\ncropped_face_path = '/kaggle/input/raf-db-trial/92_86/92/dataset/RAF/test_0001.jpg'\ncropped_face = cv2.imread(cropped_face_path)\ntop = 100\nbottom = 100\nleft = 100\nright = 100\n\n# Pad the image\npadded_image = cv2.copyMakeBorder(\n    cropped_face,\n    top,\n    bottom,\n    left,\n    right,\n    cv2.BORDER_CONSTANT,  # Border type\n    value=[0, 0, 0]       # Padding color (black in this case)\n)\n# cv2_imshow(padded_image)\nfor i in range(10):\n    out = app.get(padded_image)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:17:43.001508Z","iopub.execute_input":"2024-09-03T10:17:43.002108Z","iopub.status.idle":"2024-09-03T10:17:48.322154Z","shell.execute_reply.started":"2024-09-03T10:17:43.002078Z","shell.execute_reply":"2024-09-03T10:17:48.320925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build a dynamic FER model which takes video ","metadata":{}}]}