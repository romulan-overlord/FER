{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9350843,"sourceType":"datasetVersion","datasetId":5081678}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install onnxruntime-gpu insightface","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T06:36:48.474947Z","iopub.execute_input":"2024-09-09T06:36:48.475286Z","iopub.status.idle":"2024-09-09T06:37:11.215587Z","shell.execute_reply.started":"2024-09-09T06:36:48.475257Z","shell.execute_reply":"2024-09-09T06:37:11.214586Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nGet:2 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]   \nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1673 kB]\nGet:4 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:5 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nHit:6 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:7 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nGet:8 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [26.3 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:10 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1517 kB]\nGet:11 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3232 kB]\nGet:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1270 kB]\nHit:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1555 kB]\nGet:15 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3937 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4079 kB]\nGet:19 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3928 kB]\nGet:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4399 kB]\nFetched 25.9 MB in 4s (6723 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngnupg is already the newest version (2.2.19-3ubuntu2.2).\nThe following packages will be upgraded:\n  wget\n1 upgraded, 0 newly installed, 0 to remove and 97 not upgraded.\nNeed to get 349 kB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 wget amd64 1.20.3-1ubuntu2.1 [349 kB]\nFetched 349 kB in 1s (262 kB/s)\n(Reading database ... 113807 files and directories currently installed.)\nPreparing to unpack .../wget_1.20.3-1ubuntu2.1_amd64.deb ...\nUnpacking wget (1.20.3-1ubuntu2.1) over (1.20.3-1ubuntu2) ...\nSetting up wget (1.20.3-1ubuntu2.1) ...\nProcessing triggers for install-info (6.7.0.dfsg.2-5) ...\nProcessing triggers for man-db (2.9.1-1) ...\n--2024-09-09 06:37:02--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\nResolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\nConnecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 190 [application/octet-stream]\nSaving to: 'cuda-ubuntu2004.pin'\n\ncuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n\n2024-09-09 06:37:03 (4.52 MB/s) - 'cuda-ubuntu2004.pin' saved [190/190]\n\n--2024-09-09 06:37:05--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\nResolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\nConnecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 190 [application/octet-stream]\nSaving to: 'cuda-ubuntu2004.pin'\n\ncuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n\n2024-09-09 06:37:05 (4.97 MB/s) - 'cuda-ubuntu2004.pin' saved [190/190]\n\nExecuting: /tmp/apt-key-gpghome.eSvE97k8g5/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 7fa2af80\ngpg: key F60F4B3D7FA2AF80: public key \"cudatools <cudatools@nvidia.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1\nHit:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\nHit:2 https://packages.cloud.google.com/apt gcsfuse-focal InRelease            \nHit:3 https://packages.cloud.google.com/apt cloud-sdk InRelease                \nHit:4 http://archive.ubuntu.com/ubuntu focal InRelease              \nHit:5 http://security.ubuntu.com/ubuntu focal-security InRelease    \nHit:6 https://packages.cloud.google.com/apt google-fast-socket InRelease\nHit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\nHit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nReading package lists... Done\nW: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:50 and /etc/apt/sources.list.d/cuda.list:1\nW: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:50 and /etc/apt/sources.list.d/cuda.list:1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\nimport pandas as pd\nimport time\nimport copy\nfrom collections import Counter\nimport ast\n\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.init as init\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import KFold\n\nfrom timm.scheduler.scheduler import Scheduler\nfrom timm.loss import LabelSmoothingCrossEntropy\nimport timm\n\nfrom torchvision import transforms as T\nimport torchvision\n\nimport cv2\n# import insightface\n# from insightface.app import FaceAnalysis\n# from insightface.data import get_image as ins_get_image\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:28:54.642105Z","iopub.execute_input":"2024-09-10T08:28:54.642881Z","iopub.status.idle":"2024-09-10T08:28:54.651920Z","shell.execute_reply.started":"2024-09-10T08:28:54.642847Z","shell.execute_reply":"2024-09-10T08:28:54.650896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation for ABAW","metadata":{}},{"cell_type":"code","source":"def data_builder(label_path, train = True):\n    # read annotations\n    df = pd.read_csv(label_path, names = ['image', 'val', 'arousal', 'expr', 'au1', 'au2', 'au3', 'au4', 'au5', 'au6', 'au7', 'au8', 'au9', 'au10', 'au11', 'au12'] )\n    # Remove the first row\n    df = df.drop(index=0)\n\n    # reset index\n    df = df.reset_index(drop=True)\n\n    df[['folder', 'image']] = df['image'].str.split('/', expand=True)\n\n    grouped = df.groupby(['folder'], as_index= False).apply(lambda x: x.sort_values('image'))\n    grouped['expr'] = grouped['expr'].astype(int)\n    grouped = grouped.reset_index(drop=True)\n\n    # generating data dictionary\n    data_dict = {}\n    emo = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness','Surprise', 'Other']\n    seq = 0\n    current = -1\n    for index, row in grouped.iterrows():\n        if row['expr'] != current:\n            current = row['expr']\n            if train:\n                seq += 1\n\n        if row['expr'] == -1:\n            continue\n\n        if row['folder'] in data_dict:\n            data_dict[row['folder']].append(\n                (os.path.join(row['folder'], row['image']), row['expr'])\n            )\n        else:\n            data_dict[row['folder']] = [(os.path.join(row['folder'], row['image']), row['expr']), ]\n            \n    return data_dict\n\ndef most_common_element(lst):\n    if not lst:\n        return None  # Return None if the list is empty\n    counter = Counter(lst)\n    most_common = counter.most_common(1)[0]  # Get the most common element\n    return most_common[0]\n\ndef sequence_extractor(data_dict, data_path, min_stride = 5, sequence_length = 5, train = True):\n    train_seqs = []\n    train_labels = []\n    for folder in data_dict.keys():\n        s_len = len(data_dict[folder])\n        max_start_index = s_len - (sequence_length - 1) * min_stride\n        for i in range(0, max_start_index, sequence_length * min_stride):\n            sublist = [os.path.join(data_path, data_dict[folder][i + j * min_stride][0]) for j in range(5)]\n            train_seqs.append(sublist)\n            t_lab = [data_dict[folder][i + j * min_stride][1] for j in range(5)]\n\n            train_labels.append(most_common_element(t_lab))\n    return train_seqs, train_labels\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T15:40:06.331599Z","iopub.execute_input":"2024-09-02T15:40:06.332323Z","iopub.status.idle":"2024-09-02T15:40:06.348316Z","shell.execute_reply.started":"2024-09-02T15:40:06.332293Z","shell.execute_reply":"2024-09-02T15:40:06.347189Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# len(data_dict['107']['Other0'])\nval_seq = []\nval_labels = []\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\n\n\ndef most_common_element(lst):\n    if not lst:\n        return None  # Return None if the list is empty\n    counter = Counter(lst)\n    most_common = counter.most_common(1)[0]  # Get the most common element\n    return most_common[0]\n\nstride = 4\nfor key in data_dict.keys():\n    for exp in data_dict[key].keys():\n        n = len(data_dict[key][exp])\n        # Calculate the maximum starting index that allows creating a sublist of length 5\n        max_start_index = n - (5 - 1) * stride\n        for i in range(0, max_start_index, 5 * stride):\n            sublist = [os.path.join(data_path, data_dict[key][exp][i + j * stride][0]) for j in range(5)]\n            val_seq.append(sublist)\n            t_lab = [data_dict[key][exp][i + j * stride][1] for j in range(5)]\n            \n            val_labels.append(most_common_element(t_lab))\n\n\nlen(val_seq)\n# data_dict['107']['Other0'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-22T05:47:10.971760Z","iopub.execute_input":"2024-08-22T05:47:10.972147Z","iopub.status.idle":"2024-08-22T05:47:12.039884Z","shell.execute_reply.started":"2024-08-22T05:47:10.972117Z","shell.execute_reply":"2024-08-22T05:47:12.038744Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m most_common[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m data_dict[key]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     17\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_dict[key][exp])\n","\u001b[0;31mNameError\u001b[0m: name 'data_dict' is not defined"],"ename":"NameError","evalue":"name 'data_dict' is not defined","output_type":"error"}]},{"cell_type":"code","source":"class ABAWFeatureDataset(Dataset):\n    def __init__(self, features, labels, transform = None):\n        self.features = features\n        self.labels = labels\n        self.length = len(labels)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        frames = [Image.open(i).convert('RGB') for i in self.features[idx]]\n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n        frames = torch.stack(frames)        # Stack frames to form a 4D tensor (T, C, H, W)\n        frames = frames.permute(1, 0, 2, 3)  # Change order to (C, T, H, W)\n        label = self.labels[idx]\n        return frames, label","metadata":{"execution":{"iopub.status.busy":"2024-09-02T15:40:12.423934Z","iopub.execute_input":"2024-09-02T15:40:12.424320Z","iopub.status.idle":"2024-09-02T15:40:12.432353Z","shell.execute_reply.started":"2024-09-02T15:40:12.424291Z","shell.execute_reply":"2024-09-02T15:40:12.430897Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0):\n\n    dataset_train = RAFDBDataset(choose=\"train\",\n        data_path=data_path,\n        label_path=label_path,\n        app = None,\n        transform = None,\n        img_size = 224,\n        num_epochs = num_epochs,\n        epochs = epochs\n                                 \n    )\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        batch_size=batch_size,\n        drop_last=True,\n    )\n    return data_loader_train, dataset_train.length, dataset_train\n\n\ndef get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0):\n\n    dataset_val = RAFDBDataset(choose=\"test\",\n        data_path=data_path,\n        label_path=label_path,\n        app = None,        \n        transform = None,\n        img_size = 224,\n        num_epochs = num_epochs,\n        epochs = epochs\n    )\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False\n    )\n    return data_loader_val, dataset_val.length, dataset_val","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:19:00.987742Z","iopub.execute_input":"2024-09-10T09:19:00.988393Z","iopub.status.idle":"2024-09-10T09:19:00.996124Z","shell.execute_reply.started":"2024-09-10T09:19:00.988360Z","shell.execute_reply":"2024-09-10T09:19:00.995248Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"landmarks_cache = {}\n\n# Open the file and read all lines first to calculate total length for progress bar\nwith open('/kaggle/input/raf-db-trial/keypoints.txt', 'r') as file:\n    lines = file.readlines()\n\n# Iterate over lines with progress bar\nfor line in tqdm(lines, desc=\"Processing landmarks\", unit=\"lines\"):\n    # Split at the first occurrence of the colon\n    file_name, coordinates = line.split(\":\", 1)\n\n    # Remove whitespace around the file name\n    file_name = file_name.strip()\n\n    # Convert the string representation of the list of tuples to an actual list of tuples\n    coordinates = ast.literal_eval(coordinates.strip())\n\n    # Add to the dictionary\n    landmarks_cache[file_name] = coordinates","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:29:04.465930Z","iopub.execute_input":"2024-09-10T08:29:04.466660Z","iopub.status.idle":"2024-09-10T08:29:20.898470Z","shell.execute_reply.started":"2024-09-10T08:29:04.466627Z","shell.execute_reply":"2024-09-10T08:29:20.897540Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing landmarks:   0%|          | 0/15308 [00:00<?, ?lines/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154ea0763a75458da028854d039df0ce"}},"metadata":{}}]},{"cell_type":"code","source":"for key in landmarks_cache.keys():\n    break\nkey","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:20:13.825469Z","iopub.execute_input":"2024-09-10T08:20:13.825918Z","iopub.status.idle":"2024-09-10T08:20:13.834573Z","shell.execute_reply.started":"2024-09-10T08:20:13.825886Z","shell.execute_reply":"2024-09-10T08:20:13.833122Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'train_05914.jpg'"},"metadata":{}}]},{"cell_type":"code","source":"type(landmarks_cache['train_05914.jpg'][0])","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:20:28.198379Z","iopub.execute_input":"2024-09-10T08:20:28.198784Z","iopub.status.idle":"2024-09-10T08:20:28.205786Z","shell.execute_reply.started":"2024-09-10T08:20:28.198753Z","shell.execute_reply":"2024-09-10T08:20:28.204700Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tuple"},"metadata":{}}]},{"cell_type":"code","source":"class RAFDBDataset(Dataset):\n    def __init__(self, choose, data_path, label_path, app, transform=None, img_size=224, num_epochs = 20, epochs = 0):\n        self.image_paths = []\n        self.labels = []\n        self.data_path = data_path\n        self.label_path = label_path\n        self.app = app\n        self.landmarks_cache = landmarks_cache  # Cache to store landmark coordinates\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.num_epoch = num_epochs\n        self.epochs = epochs\n        self.choose = choose\n\n        if transform:\n            self.transform = transform\n        else:\n            self.transform = T.Compose([\n                T.Resize(256),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n            ])\n\n        self.train = True if choose == \"train\" else False\n\n        if choose == \"train\" or choose == \"test\":\n            with open(self.label_path, \"r\") as f:\n                data = f.readlines()\n\n            for i in range(0, len(data)):\n                line = data[i].strip('\\n').split(\" \")\n\n                image_name = line[0]\n                sample_temp = image_name.split(\"_\")[0]\n\n                if self.train and sample_temp == \"train\":\n                    image_path = os.path.join(self.data_path, image_name)\n                    self.image_paths.append(image_path)\n                    self.labels.append(int(line[1]) - 1)\n\n                elif not self.train and sample_temp == \"test\":\n                    image_path = os.path.join(self.data_path, image_name)\n                    self.image_paths.append(image_path)\n                    self.labels.append(int(line[1]) - 1)\n                    \n        self.length = len(self.labels)\n        self.labels = np.asarray(self.labels)\n\n    def draw_squares_on_landmarks(self, img, landmarks, n):\n        dimg = img.copy()\n        half_n = n // 2\n        for kp in landmarks:\n            # Ensure coordinates are integers\n            top_left = (int(kp[0] - half_n), int(kp[1] - half_n))\n            bottom_right = (int(kp[0] + half_n), int(kp[1] + half_n))\n            cv2.rectangle(dimg, top_left, bottom_right, (0, 0, 0), -1)  # -1 fills the rectangle\n        return dimg\n\n    def get_landmarks(self, img_path):\n        if img_path not in self.landmarks_cache:\n            return None\n            img = cv2.imread(img_path)\n            nimg = cv2.resize(img, (224,224))\n            k = 100\n            padded_image = cv2.copyMakeBorder(\n                nimg,\n                k,\n                k,\n                k,\n                k,\n                cv2.BORDER_CONSTANT,  # Border type\n                value=[0, 0, 0]       # Padding color (black in this case)\n            )\n#             padded_image = padded_image.to(self.device)  # Ensure the image is on the right device\n            out = self.app.get(padded_image)\n            \n            if len(out) == 0:  # Check if any faces were detected\n                return None\n            \n            landmarks = out[0].landmark_2d_106.astype(np.int64)\n            self.landmarks_cache[img_path] = landmarks\n        return self.landmarks_cache[img_path]\n\n    def augment(self, img_path):\n        img = cv2.imread(img_path)\n        nimg = cv2.resize(img, (224, 224))\n        landmarks = self.get_landmarks(img_path.split('/')[-1])\n        \n        if landmarks is None:  # Skip if no faces are detected\n            return nimg, nimg\n        \n        k = 100\n        padded_image = cv2.copyMakeBorder(\n            nimg,\n            k,\n            k,\n            k,\n            k,\n            cv2.BORDER_CONSTANT,  # Border type\n            value=[0, 0, 0]       # Padding color (black in this case)\n        )\n        \n        # Use landmarks to draw squares on the padded image\n        aug = self.draw_squares_on_landmarks(padded_image, landmarks, n=max(0,self.num_epoch - self.epochs - 5))\n        height, width = padded_image.shape[:2]\n        cropped_image = aug[k:height-k, k:width-k]\n        return nimg, cropped_image\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        if self.choose == \n        img1, img2 = self.augment(img_path)\n        \n        if self.transform:\n            img1 = self.transform(Image.fromarray(img1))\n            img2 = self.transform(Image.fromarray(img2))\n            \n        img1 = img1.unsqueeze(1)\n        img2 = img2.unsqueeze(1)\n        label = torch.tensor(self.labels[idx])\n        return img1, img2, label\n\n    def __len__(self):\n        return self.length","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:16:18.771478Z","iopub.execute_input":"2024-09-10T09:16:18.772145Z","iopub.status.idle":"2024-09-10T09:16:18.795512Z","shell.execute_reply.started":"2024-09-10T09:16:18.772111Z","shell.execute_reply":"2024-09-10T09:16:18.794487Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/raf-db-trial/92_86/92/dataset/RAF'\nlabel_path = '/kaggle/input/raf-db-trial/92_86/92/dataset/list_patition_label.txt'\nbatch_size = 16\n\n# data_loader_train, train_len, d = get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:30:26.920837Z","iopub.execute_input":"2024-09-10T08:30:26.921658Z","iopub.status.idle":"2024-09-10T08:30:26.925637Z","shell.execute_reply.started":"2024-09-10T08:30:26.921625Z","shell.execute_reply":"2024-09-10T08:30:26.924766Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"t = d.augment('/kaggle/input/raf-db-trial/92_86/92/dataset/RAF/test_0001.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:23:21.492870Z","iopub.execute_input":"2024-09-10T08:23:21.493237Z","iopub.status.idle":"2024-09-10T08:23:21.504634Z","shell.execute_reply.started":"2024-09-10T08:23:21.493210Z","shell.execute_reply":"2024-09-10T08:23:21.503226Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"type(t[1])","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:23:41.476839Z","iopub.execute_input":"2024-09-10T08:23:41.477215Z","iopub.status.idle":"2024-09-10T08:23:41.484937Z","shell.execute_reply.started":"2024-09-10T08:23:41.477187Z","shell.execute_reply":"2024-09-10T08:23:41.483706Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"cell_type":"markdown","source":"# Code below is for ABAW, code above is for RAFDB","metadata":{}},{"cell_type":"code","source":"train_label_path = '/kaggle/input/abaw-7-dataset/training_set_annotations.txt'\nval_label_path = '/kaggle/input/abaw-7-dataset/validation_set_annotations.txt'\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\n\nbatch_size = 8\nstride = 5\nsequence_length = 10\n\ntrain_seqs, train_labels = sequence_extractor(\n    data_dict = data_builder(train_label_path, train = False),\n    data_path = data_path,\n    min_stride = stride,\n    sequence_length = sequence_length\n)\n\nval_seqs, val_labels = sequence_extractor(\n    data_dict = data_builder(val_label_path, train = False),\n    data_path = data_path,\n    min_stride = stride,\n    sequence_length = sequence_length,\n    train = False\n)\n\ntrain_dataset = ABAWFeatureDataset(train_seqs, train_labels, transform = T.Compose([\n        T.RandomHorizontalFlip(p=0.5),  # Flip the frame with a probability of 0.5\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),  # Converts the image to a tensor and normalizes to [0, 1]\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD),  # Normalization using ImageNet mean and std\n    ])\n)\nval_dataset = ABAWFeatureDataset(val_seqs, val_labels, transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n    ])\n)\n\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle = True)\n\ndataloaders = {\n    \"train\": data_loader_train,\n    \"test\": data_loader_val\n}\n\ndataset_sizes = {\n    \"train\": len(train_dataset),\n    \"test\": len(val_dataset)\n}\n\ndataset_sizes","metadata":{"execution":{"iopub.status.busy":"2024-09-02T15:40:54.406455Z","iopub.execute_input":"2024-09-02T15:40:54.407410Z","iopub.status.idle":"2024-09-02T15:41:06.088221Z","shell.execute_reply.started":"2024-09-02T15:40:54.407378Z","shell.execute_reply":"2024-09-02T15:41:06.087044Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3951147203.py:3: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(label_path, names = ['image', 'val', 'arousal', 'expr', 'au1', 'au2', 'au3', 'au4', 'au5', 'au6', 'au7', 'au8', 'au9', 'au10', 'au11', 'au12'] )\n/tmp/ipykernel_34/3951147203.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  grouped = df.groupby(['folder'], as_index= False).apply(lambda x: x.sort_values('image'))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_seqs, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43msequence_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_label_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m val_seqs, val_labels \u001b[38;5;241m=\u001b[39m sequence_extractor(\n\u001b[1;32m     17\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m data_builder(val_label_path, train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m data_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ABAWFeatureDataset(train_seqs, train_labels, transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     25\u001b[0m         T\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# Flip the frame with a probability of 0.5\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         T\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     ])\n\u001b[1;32m     31\u001b[0m )\n","Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36msequence_extractor\u001b[0;34m(data_dict, data_path, min_stride, sequence_length, train)\u001b[0m\n\u001b[1;32m     50\u001b[0m s_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_dict[folder])\n\u001b[1;32m     51\u001b[0m max_start_index \u001b[38;5;241m=\u001b[39m s_len \u001b[38;5;241m-\u001b[39m (sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m min_stride\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_stride\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     53\u001b[0m     sublist \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, data_dict[folder][i \u001b[38;5;241m+\u001b[39m j \u001b[38;5;241m*\u001b[39m min_stride][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n\u001b[1;32m     54\u001b[0m     train_seqs\u001b[38;5;241m.\u001b[39mappend(sublist)\n","\u001b[0;31mValueError\u001b[0m: range() arg 3 must not be zero"],"ename":"ValueError","evalue":"range() arg 3 must not be zero","output_type":"error"}]},{"cell_type":"code","source":"model = torchvision.models.video.swin3d_b(weights=\"KINETICS400_IMAGENET22K_V1\")\n\n# model = torchvision.models.video.swin3d_b(weights=None)\n\n# state_dict = torch.load('/kaggle/working/trained_model/freezing_epoch_10.pth', map_location=torch.device('cpu'))\n\n# Step 3: Apply the loaded weights to the model\n# model.load_state_dict(state_dict)\n\nnum_ftrs = model.head.in_features # Get the input features of the current head\n\n# Step 2: Create a new head\nnew_head = nn.Sequential(\n nn.Linear(num_ftrs, 7) # Final layer with 3 output units (for 3 classes)\n)\n\nmodel.head = new_head\n\n# Freeze all layers first\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last few layers\nfor name, param in model.named_parameters():\n    if any(layer_name in name for layer_name in [\"features.6\", \"norm\", \"avgpool\", \"head\"]):\n        param.requires_grad = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)\n\n# app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n# app.prepare(ctx_id=0, det_size=(640, 640))\n# app.to(device)\n\nrun = 9\n\n# labels_np = np.array(train_labels)\n# class_counts = np.bincount(labels_np)          # Generate class counts\n# class_weights = 1.0 / class_counts             # Calculate inverse of class counts\n# class_weights = class_weights / class_weights.sum()            # Normalize the weights to sum to 1 (optional)\n# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n# criterion = LabelSmoothingCrossEntropy()\ncriterion = nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\noptimizer = optim.AdamW(model.head.parameters(), lr=0.001)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)\nwriter = SummaryWriter(f'run{run}')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:15:39.750302Z","iopub.execute_input":"2024-09-10T09:15:39.751089Z","iopub.status.idle":"2024-09-10T09:15:42.085238Z","shell.execute_reply.started":"2024-09-10T09:15:39.751055Z","shell.execute_reply":"2024-09-10T09:15:42.084422Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"os.remove('/kaggle/working/run9/events.out.tfevents.1725358595.da10f4d75707.34.0')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:15:28.125501Z","iopub.execute_input":"2024-09-10T09:15:28.126385Z","iopub.status.idle":"2024-09-10T09:15:28.132724Z","shell.execute_reply.started":"2024-09-10T09:15:28.126351Z","shell.execute_reply":"2024-09-10T09:15:28.131121Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T14:48:36.390541Z","iopub.execute_input":"2024-08-17T14:48:36.390925Z","iopub.status.idle":"2024-08-17T14:48:36.399794Z","shell.execute_reply.started":"2024-08-17T14:48:36.390894Z","shell.execute_reply":"2024-08-17T14:48:36.398874Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device, writer, run, num_epochs=20, app=None):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n        \n        data_loader_train, train_len, dataset_train = get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = epoch)\n        data_loader_val, val_len, dataset_val = get_analysis_val_dataloader(data_path, label_path, batch_size, num_epochs = num_epochs, epochs = epoch)\n\n        dataloaders = {\n            \"train\": data_loader_train,\n            \"test\": data_loader_val\n        }\n\n        dataset_sizes = {\n            \"train\": train_len,\n            \"test\": val_len\n        }\n        \n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            all_preds = []\n            all_labels = []\n            \n            for input1, input2, labels in tqdm(dataloaders[phase]):\n                inputs = torch.cat((input1, input2), dim = 0)\n                labels = torch.cat((labels, labels), dim = 0)\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n            \n#             if phase == 'train' and (epoch+1) % 5 == 0:\n#                 scheduler.step()\n#                 torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase6_freezing_epoch_{epoch}.pth')\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / (dataset_sizes[phase]*2)\n            # Calculate F1 score, precision, and recall\n            epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n            \n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            \n            # Log the metrics\n            writer.add_scalar(f'{phase}{run}/Loss', epoch_loss, epoch)\n            writer.add_scalar(f'{phase}{run}/Accuracy', epoch_acc, epoch)\n            writer.add_scalar(f'{phase}{run}/F1', epoch_f1, epoch)\n            writer.add_scalar(f'{phase}{run}/Precision', epoch_precision, epoch)\n            writer.add_scalar(f'{phase}{run}/Recall', epoch_recall, epoch)\n            \n            # Log the learning rate as a scalar\n            current_lr = optimizer.param_groups[0]['lr']\n            writer.add_scalar(f'{phase}6/Learning_Rate', current_lr, epoch)\n        \n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase6{run}freezing_final_epoch_{num_epochs}.pth')  # Save the model\n    best_model_wts = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:17:53.622775Z","iopub.execute_input":"2024-09-10T09:17:53.623099Z","iopub.status.idle":"2024-09-10T09:17:53.640545Z","shell.execute_reply.started":"2024-09-10T09:17:53.623074Z","shell.execute_reply":"2024-09-10T09:17:53.639636Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Increase the message rate limit for printing training progress\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager().update('notebook', {\n    \"NotebookApp\": {\n        \"iopub_msg_rate_limit\": 10000,  # Increase to 10000 messages/sec\n        \"rate_limit_window\": 10.0,      # Increase the rate limit window to 10 seconds\n    }\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:15:45.548493Z","iopub.execute_input":"2024-09-10T09:15:45.548866Z","iopub.status.idle":"2024-09-10T09:15:45.555450Z","shell.execute_reply.started":"2024-09-10T09:15:45.548836Z","shell.execute_reply":"2024-09-10T09:15:45.554190Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\n\ndescription = (\n    f\"Training parameters:\\n\"\n    f\"Model: {model.__class__.__name__}\\n\"\n    f\"Criterion: {criterion.__class__.__name__}\\n\"\n    f\"Optimizer: {optimizer.__class__.__name__}\\n\"\n    f\"Scheduler: {exp_lr_scheduler.__class__.__name__}\\n\"\n    f\"Device: {device}\\n\"\n    f\"Number of epochs: {num_epochs}\\n\"\n    f\"Batch Size: {batch_size}\\n\"\n#     f\"Stride: {stride}\\n\"\n#     f\"Sequence Length: {sequence_length}\"\n)\nwriter.add_text(f\"Desc_{run}/Training Parameters\", description, global_step=0)\nmodel_ft = train_model(model,criterion, optimizer, exp_lr_scheduler, dataloaders = None, dataset_sizes = None, device = device, writer = writer, run = run, num_epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:19:08.347146Z","iopub.execute_input":"2024-09-10T09:19:08.347823Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ea0890723e46f8b86c768f8ff65db6"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.5393 Acc: 0.7895 F1: 0.7850 Precision: 0.7849 Recall: 0.7905\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ebfcf9ec2f491d9e802ba548e911b7"}},"metadata":{}},{"name":"stdout","text":"test Loss: 28.1276 Acc: 0.2216 F1: 0.0804 Precision: 0.0491 Recall: 0.2216\n\nEpoch 1/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3d913ae8354517ba1a85aae9716d89"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.7671 Acc: 0.8059 F1: 0.8028 Precision: 0.8006 Recall: 0.8069\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14020ceabd774d1ea870a8e16469030a"}},"metadata":{}},{"name":"stdout","text":"test Loss: 14.9922 Acc: 0.3390 F1: 0.2776 Precision: 0.6471 Recall: 0.3390\n\nEpoch 2/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3350e76b6ca64ec5b0a7f0b16d58cf1c"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.4606 Acc: 0.8178 F1: 0.8158 Precision: 0.8141 Recall: 0.8188\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2336ce9e3560401189760fff891b397e"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.9388 Acc: 0.3781 F1: 0.3313 Precision: 0.6275 Recall: 0.3781\n\nEpoch 3/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"412e6c19cb9c458f963a9536659c771f"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.3874 Acc: 0.8248 F1: 0.8233 Precision: 0.8219 Recall: 0.8258\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015d57cace9148009a01b90b3975ff22"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.4597 Acc: 0.3908 F1: 0.3489 Precision: 0.7324 Recall: 0.3908\n\nEpoch 4/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c939416aad4c98a599d1b352bdd05c"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.3414 Acc: 0.8284 F1: 0.8268 Precision: 0.8254 Recall: 0.8294\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3f77371ce94d1c9ba761ce07384fb8"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.4059 Acc: 0.4045 F1: 0.3668 Precision: 0.7060 Recall: 0.4045\n\nEpoch 5/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6d7ac4c77b48c48c914c2523269def"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.3264 Acc: 0.8315 F1: 0.8304 Precision: 0.8292 Recall: 0.8325\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b67259dacf459abda4b077df5b2c30"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.3026 Acc: 0.8358 F1: 0.8345 Precision: 0.8332 Recall: 0.8368\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db62a1b5d0347689761b54f086bc1e7"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.6085 Acc: 0.4065 F1: 0.3706 Precision: 0.7174 Recall: 0.4065\n\nEpoch 7/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad385b93ab7346d99fd8cf56cd71eedb"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.2707 Acc: 0.8375 F1: 0.8365 Precision: 0.8354 Recall: 0.8385\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97aac81e439f4861b728330102347b31"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.4405 Acc: 0.4091 F1: 0.3733 Precision: 0.7156 Recall: 0.4091\n\nEpoch 8/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"439c08808aa64ff9a371f2f0982daaba"}},"metadata":{}},{"name":"stdout","text":"train Loss: 1.2639 Acc: 0.8386 F1: 0.8376 Precision: 0.8365 Recall: 0.8396\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ecf41dd8dfc4fb8a098bff822bd696a"}},"metadata":{}},{"name":"stdout","text":"test Loss: 13.7767 Acc: 0.4094 F1: 0.3734 Precision: 0.7337 Recall: 0.4094\n\nEpoch 9/19\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/766 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfc5d492c2242d4ad4f2b0500e50960"}},"metadata":{}}]},{"cell_type":"code","source":"data_loader_train, train_len = get_analysis_train_dataloader(data_path, label_path, batch_size, num_epochs = 20, epochs = 0)\n\nfor a,b,c in data_loader_train:\n    break\n    \na.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:48:28.470633Z","iopub.execute_input":"2024-09-03T10:48:28.470969Z","iopub.status.idle":"2024-09-03T10:48:36.379192Z","shell.execute_reply.started":"2024-09-03T10:48:28.470944Z","shell.execute_reply":"2024-09-03T10:48:36.378238Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"self num_epochs =  <class 'int'> 20\nself epochs =  <class 'int'> 0\nout num_epochs =  <class 'int'> 20\nout epochs =  <class 'int'> 0\n","output_type":"stream"},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 3, 1, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"inputs = torch.cat((a, b), dim = 0)\nlabels = torch.cat((c, c), dim = 0)\n\ninputs.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:49:56.937475Z","iopub.execute_input":"2024-09-03T10:49:56.937859Z","iopub.status.idle":"2024-09-03T10:49:56.951805Z","shell.execute_reply.started":"2024-09-03T10:49:56.937826Z","shell.execute_reply":"2024-09-03T10:49:56.950919Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 3, 1, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"\na.unsqueeze(0).shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:41:58.118878Z","iopub.execute_input":"2024-09-03T10:41:58.119701Z","iopub.status.idle":"2024-09-03T10:41:58.125761Z","shell.execute_reply.started":"2024-09-03T10:41:58.119666Z","shell.execute_reply":"2024-09-03T10:41:58.124814Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 16, 3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"x = inputs.to(device)\nt = model(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:50:01.431168Z","iopub.execute_input":"2024-09-03T10:50:01.431639Z","iopub.status.idle":"2024-09-03T10:50:01.674753Z","shell.execute_reply.started":"2024-09-03T10:50:01.431606Z","shell.execute_reply":"2024-09-03T10:50:01.674013Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"t.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:50:05.470063Z","iopub.execute_input":"2024-09-03T10:50:05.470455Z","iopub.status.idle":"2024-09-03T10:50:05.476437Z","shell.execute_reply.started":"2024-09-03T10:50:05.470417Z","shell.execute_reply":"2024-09-03T10:50:05.475523Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 7])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Independent Validation","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:13:35.136848Z","iopub.execute_input":"2024-09-10T09:13:35.137967Z","iopub.status.idle":"2024-09-10T09:13:35.644144Z","shell.execute_reply.started":"2024-09-10T09:13:35.137920Z","shell.execute_reply":"2024-09-10T09:13:35.643106Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"val_seqs, val_labels = sequence_extractor(\n    data_dict = data_builder(val_label_path, train = False),\n    data_path = data_path,\n    min_stride = 1,\n    sequence_length = 5,\n    train = False\n)\n\nval_dataset = ABAWFeatureDataset(val_seqs, val_labels, transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n    ])\n)\n\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle = True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:05.277394Z","iopub.execute_input":"2024-08-19T09:28:05.277779Z","iopub.status.idle":"2024-08-19T09:28:07.581753Z","shell.execute_reply.started":"2024-08-19T09:28:05.277749Z","shell.execute_reply":"2024-08-19T09:28:07.580948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"running_loss = 0.0\nrunning_corrects = 0.0\nall_preds = []\nall_labels = []\n            \n    \nfor inputs, labels in tqdm(data_loader_val):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n#     optimizer.zero_grad()\n\n    with torch.no_grad():\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n\n    running_loss += loss.item() * inputs.size(0)\n    running_corrects += torch.sum(preds == labels.data)\n\n    all_preds.extend(preds.cpu().numpy())\n    all_labels.extend(labels.cpu().numpy())\n\nepoch_loss = running_loss / dataset_sizes['val']\nepoch_acc = running_corrects.double() / dataset_sizes['val']\n\n# Calculate F1 score, precision, and recall\nepoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n\nprint(f\"{'val'} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:11.406099Z","iopub.execute_input":"2024-08-19T09:28:11.406430Z","iopub.status.idle":"2024-08-19T09:30:41.886717Z","shell.execute_reply.started":"2024-08-19T09:28:11.406405Z","shell.execute_reply":"2024-08-19T09:30:41.885835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Example list of image paths\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nimage_paths = [os.path.join(data_path, x) for x in train_seqs[0]]\nlabels = [train_labels[0] for x in train_seqs[0]]\n# Number of images\nn_images = len(image_paths)\n\n# Calculate grid size (assuming a square grid)\ngrid_size = int(n_images**0.5) + (n_images**0.5 != int(n_images**0.5))\n\n# Create a figure with a grid of subplots\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n\n# Flatten the axes array for easy iteration\naxes = axes.flatten()\n\n# Iterate over images and plot them with labels\nfor i, (image_path, label) in enumerate(zip(image_paths, labels)):\n    img = mpimg.imread(image_path)\n    axes[i].imshow(img)\n    axes[i].axis('off')  # Hide axes\n    axes[i].set_title(label, fontsize=12)  # Add label\n\n# Hide any remaining empty subplots\nfor j in range(i+1, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:12:32.517175Z","iopub.execute_input":"2024-08-14T10:12:32.518098Z","iopub.status.idle":"2024-08-14T10:12:33.938161Z","shell.execute_reply.started":"2024-08-14T10:12:32.518062Z","shell.execute_reply":"2024-08-14T10:12:33.937147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Facial Landmark detector on cuda","metadata":{}},{"cell_type":"code","source":"# app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n# app.prepare(ctx_id=0, det_size=(640, 640))  # Set detection size, though not used directly for cropped images\n\n# Example usage\ncropped_face_path = '/kaggle/input/raf-db-trial/92_86/92/dataset/RAF/test_0001.jpg'\ncropped_face = cv2.imread(cropped_face_path)\ntop = 100\nbottom = 100\nleft = 100\nright = 100\n\n# Pad the image\npadded_image = cv2.copyMakeBorder(\n    cropped_face,\n    top,\n    bottom,\n    left,\n    right,\n    cv2.BORDER_CONSTANT,  # Border type\n    value=[0, 0, 0]       # Padding color (black in this case)\n)\n# cv2_imshow(padded_image)\nfor i in range(10):\n    out = app.get(padded_image)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:17:43.001508Z","iopub.execute_input":"2024-09-03T10:17:43.002108Z","iopub.status.idle":"2024-09-03T10:17:48.322154Z","shell.execute_reply.started":"2024-09-03T10:17:43.002078Z","shell.execute_reply":"2024-09-03T10:17:48.320925Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"out[0]['landmark_2d_106']","metadata":{"execution":{"iopub.status.busy":"2024-09-03T10:11:55.504332Z","iopub.execute_input":"2024-09-03T10:11:55.505160Z","iopub.status.idle":"2024-09-03T10:11:55.515999Z","shell.execute_reply.started":"2024-09-03T10:11:55.505120Z","shell.execute_reply":"2024-09-03T10:11:55.515007Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[156.45071 , 210.42764 ],\n       [118.11843 , 147.66898 ],\n       [126.07091 , 191.4655  ],\n       [128.95174 , 195.59265 ],\n       [132.36949 , 199.33719 ],\n       [136.2097  , 202.70042 ],\n       [140.47379 , 205.81747 ],\n       [145.20027 , 208.36473 ],\n       [150.51312 , 210.01663 ],\n       [117.85137 , 152.72223 ],\n       [117.86227 , 157.70833 ],\n       [118.11784 , 162.64897 ],\n       [118.63982 , 167.56123 ],\n       [119.438286, 172.4909  ],\n       [120.51621 , 177.42566 ],\n       [121.8665  , 182.30336 ],\n       [123.71557 , 187.01166 ],\n       [189.53238 , 145.91133 ],\n       [183.72394 , 189.44621 ],\n       [181.2376  , 193.71155 ],\n       [178.25674 , 197.62396 ],\n       [174.90025 , 201.20598 ],\n       [171.23077 , 204.61256 ],\n       [167.04013 , 207.50168 ],\n       [162.1745  , 209.54306 ],\n       [190.07605 , 150.88753 ],\n       [190.31711 , 155.8381  ],\n       [190.25768 , 160.73816 ],\n       [189.91148 , 165.61153 ],\n       [189.28094 , 170.48413 ],\n       [188.3864  , 175.36115 ],\n       [187.29309 , 180.19016 ],\n       [185.74463 , 184.9095  ],\n       [137.32426 , 153.866   ],\n       [137.52438 , 150.77863 ],\n       [130.19966 , 151.89279 ],\n       [133.41046 , 153.26012 ],\n       [141.39517 , 153.42532 ],\n       [137.5238  , 150.77563 ],\n       [145.06544 , 152.72986 ],\n       [137.54025 , 148.2768  ],\n       [133.32774 , 149.30574 ],\n       [141.84116 , 149.50282 ],\n       [124.895775, 144.47128 ],\n       [130.58238 , 142.70148 ],\n       [136.53867 , 141.59776 ],\n       [149.12703 , 142.41649 ],\n       [142.94873 , 141.41057 ],\n       [129.38664 , 138.9772  ],\n       [136.09158 , 136.66321 ],\n       [149.52048 , 138.75424 ],\n       [143.44284 , 136.84935 ],\n       [142.24858 , 189.93985 ],\n       [156.05614 , 195.594   ],\n       [149.27466 , 189.9173  ],\n       [146.02318 , 192.80676 ],\n       [150.4551  , 194.88988 ],\n       [162.52383 , 189.57675 ],\n       [165.65584 , 192.29266 ],\n       [161.50806 , 194.58267 ],\n       [156.05054 , 190.03592 ],\n       [169.08994 , 189.34567 ],\n       [156.03094 , 189.40399 ],\n       [152.6     , 184.96669 ],\n       [147.08516 , 187.04283 ],\n       [144.31296 , 189.90659 ],\n       [149.22533 , 189.41599 ],\n       [159.43915 , 184.84286 ],\n       [164.62137 , 186.66502 ],\n       [167.13652 , 189.39223 ],\n       [162.53189 , 189.11806 ],\n       [156.03883 , 185.77596 ],\n       [155.45569 , 150.09248 ],\n       [155.73372 , 157.33655 ],\n       [156.03462 , 164.56415 ],\n       [149.6748  , 152.93166 ],\n       [147.35434 , 168.21147 ],\n       [145.0446  , 174.5643  ],\n       [148.45117 , 176.68837 ],\n       [152.15297 , 177.49171 ],\n       [156.2623  , 178.63647 ],\n       [161.1934  , 152.72166 ],\n       [164.25244 , 167.93954 ],\n       [166.72203 , 174.13892 ],\n       [163.66785 , 176.35461 ],\n       [160.17871 , 177.29137 ],\n       [156.32385 , 171.80399 ],\n       [173.09798 , 152.90332 ],\n       [172.26028 , 149.86089 ],\n       [165.34712 , 152.19263 ],\n       [169.03745 , 152.68571 ],\n       [176.88405 , 152.057   ],\n       [172.25713 , 149.86037 ],\n       [179.89688 , 150.52103 ],\n       [172.48438 , 147.38255 ],\n       [168.306   , 148.83739 ],\n       [176.68895 , 148.15764 ],\n       [162.13924 , 142.30649 ],\n       [167.96436 , 141.07281 ],\n       [174.01411 , 140.92749 ],\n       [179.65567 , 141.5382  ],\n       [185.06725 , 142.67804 ],\n       [161.62222 , 138.81625 ],\n       [167.32225 , 136.73071 ],\n       [174.23312 , 136.1341  ],\n       [180.61938 , 137.80186 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"Build a dynamic FER model which takes video ","metadata":{}}]}