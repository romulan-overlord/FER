{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9418102,"sourceType":"datasetVersion","datasetId":5714598},{"sourceId":10546963,"sourceType":"datasetVersion","datasetId":6525643},{"sourceId":10547510,"sourceType":"datasetVersion","datasetId":6526012},{"sourceId":10547680,"sourceType":"datasetVersion","datasetId":6526137}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !git clone https://github.com/zengqunzhao/Exp-CLIP.git\n!pip install ftfy salesforce-lavis","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:08:05.771782Z","iopub.execute_input":"2025-01-27T04:08:05.772068Z","iopub.status.idle":"2025-01-27T04:08:43.506374Z","shell.execute_reply.started":"2025-01-27T04:08:05.772038Z","shell.execute_reply":"2025-01-27T04:08:43.505581Z"}},"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting salesforce-lavis\n  Downloading salesforce_lavis-1.0.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\nCollecting contexttimer (from salesforce-lavis)\n  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting decord (from salesforce-lavis)\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.8.0)\nCollecting fairscale==0.4.4 (from salesforce-lavis)\n  Downloading fairscale-0.4.4.tar.gz (235 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting iopath (from salesforce-lavis)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (7.34.0)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.3.0)\nCollecting opencv-python-headless==4.5.5.64 (from salesforce-lavis)\n  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting opendatasets (from salesforce-lavis)\n  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (24.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.2.2)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (5.24.1)\nCollecting pre-commit (from salesforce-lavis)\n  Downloading pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting pycocoevalcap (from salesforce-lavis)\n  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.0.8)\nCollecting python-magic (from salesforce-lavis)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.25.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.2.0)\nRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (3.7.5)\nCollecting streamlit (from salesforce-lavis)\n  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting timm==0.4.12 (from salesforce-lavis)\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.20.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (4.67.1)\nCollecting transformers<4.27,>=4.25.0 (from salesforce-lavis)\n  Downloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting webdataset (from salesforce-lavis)\n  Downloading webdataset-0.2.100-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.45.1)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless==4.5.5.64->salesforce-lavis) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->salesforce-lavis) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (0.27.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting portalocker (from iopath->salesforce-lavis)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (4.9.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->salesforce-lavis) (4.9.3)\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets->salesforce-lavis) (1.6.17)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets->salesforce-lavis) (8.1.7)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->salesforce-lavis) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->salesforce-lavis) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->salesforce-lavis) (2024.2)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->salesforce-lavis) (9.0.0)\nCollecting cfgv>=2.0.0 (from pre-commit->salesforce-lavis)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit->salesforce-lavis)\n  Downloading identify-2.6.6-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit->salesforce-lavis)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nCollecting virtualenv>=20.10.0 (from pre-commit->salesforce-lavis)\n  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->salesforce-lavis) (3.7.5)\nRequirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (1.13.1)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (11.0.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (2.36.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (2024.12.12)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (0.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (0.15.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.10.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.5.0)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (5.5.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (17.0.0)\nRequirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (13.9.4)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (0.10.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (3.1.43)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->salesforce-lavis)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (6.3.3)\nCollecting braceexpand (from webdataset->salesforce-lavis)\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (4.0.11)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->salesforce-lavis) (0.8.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2.4.1)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->salesforce-lavis) (0.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (2.27.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->salesforce-lavis) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->salesforce-lavis) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2024.12.14)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->salesforce-lavis) (3.0.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->salesforce-lavis) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->salesforce-lavis) (0.1.5)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->salesforce-lavis) (1.5.4)\nCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->salesforce-lavis)\n  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (4.3.6)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (7.0.5)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets->salesforce-lavis) (8.0.4)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets->salesforce-lavis) (6.2.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (5.0.1)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (24.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.22.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis) (1.2.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->salesforce-lavis) (0.1.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (1.17.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets->salesforce-lavis) (0.5.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2024.2.0)\nRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets->salesforce-lavis) (1.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python-headless==4.5.5.64->salesforce-lavis) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading salesforce_lavis-1.0.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\nDownloading pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading webdataset-0.2.100-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading identify-2.6.6-py2.py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fairscale, contexttimer, iopath\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292882 sha256=33d1a3f89102b48a410fc2cf5813986e26186bdfe33f38c4f65bcd7ed620e992\n  Stored in directory: /root/.cache/pip/wheels/08/58/6f/56c57fa8315eb0bcf0287b580c850845be5f116359b809e9f1\n  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=fbfd58835b0e081973f31d515a2642d61fc19ef2a69b0ed0a4fe79712fd3a06d\n  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=aebeb3b82ad9675a7fb474f413fcecab858a7bed5b27332bfa8de26210bedd42\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fairscale contexttimer iopath\nInstalling collected packages: tokenizers, distlib, contexttimer, braceexpand, virtualenv, python-magic, portalocker, nodeenv, identify, ftfy, cfgv, pre-commit, iopath, opendatasets, fairscale, pydeck, webdataset, transformers, timm, streamlit, pycocoevalcap, opencv-python-headless, decord, salesforce-lavis\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.12\n    Uninstalling timm-1.0.12:\n      Successfully uninstalled timm-1.0.12\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.10.0.84\n    Uninstalling opencv-python-headless-4.10.0.84:\n      Successfully uninstalled opencv-python-headless-4.10.0.84\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbucore 0.0.19 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\nalbumentations 1.4.20 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\nkaggle-environments 1.16.10 requires transformers>=4.33.1, but you have transformers 4.26.1 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed braceexpand-0.1.7 cfgv-3.4.0 contexttimer-0.3.3 decord-0.6.0 distlib-0.3.9 fairscale-0.4.4 ftfy-6.3.1 identify-2.6.6 iopath-0.1.10 nodeenv-1.9.1 opencv-python-headless-4.5.5.64 opendatasets-0.1.22 portalocker-3.1.1 pre-commit-4.1.0 pycocoevalcap-1.2 pydeck-0.9.1 python-magic-0.4.27 salesforce-lavis-1.0.2 streamlit-1.41.1 timm-0.4.12 tokenizers-0.13.3 transformers-4.26.1 virtualenv-20.29.1 webdataset-0.2.100\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.insert(0, '/kaggle/working/Exp-CLIP')\n\nimport torch\nfrom torch import nn\nfrom models.clip import clip\n# from models.BLIP2_T5 import *\nimport torch.nn.functional as F\n\nimport argparse\nimport time\nimport os\nimport random\nimport numpy as np\nimport datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\n\nimport pickle\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:08:48.049140Z","iopub.execute_input":"2025-01-27T04:08:48.049414Z","iopub.status.idle":"2025-01-27T04:08:54.069566Z","shell.execute_reply.started":"2025-01-27T04:08:48.049391Z","shell.execute_reply":"2025-01-27T04:08:54.068865Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument('--workers', type=int, default=8)\nparser.add_argument('--epochs', type=int, default=5)\nparser.add_argument('--batch-size', type=int, default=512)\nparser.add_argument('--batch-size-test-image', type=int, default=512)\nparser.add_argument('--batch-size-test-video', type=int, default=64)\nparser.add_argument('--lr', type=float, default=1e-3)\nparser.add_argument('--weight-decay', type=float, default=1e-4)\nparser.add_argument('--momentum', type=float, default=0.9)\nparser.add_argument('--print-freq', type=int, default=100)\nparser.add_argument('--milestones', nargs='+', type=int, default=30)\nparser.add_argument('--seed', type=int, default = 1)\nparser.add_argument('--job-id', type=str, default=str(int(time.time())))\nparser.add_argument('--instruction', type=str, default='Please play the role of a facial action describer. Objectively describe the detailed facial actions of the person in the image.')\nparser.add_argument('--load-model', type=str, default='CLIP_L14')\n\n# Use parse_known_args() to handle extra arguments\nargs, unknown = parser.parse_known_args()\n\nrandom.seed(args.seed)  \nnp.random.seed(args.seed) \ntorch.manual_seed(args.seed)\ntorch.cuda.manual_seed(args.seed)\ntorch.cuda.manual_seed_all(args.seed)\n\nnow = datetime.datetime.now()\ntrain_time = now.strftime(\"%y-%m-%d %H:%M\")\nprint(\"Training date: \", train_time)\njob_id = args.job_id\n\nprint('************************')\nfor k, v in vars(args).items():\n    print(k,'=',v)\nprint('************************')\n# Handle unknown arguments if needed\nif unknown:\n    print('Ignored Arguments:', unknown)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:32:03.126817Z","iopub.execute_input":"2025-01-24T10:32:03.127220Z","iopub.status.idle":"2025-01-24T10:32:03.149910Z","shell.execute_reply.started":"2025-01-24T10:32:03.127194Z","shell.execute_reply":"2025-01-24T10:32:03.149221Z"}},"outputs":[{"name":"stdout","text":"Training date:  25-01-24 10:32\n************************\nworkers = 8\nepochs = 5\nbatch_size = 512\nbatch_size_test_image = 512\nbatch_size_test_video = 64\nlr = 0.001\nweight_decay = 0.0001\nmomentum = 0.9\nprint_freq = 100\nmilestones = 30\nseed = 1\njob_id = 1737714723\ninstruction = Please play the role of a facial action describer. Objectively describe the detailed facial actions of the person in the image.\nload_model = CLIP_L14\n************************\nIgnored Arguments: ['-f', '/root/.local/share/jupyter/runtime/kernel-84438eb5-1862-46c3-b91c-e6c57092cf47.json']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# RAF-DB Dataloader","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass CustomImageDataset(Dataset):\n    def __init__(self, txt_file, img_dir, transform=None):\n        self.img_labels = []\n        self.img_dir = img_dir\n        self.transform = transform\n\n        with open(txt_file, 'r') as file:\n            for line in file.readlines():\n                filename, label = line.strip().split()\n                name, ext = filename.strip().split('.')\n                filename = name + '_aligned.' + ext\n                self.img_labels.append((filename, int(label)))\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_name, label = self.img_labels[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, img_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:29:12.223047Z","iopub.execute_input":"2025-01-24T09:29:12.223801Z","iopub.status.idle":"2025-01-24T09:29:12.232698Z","shell.execute_reply.started":"2025-01-24T09:29:12.223762Z","shell.execute_reply":"2025-01-24T09:29:12.231479Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Dataset and DataLoader setup\ntxt_file = '/kaggle/input/rafdb-dg/EmoLabel/train_label.txt'\nimg_dir = '/kaggle/input/rafdb-dg/aligned/aligned'\n\ndataset = CustomImageDataset(txt_file=txt_file, img_dir=img_dir, transform=transform)\ndata_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=8, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CAER-S Dataloader","metadata":{}},{"cell_type":"code","source":"class FolderBasedDataset(Dataset):\n    def __init__(self, root_dirs, transform=None):\n        \"\"\"\n        Args:\n            root_dirs (list): List of root directories containing class folders.\n            transform (callable, optional): A function/transform to apply to the images.\n        \"\"\"\n        self.img_labels = []  # To store (image_path, class_label)\n        self.transform = transform\n        self.labels = []\n        self.img_names = []\n\n        # Loop through each root directory\n        for root_dir in root_dirs:\n            # Iterate over the class folders\n            for class_folder in sorted(os.listdir(root_dir)):  # Alphabetically sorted for consistent label assignment\n                self.labels.append(class_folder)\n                class_path = os.path.join((os.path.join(root_dir, class_folder)), class_folder)\n                if os.path.isdir(class_path):  # Ensure it's a folder\n                    # Assign the folder name as the class label\n                    class_label = class_folder\n                    # Collect all image paths in this folder\n                    for img_name in os.listdir(class_path):\n                        img_path = os.path.join(class_path, img_name)\n                        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):  # Supported image formats\n                            self.img_labels.append((img_path, class_label))\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path, class_label = self.img_labels[idx]\n        image = Image.open(img_path).convert('RGB')  # Convert to RGB\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, class_label, img_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:04:14.852841Z","iopub.execute_input":"2025-01-24T09:04:14.853194Z","iopub.status.idle":"2025-01-24T09:04:14.860452Z","shell.execute_reply.started":"2025-01-24T09:04:14.853166Z","shell.execute_reply":"2025-01-24T09:04:14.859428Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# List of root directories for CAERS datasets\nroot_dirs = [\n    '/kaggle/input/caer-s-train-1',\n    '/kaggle/input/caer-s-train-2',\n    '/kaggle/input/caer-s-train-3'\n]\n\n# Initialize dataset\ndataset = FolderBasedDataset(root_dirs=root_dirs, transform=transform)\n\n# Initialize DataLoader\ndata_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n\n# Inspect some samples\nfor images, labels, img_paths in data_loader:\n    print(\"Batch size:\", len(images))\n    print(\"Labels:\", labels[:3])  # Print first 3 labels in the batch\n    print(\"Image paths:\", img_paths[:3])  # Print first 3 image paths in the batch\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:04:17.764004Z","iopub.execute_input":"2025-01-24T09:04:17.764404Z","iopub.status.idle":"2025-01-24T09:04:18.305656Z","shell.execute_reply.started":"2025-01-24T09:04:17.764374Z","shell.execute_reply":"2025-01-24T09:04:18.304182Z"}},"outputs":[{"name":"stdout","text":"Batch size: 8\nLabels: ('Neutral', 'Neutral', 'Fear')\nImage paths: ('/kaggle/input/caer-s-train-3/Neutral/Neutral/4567.png', '/kaggle/input/caer-s-train-3/Neutral/Neutral/0897.png', '/kaggle/input/caer-s-train-2/Fear/Fear/6343.png')\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# CLIP Extraction","metadata":{}},{"cell_type":"code","source":"# Load CLIP model\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-L/14\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T09:29:13.250956Z","iopub.execute_input":"2025-01-23T09:29:13.251280Z","iopub.status.idle":"2025-01-23T09:29:31.923718Z","shell.execute_reply.started":"2025-01-23T09:29:13.251240Z","shell.execute_reply":"2025-01-23T09:29:31.923044Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 890M/890M [00:05<00:00, 172MiB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Output directory\noutput_dir = \"/kaggle/working/clip_encodings_caer\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save a chunk of data\ndef save_chunk(data, chunk_index):\n    file_path = os.path.join(output_dir, f\"encodings_chunk_{chunk_index}.pkl\")\n    with open(file_path, \"wb\") as f:\n        pickle.dump(data, f)\n\n# Save label mapping\ndef save_label_mapping(label_mapping):\n    mapping_path = os.path.join(output_dir, \"label_mapping.pkl\")\n    with open(mapping_path, \"wb\") as f:\n        pickle.dump(label_mapping, f)\n\n# Generate a label mapping\nall_labels = sorted(set(dataset.labels))  # Assuming `dataset.labels` contains all possible labels\nlabel_mapping = {label: idx for idx, label in enumerate(all_labels)}\n\n# Save the label mapping\nsave_label_mapping(label_mapping)\n\n# Main loop to process data\nchunk_size = 20000  # Number of entries per pickle file\nchunk_data = {}     # Dictionary to store data indexed by filename\nchunk_index = 0\n\n# Check for progress file\nprogress_file = os.path.join(output_dir, \"progress.txt\")\nstart_index = 0\nif os.path.exists(progress_file):\n    with open(progress_file, \"r\") as f:\n        start_index = int(f.read().strip())\n\nprint(\"Starting from batch index:\", start_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T09:30:05.143777Z","iopub.execute_input":"2025-01-23T09:30:05.144061Z","iopub.status.idle":"2025-01-23T09:30:05.152001Z","shell.execute_reply.started":"2025-01-23T09:30:05.144041Z","shell.execute_reply":"2025-01-23T09:30:05.151157Z"}},"outputs":[{"name":"stdout","text":"Starting from batch index: 0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"clip_model.eval()\nwith torch.no_grad():\n    for batch_idx, (images, labels, img_names) in enumerate(tqdm(data_loader, desc=\"Processing batches\")):\n        if batch_idx < start_index:\n            continue\n\n        images = images.to(device)\n\n        # Generate image encodings\n        features = clip_model.encode_image(images)\n\n        # Assume logit scale is part of the model (adjust if needed)\n        logit_scale = clip_model.logit_scale.exp().item()\n\n        for i, feature in enumerate(features):\n            img_name = img_names[i]\n            label = labels[i]  # String label, e.g., \"Angry\", \"Sad\"\n            label_idx = label_mapping[label]  # Map to integer index\n\n            # Add data to the dictionary, indexed by filename\n            chunk_data[img_name] = {\n                \"encoding\": feature.cpu().numpy(),\n                \"logit_scale\": logit_scale,\n                \"label\": label,\n                \"label_idx\": label_idx,\n            }\n\n            # Save the chunk when the buffer is full\n            if len(chunk_data) >= chunk_size:\n                save_chunk(chunk_data, chunk_index)\n                chunk_data = {}\n                chunk_index += 1\n\n        # Save progress to resume in case of interruption\n        with open(progress_file, \"w\") as f:\n            f.write(str(batch_idx))\n\n# Save any remaining data\nif chunk_data:\n    save_chunk(chunk_data, chunk_index)\n\n# Clean up progress file\nif os.path.exists(progress_file):\n    os.remove(progress_file)\n\nprint(\"Encoding complete and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T09:30:27.900115Z","iopub.execute_input":"2025-01-23T09:30:27.900538Z","iopub.status.idle":"2025-01-23T09:53:25.381642Z","shell.execute_reply.started":"2025-01-23T09:30:27.900504Z","shell.execute_reply":"2025-01-23T09:53:25.380714Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 100%|██████████| 6126/6126 [22:57<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"Encoding complete and saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"output_dir = \"/kaggle/working/clip_encodings_caer\"\n\n# Function to load all chunks of encodings\ndef load_encodings(output_dir):\n    encodings = {}\n    for file_name in os.listdir(output_dir):\n        if file_name.startswith(\"encodings_chunk_\") and file_name.endswith(\".pkl\"):\n            file_path = os.path.join(output_dir, file_name)\n            with open(file_path, \"rb\") as f:\n                chunk = pickle.load(f)\n                encodings.update(chunk)\n    return encodings\n\n# Function to load the label mapping\ndef load_label_mapping(output_dir):\n    mapping_path = os.path.join(output_dir, \"label_mapping.pkl\")\n    with open(mapping_path, \"rb\") as f:\n        label_mapping = pickle.load(f)\n    return label_mapping\n\n# Load encodings and label mapping\nencodings = load_encodings(output_dir)\nlabel_mapping = load_label_mapping(output_dir)\n\n# Print basic info about the loaded data\nprint(f\"Total images encoded: {len(encodings)}\")\nprint(f\"Label mapping: {label_mapping}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:20:56.644574Z","iopub.execute_input":"2025-01-24T10:20:56.645065Z","iopub.status.idle":"2025-01-24T10:20:57.253510Z","shell.execute_reply.started":"2025-01-24T10:20:56.645027Z","shell.execute_reply":"2025-01-24T10:20:57.252585Z"}},"outputs":[{"name":"stdout","text":"Total images encoded: 49007\nLabel mapping: {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Neutral': 4, 'Sad': 5, 'Surprise': 6}\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# BLIP_2 Extraction","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n\ndef get_blip2t5_model(device):\n    model, _, _ = load_model_and_preprocess(\n        name=\"blip2_t5\",\n        model_type=\"pretrain_flant5xl\",\n        is_eval=True,\n        device=device,\n    )\n    model.generate = MethodType(generate, model)\n    return model\n\nblip_instruction = args.instruction\n\nblip2 = get_blip2t5_model(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T09:55:49.428692Z","iopub.execute_input":"2025-01-23T09:55:49.429010Z","iopub.status.idle":"2025-01-23T09:58:38.000464Z","shell.execute_reply.started":"2025-01-23T09:55:49.428986Z","shell.execute_reply":"2025-01-23T09:58:37.999522Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb203c2b44046e7987f87d828ecaaab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3370933d224d02a1c48e9a9bc6792d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e134bbfaf0d04daeb93792b343b6dbb4"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1.89G/1.89G [00:08<00:00, 230MB/s] \n/usr/local/lib/python3.10/dist-packages/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(cached_file, map_location=\"cpu\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2220d933150f4662855e5de7efdeb28a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff5abe6669a4be0ba65bf62bad905b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5448c3c6698046eb95050938df8f1c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b696d1b6815f40cdb5f2869b128267e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15bdc1a3e61b42e0844670532205b2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3485e56e32b34e24aea6691716c0c551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9872cfdbee74449aa52901060aecb197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ba1fc0450f4e4389130cf96cc849fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4e486c622a4fe6ae8de4e7c289e374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8a0fe01fc24d0b9b47a173996f0821"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45615e22929448e788dcc26482ebab2a"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 407M/407M [00:01<00:00, 227MB/s]  \n/usr/local/lib/python3.10/dist-packages/lavis/models/blip2_models/blip2.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Save a chunk of data indexed by image name\ndef save_chunk(data, chunk_index, output_dir):\n    file_path = os.path.join(output_dir, f\"encodings_chunk_{chunk_index}.pkl\")\n    with open(file_path, \"wb\") as f:\n        pickle.dump(data, f)\n\n# Function to save text features in chunks, indexed by image name\ndef save_text_features_in_chunks(dataloader, blip2, tokens, output_dir, chunk_size=1000, batch_size=8):\n    os.makedirs(output_dir, exist_ok=True)\n\n    chunk_data = {}\n    chunk_index = 0\n\n    # Check for progress file\n    progress_file = os.path.join(output_dir, \"progress.txt\")\n    start_index = 0\n    if os.path.exists(progress_file):\n        with open(progress_file, \"r\") as f:\n            start_index = int(f.read().strip())\n\n    print(\"Starting from batch index:\", start_index)\n\n    for batch_idx, (images, labels, img_names) in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n        if batch_idx < start_index:\n            continue\n\n        images = images.to(device)\n\n        # Regenerate tokens dynamically if this is the last batch and its size is smaller\n        if len(images) < batch_size:\n            tokens = get_tokens(blip_instruction, blip2.t5_tokenizer, len(images), device)\n\n        # Generate text features\n        _, text_features = blip2.generate({\"image\": images, \"tokens\": tokens})\n        text_features = torch.mean(text_features, dim=1)\n        text_features = text_features.float()\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        for i, feature in enumerate(text_features):\n            img_name = img_names[i]\n            label = labels[i]\n            chunk_data[img_name] = {\n                \"encoding\": feature.cpu().numpy(),\n                \"label\": label,\n            }\n\n            if len(chunk_data) >= chunk_size:\n                save_chunk(chunk_data, chunk_index, output_dir)\n                chunk_data = {}\n                chunk_index += 1\n\n        # Save progress\n        with open(progress_file, \"w\") as f:\n            f.write(str(batch_idx))\n\n    # Save any remaining data\n    if chunk_data:\n        save_chunk(chunk_data, chunk_index, output_dir)\n\n    # Clean up progress file\n    if os.path.exists(progress_file):\n        os.remove(progress_file)\n\n    print(\"Encoding complete and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T10:04:13.543528Z","iopub.execute_input":"2025-01-23T10:04:13.543896Z","iopub.status.idle":"2025-01-23T10:04:13.552953Z","shell.execute_reply.started":"2025-01-23T10:04:13.543863Z","shell.execute_reply":"2025-01-23T10:04:13.551947Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\nbatch_size = 8\n\ntokens = get_tokens(blip_instruction, blip2.t5_tokenizer, batch_size, device)\n\n# Define transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n# ])\n\n# # Dataset and DataLoader setup\n# txt_file = '/kaggle/input/rafdb-dg/EmoLabel/train_label.txt'\n# img_dir = '/kaggle/input/rafdb-dg/aligned/aligned'\n\n# dataset = CustomImageDataset(txt_file=txt_file, img_dir=img_dir, transform=transform)\n# data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n\noutput_dir = \"/kaggle/working/blip2_encodings_caer\"\nchunk_size = 20000  # Number of encodings per pickle file\n\nsave_text_features_in_chunks(data_loader, blip2, tokens, output_dir, chunk_size=chunk_size, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T10:04:16.384269Z","iopub.execute_input":"2025-01-23T10:04:16.384624Z","iopub.status.idle":"2025-01-23T11:38:55.150754Z","shell.execute_reply.started":"2025-01-23T10:04:16.384593Z","shell.execute_reply":"2025-01-23T11:38:55.149700Z"}},"outputs":[{"name":"stdout","text":"Starting from batch index: 0\n","output_type":"stream"},{"name":"stderr","text":"Processing batches:   0%|          | 0/6126 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/lavis/models/blip2_models/blip2.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast(dtype=dtype)\nProcessing batches: 100%|██████████| 6126/6126 [1:34:38<00:00,  1.08it/s]","output_type":"stream"},{"name":"stdout","text":"Encoding complete and saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Consolidating Data","metadata":{}},{"cell_type":"code","source":"class ProcessedDataset(Dataset):\n    def __init__(self, clip_encodings, blip_encodings):\n        self.encodings = []\n        self.labels = []\n        \n        for key in clip_encodings.keys():\n            self.encodings.append((\n                clip_encodings[key]['encoding'],\n                blip_encodings[key]['encoding'],\n                clip_encodings[key]['logit_scale']\n            ))\n            self.labels.append(clip_encodings[key]['label'])\n            \n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image_features, text_features, logit_scale = self.encodings[idx]\n        label = self.labels[idx]\n        return image_features, text_features, logit_scale, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T05:47:26.601369Z","iopub.execute_input":"2025-01-24T05:47:26.601785Z","iopub.status.idle":"2025-01-24T05:47:26.608112Z","shell.execute_reply.started":"2025-01-24T05:47:26.601753Z","shell.execute_reply":"2025-01-24T05:47:26.606890Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Function to load all chunks of encodings\ndef load_clip_encodings(output_dir):\n    encodings = {}\n    for file_name in os.listdir(output_dir):\n        if file_name.startswith(\"encodings_chunk_\") and file_name.endswith(\".pkl\"):\n            file_path = os.path.join(output_dir, file_name)\n            with open(file_path, \"rb\") as f:\n                chunk = pickle.load(f)\n                encodings.update(chunk)\n    return encodings\n\ndef load_blip_encodings(output_dir):\n    encodings = {}\n    for file_name in sorted(os.listdir(output_dir)):\n        if file_name.startswith(\"encodings_chunk_\") and file_name.endswith(\".pkl\"):\n            print(f'processing {file_name}')\n            file_path = os.path.join(output_dir, file_name)\n            with open(file_path, \"rb\") as f:\n                chunk_data = pickle.load(f)\n                encodings.update(chunk_data)\n    return encodings\n\n# Function to load the label mapping\ndef load_label_mapping(output_dir):\n    mapping_path = os.path.join(output_dir, \"label_mapping.pkl\")\n    with open(mapping_path, \"rb\") as f:\n        label_mapping = pickle.load(f)\n    return label_mapping\n\nclip_encodings = load_clip_encodings('/kaggle/working/clip_encodings_caer')\nblip_encodings = load_blip_encodings('/kaggle/working/blip2_encodings_caer')\n\nlabel_mapping = load_label_mapping('/kaggle/working/clip_encodings_caer')\n\n# Print basic info about the loaded data\nprint(f\"Total images encoded: {len(clip_encodings)}\")\nprint(f\"Label mapping: {label_mapping}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T05:47:29.367414Z","iopub.execute_input":"2025-01-24T05:47:29.367999Z","iopub.status.idle":"2025-01-24T05:47:30.271771Z","shell.execute_reply.started":"2025-01-24T05:47:29.367963Z","shell.execute_reply":"2025-01-24T05:47:30.270789Z"}},"outputs":[{"name":"stdout","text":"processing encodings_chunk_0.pkl\nprocessing encodings_chunk_1.pkl\nprocessing encodings_chunk_2.pkl\nTotal images encoded: 49007\nLabel mapping: {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Neutral': 4, 'Sad': 5, 'Surprise': 6}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset = ProcessedDataset(clip_encodings, blip_encodings)\ndataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:20:18.996213Z","iopub.execute_input":"2025-01-24T06:20:18.996592Z","iopub.status.idle":"2025-01-24T06:20:19.119220Z","shell.execute_reply.started":"2025-01-24T06:20:18.996560Z","shell.execute_reply":"2025-01-24T06:20:19.118168Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"class ExpCLIP_Train(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n    \n        self.projection_head = Linear_Matrix_L14()\n\n    def forward(self, image_features):\n        image_features = image_features.float()\n        image_features = self.projection_head(image_features)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        \n        return image_features\n\nclass Linear_Matrix_L14(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = nn.Linear(768, 2048, bias=False)\n    def forward(self, x):\n        return self.mlp(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:20:29.634132Z","iopub.execute_input":"2025-01-24T06:20:29.634489Z","iopub.status.idle":"2025-01-24T06:20:29.642109Z","shell.execute_reply.started":"2025-01-24T06:20:29.634462Z","shell.execute_reply":"2025-01-24T06:20:29.640812Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\", log_txt_path=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n        self.log_txt_path = log_txt_path\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print_txt = '\\t'.join(entries)\n        print(print_txt)\n        with open(self.log_txt_path, 'a') as f:\n            f.write(print_txt + '\\n')\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\nclass RecorderMeter(object):\n    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n    def __init__(self, total_epoch):\n        self.reset(total_epoch)\n\n    def reset(self, total_epoch):\n        self.total_epoch = total_epoch\n        self.current_epoch = 0\n        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)    # [epoch, train/val]\n        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n\n    def update(self, idx, train_loss, train_acc):\n        self.epoch_losses[idx, 0] = train_loss * 50\n        self.epoch_accuracy[idx, 0] = train_acc\n        self.current_epoch = idx + 1\n\n    def plot_curve(self, save_path):\n\n        title = 'the accuracy/loss curve of train/val'\n        dpi = 80\n        width, height = 1600, 800\n        legend_fontsize = 10\n        figsize = width / float(dpi), height / float(dpi)\n\n        fig = plt.figure(figsize=figsize)\n        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n        y_axis = np.zeros(self.total_epoch)\n\n        plt.xlim(0, self.total_epoch)\n        plt.ylim(0, 100)\n        interval_y = 5\n        interval_x = 1\n        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n        plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n        plt.grid()\n        plt.title(title, fontsize=20)\n        plt.xlabel('the training epoch', fontsize=16)\n        plt.ylabel('accuracy', fontsize=16)\n\n        y_axis[:] = self.epoch_accuracy[:, 0]\n        plt.plot(x_axis, y_axis, color='g', linestyle='-', label='train-accuracy', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_accuracy[:, 1]\n        plt.plot(x_axis, y_axis, color='y', linestyle='-', label='valid-accuracy', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 0]\n        plt.plot(x_axis, y_axis, color='g', linestyle=':', label='train-loss-x50', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 1]\n        plt.plot(x_axis, y_axis, color='y', linestyle=':', label='valid-loss-x50', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        if save_path is not None:\n            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n        plt.close(fig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:20:31.664070Z","iopub.execute_input":"2025-01-24T06:20:31.664401Z","iopub.status.idle":"2025-01-24T06:20:31.684858Z","shell.execute_reply.started":"2025-01-24T06:20:31.664375Z","shell.execute_reply":"2025-01-24T06:20:31.683325Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\ndef train(train_loader, model, criterion, optimizer, epoch, args, log_txt_path):\n    losses = AverageMeter('Loss', ':.4f')\n    top1 = AverageMeter('Accuracy', ':6.3f')\n    progress = ProgressMeter(len(train_loader),\n                             [losses, top1],\n                             prefix=\"Epoch: [{}]\".format(epoch),\n                             log_txt_path=log_txt_path)\n\n    # switch to train mode\n    model.train()\n\n    # Use tqdm to show progress\n    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch}\", unit=\"batch\") as pbar:\n        for i, (image_features, text_features, logit_scale, labels) in enumerate(train_loader):\n            image_features = image_features.to(device)\n            text_features = text_features.to(device)\n            logit_scale = logit_scale[0]\n            logit_scale = logit_scale.to(device)\n            \n            n, _ = image_features.shape\n            target = torch.arange(n)\n            target = target.to(device)\n\n            # compute output\n            image_features = model(image_features)\n            \n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n\n            loss_vision = criterion(logits_per_image, target)\n            loss_text = criterion(logits_per_text, target)\n            \n            loss = 0.5 * loss_vision + 0.5 * loss_text\n            \n            # measure accuracy and record loss\n            acc1, _ = accuracy(logits_per_image, target, topk=(1, 3))\n            losses.update(loss.item(), image_features.size(0))\n            top1.update(acc1[0], image_features.size(0))\n\n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Update tqdm progress bar\n            pbar.set_postfix({\"Loss\": losses.avg, \"Accuracy\": top1.avg.item()})\n            pbar.update(1)\n            \n            # Optionally display progress at intervals\n            # if i % args.print_freq == 0:\n            #     progress.display(i)\n            \n    return top1.avg, losses.avg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:21:25.038566Z","iopub.execute_input":"2025-01-24T06:21:25.038939Z","iopub.status.idle":"2025-01-24T06:21:25.049797Z","shell.execute_reply.started":"2025-01-24T06:21:25.038900Z","shell.execute_reply":"2025-01-24T06:21:25.048606Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"log_txt_path = './log/' + job_id + '-log.txt'\nlog_curve_path = './log/' + job_id + '-log.png'\ncheckpoint_path = './checkpoint/' + job_id\n\nos.makedirs('log', exist_ok = True)\nos.makedirs('checkpoint', exist_ok = True)\n\nrecorder = RecorderMeter(args.epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:23:27.304774Z","iopub.execute_input":"2025-01-24T06:23:27.305137Z","iopub.status.idle":"2025-01-24T06:23:27.311625Z","shell.execute_reply.started":"2025-01-24T06:23:27.305105Z","shell.execute_reply":"2025-01-24T06:23:27.310340Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import torch.backends.cudnn as cudnn\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n\nmodel = ExpCLIP_Train(args)\n# define loss function (criterion)\ncriterion = nn.CrossEntropyLoss().cuda()\n\n# only open learnable part\nfor name, param in model.named_parameters():\n    param.requires_grad = False\nfor name, param in model.named_parameters():\n    if \"projection_head\" in name:\n        param.requires_grad = True \n\nmodel = model.to(device)\n        \n# define optimizer\noptimizer = torch.optim.SGD([{\"params\": model.projection_head.parameters(), \"lr\": args.lr}],\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\n\n# define scheduler\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[args.milestones], gamma=0.1)\ncudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:23:38.693339Z","iopub.execute_input":"2025-01-24T06:23:38.693747Z","iopub.status.idle":"2025-01-24T06:23:38.714857Z","shell.execute_reply.started":"2025-01-24T06:23:38.693715Z","shell.execute_reply":"2025-01-24T06:23:38.713559Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"for epoch in range(0, args.epochs):\n\n    inf = '********************' + str(epoch) + '********************'\n    start_time = time.time()\n    current_learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n    with open(log_txt_path, 'a') as f:\n        f.write(inf + '\\n')\n        print(inf)\n        f.write('Current learning rate: ' + str(current_learning_rate) + '\\n')      \n        \n    # train for one epoch\n    train_acc, train_los = train(dataloader, model, criterion, optimizer, epoch, args, log_txt_path)\n    scheduler.step()\n\n    # print and save log\n    epoch_time = time.time() - start_time\n    recorder.update(epoch, train_los, train_acc)\n    recorder.plot_curve(log_curve_path)\n    print('The train accuracy: {:.3f}'.format(train_acc.item()))\n    print('An epoch time: {:.2f}s'.format(epoch_time))\n    with open(log_txt_path, 'a') as f:\n        f.write('The best accuracy: ' + str(train_acc.item()) + '\\n')\n        f.write('An epoch time: ' + str(epoch_time) + 's' + '\\n')\n\n#  save model and conduct zero-shot prediction\ncheckpoint_name = checkpoint_path + '-model.pth'\ntorch.save(model.projection_head.state_dict(), checkpoint_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:32:38.662080Z","iopub.execute_input":"2025-01-24T06:32:38.662531Z","iopub.status.idle":"2025-01-24T06:33:26.415994Z","shell.execute_reply.started":"2025-01-24T06:32:38.662481Z","shell.execute_reply":"2025-01-24T06:33:26.414650Z"}},"outputs":[{"name":"stdout","text":"********************0********************\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0: 100%|██████████| 96/96 [00:09<00:00, 10.66batch/s, Loss=0.705, Accuracy=89.7]\n","output_type":"stream"},{"name":"stdout","text":"The train accuracy: 89.722\nAn epoch time: 9.02s\n********************1********************\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 96/96 [00:09<00:00, 10.58batch/s, Loss=0.702, Accuracy=89.7]\n","output_type":"stream"},{"name":"stdout","text":"The train accuracy: 89.701\nAn epoch time: 9.08s\n********************2********************\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 96/96 [00:09<00:00,  9.79batch/s, Loss=0.701, Accuracy=89.9]\n","output_type":"stream"},{"name":"stdout","text":"The train accuracy: 89.887\nAn epoch time: 9.81s\n********************3********************\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 96/96 [00:09<00:00, 10.54batch/s, Loss=0.699, Accuracy=89.8]\n","output_type":"stream"},{"name":"stdout","text":"The train accuracy: 89.824\nAn epoch time: 9.12s\n********************4********************\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 96/96 [00:09<00:00, 10.66batch/s, Loss=0.7, Accuracy=89.8]  \n","output_type":"stream"},{"name":"stdout","text":"The train accuracy: 89.757\nAn epoch time: 9.01s\n","output_type":"stream"}],"execution_count":94},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport matplotlib\nmatplotlib.use('Agg')\nimport numpy as np\nfrom data_loader.video_dataloader import test_data_loader\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nfrom models.Text import *\nfrom models.Exp_CLIP import ExpCLIP_Test\nimport argparse\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport matplotlib.pyplot as plt\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:09:00.587575Z","iopub.execute_input":"2025-01-27T04:09:00.587985Z","iopub.status.idle":"2025-01-27T04:09:19.133875Z","shell.execute_reply.started":"2025-01-27T04:09:00.587964Z","shell.execute_reply":"2025-01-27T04:09:19.133146Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import time\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--load-model', type=str, default='CLIP_L14')\nparser.add_argument('--job-id', type=str, default=str(int(time.time())))  # Default job ID is a timestamp\n\n# Use parse_known_args() to handle extra arguments\nargs, unknown = parser.parse_known_args()\n\n# pretrain_model_path = './checkpoint/' + args.job_id + \"-model.pth\"\npretrain_model_path = '/kaggle/working/checkpoint/1737699389-model.pth'\n# pretrain_model_path  = '/kaggle/working/Exp-CLIP/checkpoint/ExpCLIP_L14_model.pth'\n\nprint('************************')\nfor k, v in vars(args).items():\n    print(k, '=', v)\nprint('************************')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:09:24.859016Z","iopub.execute_input":"2025-01-27T04:09:24.859659Z","iopub.status.idle":"2025-01-27T04:09:24.868272Z","shell.execute_reply.started":"2025-01-27T04:09:24.859631Z","shell.execute_reply":"2025-01-27T04:09:24.867383Z"}},"outputs":[{"name":"stdout","text":"************************\nload_model = CLIP_L14\njob_id = 1737950964\n************************\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class ExpCLIP_Test(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n    \n        if args.load_model == 'CLIP_B32':\n            self.clip_model, _ = clip.load(\"ViT-B/32\", device)\n        elif args.load_model == 'CLIP_B16':\n            self.clip_model, _ = clip.load(\"ViT-B/16\", device)\n        elif args.load_model == 'CLIP_L14':\n            self.clip_model, _ = clip.load(\"ViT-L/14\", device)\n\n        if args.load_model == 'CLIP_L14':\n            self.projection_head = Linear_Matrix_L14()\n        else:\n            self.projection_head = Linear_Matrix()\n\n    def forward(self, image, text=None, mode_task=None):\n        \n        if mode_task=='Static_FER':\n            image_features = self.clip_model.encode_image(image)\n            image_features = image_features.float()\n            image_features = self.projection_head(image_features)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        elif mode_task=='Dynamic_FER':\n            n, t, c, h, w = image.shape\n            image = image.contiguous().view(-1, c, h, w)\n            image_features = self.clip_model.encode_image(image)\n            image_features = image_features.float()\n            image_features = self.projection_head(image_features)\n            image_features = image_features.reshape(n, t, -1)\n            image_features = torch.mean(image_features, dim=1)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n        text_tokenized = clip.tokenize(text, context_length=77, truncate=True).to('cuda')\n        text_features = self.clip_model.encode_text(text_tokenized)\n        text_features = text_features.float()\n        text_features = self.projection_head(text_features)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        logit_scale = self.clip_model.logit_scale.exp()\n\n        return logit_scale, image_features, text_features\n\n\nclass Linear_Matrix_L14(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = nn.Linear(768, 2048, bias=False)\n        # self.mlp = nn.Linear(768, 4096, bias=False)\n    def forward(self, x):\n        return self.mlp(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:09:28.051844Z","iopub.execute_input":"2025-01-27T04:09:28.052140Z","iopub.status.idle":"2025-01-27T04:09:28.060738Z","shell.execute_reply.started":"2025-01-27T04:09:28.052116Z","shell.execute_reply":"2025-01-27T04:09:28.059867Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# create model and load pre_trained parameters\nmodel = ExpCLIP_Test(args)\n# model = torch.nn.DataParallel(model).cuda() \nstate_dict = model.state_dict()\npre_train_model = torch.load(pretrain_model_path, map_location=torch.device('cpu') )\n# pre_train_model = torch.load(pretrain_model_path)\nfor name, param in pre_train_model.items():\n    if \"mlp.weight\" in name:\n        state_dict[\"projection_head.mlp.weight\"].copy_(param)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:09:31.517922Z","iopub.execute_input":"2025-01-27T04:09:31.518247Z","iopub.status.idle":"2025-01-27T04:09:53.029176Z","shell.execute_reply.started":"2025-01-27T04:09:31.518221Z","shell.execute_reply":"2025-01-27T04:09:53.028317Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 890M/890M [00:09<00:00, 101MiB/s]\n<ipython-input-6-aee27e85cd16>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pre_train_model = torch.load(pretrain_model_path, map_location=torch.device('cpu') )\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ExpCLIP_Test(\n  (clip_model): CLIP(\n    (visual): VisionTransformer(\n      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): Sequential(\n          (0): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (12): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (13): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (14): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (15): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (16): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (17): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (18): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (19): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (20): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (21): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (22): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (23): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n            )\n            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (token_embedding): Embedding(49408, 768)\n    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (projection_head): Linear_Matrix_L14(\n    (mlp): Linear(in_features=768, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, txt_file, img_dir, transform=None):\n        \"\"\"\n        Args:\n            txt_file (str): Path to the .txt file with image filenames and labels.\n            img_dir (str): Path to the directory containing images.\n            transform (callable, optional): Optional transform to be applied on an image.\n        \"\"\"\n        self.img_labels = []\n        self.img_dir = img_dir\n        self.transform = transform\n\n        # Read the txt file to get the image paths and labels\n        with open(txt_file, 'r') as file:\n            for line in file.readlines():\n                # Assuming format: image_filename label\n                filename, label = line.strip().split()\n                name,ext = filename.strip().split('.')                #imagename is name.jpg  --->  name_algined.jpg\n                filename  = name + '_aligned.' + ext\n                self.img_labels.append((filename, int(label)-1))  # Store as a tuple (image_filename, label)\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_name, label = self.img_labels[idx]\n        img_path = os.path.join(self.img_dir, img_name)  # Construct the full image path\n        image = Image.open(img_path).convert('RGB')  # Open image\n\n        if self.transform:\n            image = self.transform(image)  # Apply the transformations\n\n        return image, label\n\n# Define the transformations (similar to the ones in ImageFolder)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Example usage\ntest_data_path = '/kaggle/input/rafdb-dg/aligned/aligned'  # Path to the folder containing images\ntest_txt_file = '/kaggle/input/rafdb-dg/EmoLabel/train_label.txt'  # Path to the text file listing image names and labels\n\ntest_dataset = CustomImageDataset(txt_file=test_txt_file, img_dir=test_data_path, transform=transform)\n\n# Create DataLoader for batching and shuffling\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:09:53.030184Z","iopub.execute_input":"2025-01-27T04:09:53.030397Z","iopub.status.idle":"2025-01-27T04:09:53.075427Z","shell.execute_reply.started":"2025-01-27T04:09:53.030378Z","shell.execute_reply":"2025-01-27T04:09:53.074775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"test_data = CustomImageDataset(txt_file=test_txt_file, img_dir=test_data_path,\n                              transform=transforms.Compose([transforms.Resize((224, 224)),\n                                                 transforms.ToTensor()]))\n\ntest_loader = torch.utils.data.DataLoader(test_data,\n                                              batch_size=4,\n                                              shuffle=False,\n                                              num_workers=8,\n                                              pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:10:02.777424Z","iopub.execute_input":"2025-01-27T04:10:02.777735Z","iopub.status.idle":"2025-01-27T04:10:02.802414Z","shell.execute_reply.started":"2025-01-27T04:10:02.777714Z","shell.execute_reply":"2025-01-27T04:10:02.801697Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"with torch.no_grad():\n    for i, (images, target) in enumerate(test_loader):\n    \n        images = images.to(device)\n        target = target.to(device)\n        break\n\nFER_prompt_ = {'RAFDB':prompt}\nzero_shot_prompt = FER_prompt_['RAFDB']\nn,_,_,_ = images.shape\nlogit_scale, image_features, text_features = model(image=images,text=zero_shot_prompt, mode_task=\"Static_FER\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:54:37.589860Z","iopub.execute_input":"2025-01-26T15:54:37.590163Z","iopub.status.idle":"2025-01-26T15:54:38.287581Z","shell.execute_reply.started":"2025-01-26T15:54:37.590139Z","shell.execute_reply":"2025-01-26T15:54:38.286719Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"prompt_number = int(len(zero_shot_prompt) / 7)\n\noutput = logit_scale * image_features @ text_features.t()\noutput = output.view(n, -1, prompt_number)\noutput = torch.mean(output, dim=-1)\npredicted = output.argmax(dim=1, keepdim=True)\n\npredicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:54:46.152401Z","iopub.execute_input":"2025-01-26T15:54:46.152816Z","iopub.status.idle":"2025-01-26T15:54:46.164936Z","shell.execute_reply.started":"2025-01-26T15:54:46.152781Z","shell.execute_reply":"2025-01-26T15:54:46.164108Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"tensor([[5],\n        [4],\n        [3],\n        [6]], device='cuda:0')"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:54:47.271346Z","iopub.execute_input":"2025-01-26T15:54:47.271708Z","iopub.status.idle":"2025-01-26T15:54:47.277736Z","shell.execute_reply.started":"2025-01-26T15:54:47.271676Z","shell.execute_reply":"2025-01-26T15:54:47.277041Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"tensor([4, 4, 3, 3], device='cuda:0')"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef zero_shot_test(set=0, dataset_=None, mode_task=None, FER_prompt_=None, prompt_type=None):\n\n    DATASET_PATH_MAPPING = {\n        \"RAFDB\": \"/kaggle/input/rafdb-dg/aligned/aligned\",\n        \"AffectNet7\": \"/data/EECS-IoannisLab/datasets/Static_FER_Datasets/AffectNet7_Face/test/\",\n        \"AffectNet8\": \"/data/EECS-IoannisLab/datasets/Static_FER_Datasets/AffectNet8_Face/test/\",\n        \"FERPlus\": \"/data/EECS-IoannisLab/datasets/Static_FER_Datasets/FERPlus_Face/test/\",\n        \"DFEW\": \"./annotation/DFEW_set_\"+str(set+1)+\"_test.txt\",\n        \"FERV39k\": \"./annotation/FERV39k_test.txt\",\n        \"MAFW\": \"./annotation/MAFW_set_\"+str(set+1)+\"_test.txt\",\n        \"AFEW\": \"./annotation/AFEW_validation.txt\",\n    }\n    test_data_path = DATASET_PATH_MAPPING[dataset_]\n    zero_shot_prompt = FER_prompt_[dataset_]\n    \n    if dataset_ in [\"RAFDB\", \"AffectNet7\", \"DFEW\", \"FERV39k\", \"AFEW\"]:\n        prompt_number = int(len(zero_shot_prompt) / 7)\n    elif dataset_ in [\"AffectNet8\", \"FERPlus\"]:\n        prompt_number = int(len(zero_shot_prompt) / 8)\n    elif dataset_ in [\"MAFW\"]:\n        prompt_number = int(len(zero_shot_prompt) / 11)\n\n    if mode_task == \"Static_FER\":\n        batch_size_ = 512\n        test_data = CustomImageDataset(txt_file=test_txt_file, img_dir=test_data_path,\n                                       transform=transforms.Compose([transforms.Resize((224, 224)),\n                                                                     transforms.ToTensor()]))\n        confusion_matrix_path = \"./confusion_matrix/\"+args.load_model+\"-\"+dataset_+'-'+prompt_type+'.pdf'\n    elif mode_task == \"Dynamic_FER\":\n        batch_size_ = 64\n        test_data = test_data_loader(list_file=test_data_path,\n                                     num_segments=16,\n                                     duration=1,\n                                     image_size=224)\n        confusion_matrix_path = \"./confusion_matrix/\"+args.load_model+\"-\"+dataset_+ '-' + str(set)+'-'+prompt_type+'.pdf'\n\n    test_loader = torch.utils.data.DataLoader(test_data,\n                                              batch_size=batch_size_,\n                                              shuffle=False,\n                                              num_workers=8,\n                                              pin_memory=True)\n    correct = 0\n\n    # Initialize variables for metrics\n    all_predicted = []\n    all_targets = []\n\n    # Add a progress bar using tqdm\n    with tqdm(total=len(test_loader), desc=\"Testing Progress\", unit=\"batch\") as pbar:\n        with torch.no_grad():\n            for i, (images, target) in enumerate(test_loader):\n                images = images.to(device)\n                target = target.to(device)\n                \n                n, _, _, _ = images.shape\n                logit_scale, image_features, text_features = model(image=images, text=zero_shot_prompt, mode_task=\"Static_FER\")\n\n                output = logit_scale * image_features @ text_features.t()\n                output = output.view(n, -1, prompt_number)\n                output = torch.mean(output, dim=-1)\n\n                predicted = output.argmax(dim=1, keepdim=True) % 6\n                correct += predicted.eq(target.view_as(predicted)).sum().item()\n\n                all_predicted.extend(predicted.cpu().numpy().flatten())\n                all_targets.extend(target.cpu().numpy().flatten())\n\n                pbar.update(1)\n\n    # Calculate accuracy\n    accuracy = 100. * correct / len(test_loader.dataset)\n    \n    # Calculate F1 score, precision, and recall\n    f1 = f1_score(all_targets, all_predicted, average='weighted')\n    precision = precision_score(all_targets, all_predicted, average='weighted')\n    recall = recall_score(all_targets, all_predicted, average='weighted')\n\n    # Compute confusion matrix\n    _confusion_matrix = confusion_matrix(all_targets, all_predicted)\n    np.set_printoptions(precision=4)\n    normalized_cm = _confusion_matrix.astype('float') / _confusion_matrix.sum(axis=1)[:, np.newaxis]\n    normalized_cm = normalized_cm * 100\n    list_diag = np.diag(normalized_cm)\n    uar = list_diag.mean()\n\n    # Print metrics\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"UAR: {uar:.4f}\")\n\n    # Plot normalized confusion matrix\n    plt.figure(figsize=(10, 8))\n    plt.imshow(normalized_cm, interpolation='nearest', cmap=plt.cm.Reds)\n    plt.colorbar()\n    tick_marks = np.arange(len(Emotion_name_dic[dataset_]))\n    plt.xticks(tick_marks, Emotion_name_dic[dataset_], rotation=45)\n    plt.yticks(tick_marks, Emotion_name_dic[dataset_])\n\n    fmt = '.2f'\n    thresh = normalized_cm.max() / 2.\n    for i, j in itertools.product(range(normalized_cm.shape[0]), range(normalized_cm.shape[1])):\n        plt.text(j, i, format(normalized_cm[i, j], fmt), fontsize=12,\n                 horizontalalignment=\"center\",\n                 color=\"white\" if normalized_cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=18)\n    plt.xlabel('Predicted label', fontsize=18)\n    plt.tight_layout()\n    plt.close()\n    \n    return uar, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:36:54.830938Z","iopub.execute_input":"2025-01-27T04:36:54.831270Z","iopub.status.idle":"2025-01-27T04:36:54.845797Z","shell.execute_reply.started":"2025-01-27T04:36:54.831244Z","shell.execute_reply":"2025-01-27T04:36:54.844618Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def zero_shot_test_FER(FER_prompt_,type_):\n\n    datasets_ = [\"RAFDB\",]\n    \n    for dataset in datasets_:\n        uar, war = zero_shot_test(dataset_=dataset, mode_task=\"Static_FER\", FER_prompt_=FER_prompt_, prompt_type=type_)\n        # print(f'************************* {dataset}')\n        # print(f\"UAR/WAR: {uar:.2f}/{war:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:36:57.114126Z","iopub.execute_input":"2025-01-27T04:36:57.114417Z","iopub.status.idle":"2025-01-27T04:36:57.118533Z","shell.execute_reply.started":"2025-01-27T04:36:57.114397Z","shell.execute_reply":"2025-01-27T04:36:57.117702Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class RecorderMeter(object):\n    pass    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:36:59.870650Z","iopub.execute_input":"2025-01-27T04:36:59.870972Z","iopub.status.idle":"2025-01-27T04:36:59.874942Z","shell.execute_reply.started":"2025-01-27T04:36:59.870948Z","shell.execute_reply":"2025-01-27T04:36:59.873745Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# prompt = ['Surprise',\n# 'Fear',\n# 'Disgust',\n# 'Happiness',\n# 'Sadness',\n# 'Anger',\n# 'Neutral']\n# # for key in label_mapping.keys():\n# #     prompt.append(key)\n\n# prompt\n\nemotion_words = [\n    [\"Surprise\", \"Amazement\", \"Astonishment\", \"Wonder\", \"Shock\", \"Bewilderment\"],\n    [\"Fear\", \"Terror\", \"Anxiety\", \"Dread\", \"Panic\", \"Apprehension\"],\n    [\"Disgust\", \"Revulsion\", \"Contempt\", \"Loathing\", \"Repulsion\", \"Aversion\"],\n    [\"Happiness\", \"Joy\", \"Delight\", \"Bliss\", \"Contentment\", \"Elation\"],\n    [\"Sadness\", \"Sorrow\", \"Grief\", \"Melancholy\", \"Despair\", \"Heartache\"],\n    [\"Anger\", \"Rage\", \"Fury\", \"Wrath\", \"Annoyance\", \"Resentment\"],\n    [\"Neutral\", \"Indifference\", \"Apathy\", \"Calmness\", \"Detachment\", \"Equanimity\"]\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:37:10.686985Z","iopub.execute_input":"2025-01-27T04:37:10.687273Z","iopub.status.idle":"2025-01-27T04:37:10.691969Z","shell.execute_reply.started":"2025-01-27T04:37:10.687253Z","shell.execute_reply":"2025-01-27T04:37:10.691054Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"prompt = []\nfor emotions in emotion_words:\n    prompt += emotions\n\nprompt_list = [prompt,]\n\nprompt_list.append([f'an expression of {p}' for p in prompt])\nprompt_list.append([f'a photo of a face with an expression of {p}' for p in prompt])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:37:22.073222Z","iopub.execute_input":"2025-01-27T04:37:22.073597Z","iopub.status.idle":"2025-01-27T04:37:22.078209Z","shell.execute_reply.started":"2025-01-27T04:37:22.073567Z","shell.execute_reply":"2025-01-27T04:37:22.077193Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"for p in prompt_list:\n    zero_shot_test_FER({'RAFDB':p}, 'type1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:37:24.548892Z","iopub.execute_input":"2025-01-27T04:37:24.549215Z","iopub.status.idle":"2025-01-27T04:53:22.940791Z","shell.execute_reply.started":"2025-01-27T04:37:24.549188Z","shell.execute_reply":"2025-01-27T04:53:22.939732Z"}},"outputs":[{"name":"stderr","text":"Testing Progress: 100%|██████████| 24/24 [05:18<00:00, 13.27s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 57.40%\nF1 Score: 0.5128\nPrecision: 0.5274\nRecall: 0.5740\nUAR: 47.4929\n","output_type":"stream"},{"name":"stderr","text":"Testing Progress: 100%|██████████| 24/24 [05:19<00:00, 13.33s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 61.35%\nF1 Score: 0.5296\nPrecision: 0.4848\nRecall: 0.6135\nUAR: 47.7489\n","output_type":"stream"},{"name":"stderr","text":"Testing Progress: 100%|██████████| 24/24 [05:19<00:00, 13.32s/batch]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 47.87%\nF1 Score: 0.4588\nPrecision: 0.5156\nRecall: 0.4787\nUAR: 40.9927\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"for i, FER_prompt in enumerate(FER_prompt_list):\n    print(f'************************************************************************** Zero-shot Prompt Type: ', FER_prompt_type_list[i])\n    type_ = \"type\"+str(i+1)\n    print(FER_prompt['RAFDB'][0])\n    # zero_shot_test_FER(FER_prompt, type_)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:37:52.595494Z","iopub.execute_input":"2025-01-24T10:37:52.595780Z","iopub.status.idle":"2025-01-24T10:37:52.602793Z","shell.execute_reply.started":"2025-01-24T10:37:52.595759Z","shell.execute_reply":"2025-01-24T10:37:52.601965Z"}},"outputs":[{"name":"stdout","text":"************************************************************************** Zero-shot Prompt Type:  Class Name\nhappiness.\n************************************************************************** Zero-shot Prompt Type:  An Expression of Name\nan expression of happiness.\n************************************************************************** Zero-shot Prompt Type:  A Photo of A Face with An Expression of Name\na photo of a face with an expression of happiness.\n************************************************************************** Zero-shot Prompt Type:  Expression Ensemble Five\nan expression of happiness.\n************************************************************************** Zero-shot Prompt Type:  Expression Ensemble Ten\nan expression of happiness.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}