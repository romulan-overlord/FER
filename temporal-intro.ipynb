{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9108739,"sourceType":"datasetVersion","datasetId":5473430}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riturajpradhan/temporal-intro?scriptVersionId=191804164\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.init as init\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom timm.scheduler.scheduler import Scheduler\n\nfrom torchvision import transforms\n# from transformers import AutoImageProcessor, ResNetModel\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n# from transformers.image_processing_base import BatchFeature","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T15:00:57.136473Z","iopub.execute_input":"2024-08-09T15:00:57.137585Z","iopub.status.idle":"2024-08-09T15:01:21.475814Z","shell.execute_reply.started":"2024-08-09T15:00:57.137528Z","shell.execute_reply":"2024-08-09T15:01:21.474562Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-09 15:01:04.354424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-09 15:01:04.354620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-09 15:01:04.558382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"image_paths = []\nlabels = []\nlabel_path = '/kaggle/input/abaw7-extracted-features/validation_set_annotations.txt'\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nwith open(label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_name = line[0]\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    image_path = os.path.join(data_path, image_name)\n    image_paths.append(image_path)\n    labels.append(image_label)\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T06:34:05.454535Z","iopub.execute_input":"2024-08-09T06:34:05.454904Z","iopub.status.idle":"2024-08-09T06:34:05.599724Z","shell.execute_reply.started":"2024-08-09T06:34:05.454878Z","shell.execute_reply":"2024-08-09T06:34:05.598676Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for file in os.listdir('/kaggle/working/SGD_Kaiming'):\n    if file != 'transformer_on_ViT_e80.pth':\n        os.remove(f'/kaggle/working/SGD_Kaiming/{file}')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:16:26.372401Z","iopub.execute_input":"2024-08-09T15:16:26.372834Z","iopub.status.idle":"2024-08-09T15:16:26.499443Z","shell.execute_reply.started":"2024-08-09T15:16:26.372803Z","shell.execute_reply":"2024-08-09T15:16:26.498206Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# run cell to extract image features and store them\nimage_preprocessor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nfeature_extractor = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True).to(device)\n    \n# Batch size for processing images\nbatch_size = 100\n\nbatch_count = 0\n\n# Initialize lists to store extracted features\nall_features = []\n\n# Process images in batches\nfor batch_start in tqdm(range(0, len(image_paths), batch_size), desc='Extracting Features'):\n    batch_end = min(batch_start + batch_size, len(image_paths))\n    batch_images = [Image.open(image_path) for image_path in image_paths[batch_start:batch_end]]\n    batch_labels = torch.tensor([float(label) for label in labels[batch_start:batch_end]]).to(device)\n    # Tokenize and extract features\n    inputs = image_preprocessor(images = batch_images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = feature_extractor(**inputs)\n    batch_features = output.hidden_states[-1][:,0,:].clone().detach()\n    t = torch.column_stack([batch_features, batch_labels])\n\n    # Append features to the list\n    all_features.extend(t)\n\n# Save features to a file (e.g., as a PyTorch tensor)\n    # Save features every 1000 images\n    if batch_count%50 == 0:\n        output_file = f\"./validation_features/image_features_val_{batch_count}.pt\"\n        torch.save(torch.stack(all_features).cpu(), output_file)\n        print(f\"Features saved to {output_file}\")\n        \n        # Clear memory by resetting the list\n        all_features = []\n    batch_count += 1\n    with open('counter.json', 'w') as f:\n        json.dump({'batch_count' : batch_count}, f)\n\n# Save any remaining features\nif all_features:\n    output_file = f\"./validation_features/image_features_val_{batch_count}.pt\"\n    torch.save(torch.stack(all_features).cpu(), output_file)\n    print(f\"Remaining features saved to {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:14:43.228169Z","iopub.execute_input":"2024-08-07T06:14:43.228569Z","iopub.status.idle":"2024-08-07T06:17:27.290458Z","shell.execute_reply.started":"2024-08-07T06:14:43.228541Z","shell.execute_reply":"2024-08-07T06:17:27.289385Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting Features:   0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5971aedbbd0141cb8b45a2575d139781"}},"metadata":{}},{"name":"stdout","text":"Features saved to ./validation_features/image_features_val_0.pt\nFeatures saved to ./validation_features/image_features_val_50.pt\nFeatures saved to ./validation_features/image_features_val_100.pt\nFeatures saved to ./validation_features/image_features_val_150.pt\nRemaining features saved to ./validation_features/image_features_val_155.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"path = '/kaggle/working/validation_features'\nfeat_arr = []\nfor file in os.listdir(path):\n    feat_arr.append(torch.load(os.path.join(path, file)))\n\ntraining_features = torch.row_stack(feat_arr)\ntraining_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:22:16.906892Z","iopub.execute_input":"2024-08-07T06:22:16.907268Z","iopub.status.idle":"2024-08-07T06:22:16.967469Z","shell.execute_reply.started":"2024-08-07T06:22:16.90724Z","shell.execute_reply":"2024-08-07T06:22:16.966451Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([15440, 769])"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(training_features, 'ABAW_validation_features_ViT.pt')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:22:28.725937Z","iopub.execute_input":"2024-08-07T06:22:28.726803Z","iopub.status.idle":"2024-08-07T06:22:28.784512Z","shell.execute_reply.started":"2024-08-07T06:22:28.726768Z","shell.execute_reply":"2024-08-07T06:22:28.783235Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class ImageTransformer(nn.Module):\n    def __init__(self, feature_dim, num_classes, num_heads=4, num_layers=6, dropout=0.1, sequence_length=64):\n        super(ImageTransformer, self).__init__()\n        self.feature_dim = feature_dim\n        self.num_classes = num_classes\n        \n        # Positional encoding\n        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length + 1, feature_dim))\n        \n        # Learnable [CLS] token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Classification head\n        self.fc1 = nn.Linear(feature_dim, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, num_classes)\n        \n         # Initialize weights\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        # Apply Kaiming initialization to all linear layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n\n    def forward(self, x):\n        # Add the [CLS] token to the input sequence\n        batch_size = x.size(0)\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, feature_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, sequence_length + 1, feature_dim)\n        \n        # Add positional encoding\n        x = x + self.positional_encoding[:, :x.size(1), :]\n        \n        # Pass through the transformer encoder\n        x = self.transformer_encoder(x)\n        \n        # Extract the [CLS] token's output\n        cls_output = x[:, 0, :]  # (batch_size, feature_dim)\n        \n        # Pass through the classification head\n        x = self.fc1(cls_output)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        \n        return x\n\nclass ABAWFeatureDataset(Dataset):\n    def __init__(self, features, labels, sequence_length):\n        self.features = features\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.seq_start = 0\n        self.seq_end = sequence_length\n        self.length = labels.shape[0]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if idx > (self.length - self.sequence_length):\n            feature = self.features[self.length - self.sequence_length:, : ]\n            label = self.labels[self.length - 1]\n        else:\n            feature = self.features[idx:idx + self.sequence_length, :]\n            label = self.labels[idx + self.sequence_length - 1]\n        return feature, label","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:01:50.663128Z","iopub.execute_input":"2024-08-09T15:01:50.663565Z","iopub.status.idle":"2024-08-09T15:01:51.037602Z","shell.execute_reply.started":"2024-08-09T15:01:50.663533Z","shell.execute_reply":"2024-08-09T15:01:51.036414Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nsequence_length = 32\nfeature_dim = 768\nnum_classes = 8\nnum_epochs = 20\nlearning_rate = 1e-4\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nval_label_path = '/kaggle/input/abaw7-extracted-features/validation_set_annotations.txt'\nval_labels = []\nwith open(val_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    val_labels.append(image_label)\n    \ntrain_label_path = '/kaggle/input/abaw7-extracted-features/training_set_annotations.txt'\ntrain_labels = []\nwith open(train_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    train_labels.append(image_label)\n    \n# Create dataset and dataloader\ntrain_image_features = torch.load('/kaggle/working/ABAW_training_features_ViT.pt')\ntemp = train_image_features.detach().numpy()\ntrain_image_features = torch.tensor(temp[:,:768])\n\ntrain_labels = [int(x) for x in train_labels]\ntrain_label_tensor = torch.tensor(train_labels)\ntrain_dataset = ABAWFeatureDataset(train_image_features, train_label_tensor, sequence_length = sequence_length)\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, drop_last = True, pin_memory=True)\n\ntrain_length = len(train_dataset)\n    \nval_image_features = torch.load('/kaggle/working/ABAW_validation_features_ViT.pt')\ntemp = val_image_features.detach().numpy()\nval_image_features = torch.tensor(temp[:,:768])\nval_labels = [int(x) for x in val_labels]\nval_label_tensor = torch.tensor(val_labels)\nval_dataset = ABAWFeatureDataset(val_image_features, val_label_tensor, sequence_length = sequence_length)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, drop_last = True, pin_memory=True)#, collate_fn=custom_collate_fn)\n\nval_length = len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:01:52.645038Z","iopub.execute_input":"2024-08-09T15:01:52.64611Z","iopub.status.idle":"2024-08-09T15:01:54.022206Z","shell.execute_reply.started":"2024-08-09T15:01:52.646075Z","shell.execute_reply":"2024-08-09T15:01:54.021109Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class LinearLRScheduler(Scheduler):\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 lr_min_rate: float,\n                 warmup_t=0,\n                 warmup_lr_init=0.,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True,\n                 ) -> None:\n        super().__init__(\n            optimizer, param_group_field=\"lr\",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        self.t_initial = t_initial\n        self.lr_min_rate = lr_min_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            t = t - self.warmup_t\n            total_t = self.t_initial - self.warmup_t\n            lrs = [v - ((v - v * self.lr_min_rate) * (t / total_t)) for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None","metadata":{"execution":{"iopub.status.busy":"2024-08-09T06:39:33.651145Z","iopub.execute_input":"2024-08-09T06:39:33.652108Z","iopub.status.idle":"2024-08-09T06:39:33.663838Z","shell.execute_reply.started":"2024-08-09T06:39:33.652065Z","shell.execute_reply":"2024-08-09T06:39:33.662912Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Model, loss function, optimizer\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\n# model.load_state_dict(torch.load('/kaggle/working/transformer_on_ViT_e40.pth'))\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0.1, weight_decay = 1e-4)\nscaler = GradScaler()\nwriter = SummaryWriter('SGD_Kaiming_monitor')\nscheduler = LinearLRScheduler(\n            optimizer,\n            t_initial=100,\n            lr_min_rate=0.01,\n            warmup_lr_init=5e-5,\n            warmup_t=3,\n            t_in_epochs=False,\n        )\n# training_loss_list = []\n# validation_loss_list = []\n# loss_list = []\n# Training loop\nfor epoch in range(41,140):\n    model.train()\n    running_loss = 0.0\n    progress_bar = tqdm(total=len(data_loader_train), desc=f'Epoch {epoch+1}/{num_epochs} LR = {optimizer.param_groups[0][\"lr\"]}', unit='batch')\n    \n    for i, (features, targets) in enumerate(data_loader_train):\n        features, targets = features.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(features)\n            # Calculate loss\n            loss = criterion(outputs, targets)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * features.size(0)\n#         loss_list.append(loss.item())\n#         writer.add_scalar('training_loss', loss.item(), epoch * train_length + i)\n        # Update progress bar with current loss\n        progress_bar.set_postfix(loss=loss.item())\n        progress_bar.update(1)\n        \n    epoch_loss = running_loss / train_length\n    writer.add_scalar('training_epoch_loss', epoch_loss, epoch)\n    if epoch % 10 == 0:\n        torch.save(model.state_dict(), f'/kaggle/working/SGD_Kaiming/transformer_on_ViT_e{epoch}.pth')\n    model.eval()\n    running_loss = 0.0\n    all_preds = []\n    all_targets = [] \n    with torch.no_grad():\n#         val_progress_bar = tqdm(total=len(data_loader_val), desc=f'Validation', unit='batch')\n        for features, targets in data_loader_val:\n            features, targets = features.cuda(), targets.cuda() \n            outputs = model(features)\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            # Calculate loss\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            validation_loss = criterion(outputs, targets)\n            running_loss += validation_loss.item() * features.size(0)\n#             val_progress_bar.set_postfix(validation_loss=validation_loss.item())\n#             val_progress_bar.update(1)\n    \n    validation_loss = running_loss / val_length\n    scheduler.step(validation_loss)\n    \n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    f1 = f1_score(all_targets, all_preds, average='weighted')\n    precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='weighted')\n    accuracy = accuracy_score(all_targets, all_preds)\n\n    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n    writer.add_scalar('validation_epoch_loss', validation_loss, epoch)\n    writer.add_scalar('f1_score', f1, epoch)\n    writer.add_scalar('precision', precision, epoch)\n    writer.add_scalar('recall', recall, epoch)\n    writer.add_scalar('accuracy', accuracy, epoch)\n\n#     training_loss_list.append(epoch_loss)\n#     validation_loss_list.append(validation_loss)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, validation Loss: {validation_loss:.4f}')\n    progress_bar.close()\n\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T06:39:35.596153Z","iopub.execute_input":"2024-08-09T06:39:35.59684Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 42/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2faf439e711742eaa174b7ec2d1ccea4"}},"metadata":{}},{"name":"stdout","text":"Epoch 42/20, Training Loss: 2.0681, validation Loss: 2.0516\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 43/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76c3cd4f41345e0b11eae527778149f"}},"metadata":{}},{"name":"stdout","text":"Epoch 43/20, Training Loss: 2.0458, validation Loss: 2.0359\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 44/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1937969df74f04ac798a45f83124ae"}},"metadata":{}},{"name":"stdout","text":"Epoch 44/20, Training Loss: 2.0253, validation Loss: 2.0216\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 45/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbd39d8b160497598465c4aca5aa7e9"}},"metadata":{}},{"name":"stdout","text":"Epoch 45/20, Training Loss: 2.0064, validation Loss: 2.0087\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 46/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f4433642d63439fb426b0c194a0e43c"}},"metadata":{}},{"name":"stdout","text":"Epoch 46/20, Training Loss: 1.9891, validation Loss: 1.9970\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 47/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880c526eea3a4d59bd692cb7d832ff1b"}},"metadata":{}},{"name":"stdout","text":"Epoch 47/20, Training Loss: 1.9731, validation Loss: 1.9865\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 48/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3171b184e189494eb7405e177ea0b0c5"}},"metadata":{}},{"name":"stdout","text":"Epoch 48/20, Training Loss: 1.9584, validation Loss: 1.9770\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 49/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d2c9ab25d9401793c7b93a18368c0f"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch 77/20, Training Loss: 1.8066, validation Loss: 1.9013\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 78/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f98e458f85754378a250385943b88019"}},"metadata":{}},{"name":"stdout","text":"Epoch 78/20, Training Loss: 1.8053, validation Loss: 1.9012\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 79/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264f9abc219d4b0e9355a7ed73d85d8e"}},"metadata":{}},{"name":"stdout","text":"Epoch 79/20, Training Loss: 1.8041, validation Loss: 1.9011\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 80/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff2b63c92d94e8eb3d41085a8cbd2ee"}},"metadata":{}},{"name":"stdout","text":"Epoch 80/20, Training Loss: 1.8029, validation Loss: 1.9010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 81/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73ca7a2a78d4b4fbe9f8b28155f7046"}},"metadata":{}},{"name":"stdout","text":"Epoch 81/20, Training Loss: 1.8019, validation Loss: 1.9009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 82/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c988aecd24af486d8b573930f1561c93"}},"metadata":{}},{"name":"stdout","text":"Epoch 82/20, Training Loss: 1.8009, validation Loss: 1.9009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 83/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8ba9cf1e994716ad74ed4c6521bd1d"}},"metadata":{}},{"name":"stdout","text":"Epoch 83/20, Training Loss: 1.8000, validation Loss: 1.9009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 84/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539a632554724780a06555e7e902c1bc"}},"metadata":{}},{"name":"stdout","text":"Epoch 84/20, Training Loss: 1.7991, validation Loss: 1.9009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 85/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f432056ed817418b9e12ec6db329035f"}},"metadata":{}},{"name":"stdout","text":"Epoch 85/20, Training Loss: 1.7984, validation Loss: 1.9009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 86/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a03dbf896834d17872de9c8fd72150f"}},"metadata":{}},{"name":"stdout","text":"Epoch 86/20, Training Loss: 1.7977, validation Loss: 1.9010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 87/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe5450aa8a14e24bf4ef99c88f45fdf"}},"metadata":{}},{"name":"stdout","text":"Epoch 87/20, Training Loss: 1.7969, validation Loss: 1.9010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 88/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9208a8aca37a4c3b8882b3cac5944e3e"}},"metadata":{}},{"name":"stdout","text":"Epoch 88/20, Training Loss: 1.7963, validation Loss: 1.9011\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 89/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5692414705194248bac8ffb02f38e34f"}},"metadata":{}},{"name":"stdout","text":"Epoch 89/20, Training Loss: 1.7957, validation Loss: 1.9012\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 90/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db22680a119b4191abdf2fb32d305808"}},"metadata":{}},{"name":"stdout","text":"Epoch 90/20, Training Loss: 1.7951, validation Loss: 1.9013\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 91/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b67e7fe454a4bd4be2c20f8d8adaca6"}},"metadata":{}},{"name":"stdout","text":"Epoch 91/20, Training Loss: 1.7946, validation Loss: 1.9014\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 92/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4eed5ec4b14828a7272c98a4d2ca2f"}},"metadata":{}},{"name":"stdout","text":"Epoch 92/20, Training Loss: 1.7940, validation Loss: 1.9015\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 93/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0c3b579173462e8b7ea24409a79e78"}},"metadata":{}},{"name":"stdout","text":"Epoch 93/20, Training Loss: 1.7936, validation Loss: 1.9016\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 94/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c13a3c3ed04bbcb07cc8966702ede5"}},"metadata":{}},{"name":"stdout","text":"Epoch 94/20, Training Loss: 1.7931, validation Loss: 1.9017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 95/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21675556f9204a258513c33aa17bba07"}},"metadata":{}},{"name":"stdout","text":"Epoch 95/20, Training Loss: 1.7927, validation Loss: 1.9018\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 96/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1da861d63d834454b8dfc060369f33c3"}},"metadata":{}},{"name":"stdout","text":"Epoch 96/20, Training Loss: 1.7923, validation Loss: 1.9019\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 97/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c52e3fec62459b9e736336f0eb5948"}},"metadata":{}},{"name":"stdout","text":"Epoch 97/20, Training Loss: 1.7919, validation Loss: 1.9021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 98/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54cbad44167484cb8c1540e9f713367"}},"metadata":{}},{"name":"stdout","text":"Epoch 98/20, Training Loss: 1.7915, validation Loss: 1.9022\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 99/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e22ee54ef7a4736a70cd67fbfd01440"}},"metadata":{}},{"name":"stdout","text":"Epoch 99/20, Training Loss: 1.7913, validation Loss: 1.9023\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 100/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34454ac644e491fa8215c6d2ebfdf83"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch 101/20, Training Loss: 1.7906, validation Loss: 1.9026\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 102/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c090570cfd44b7884e04a5a63eeea84"}},"metadata":{}},{"name":"stdout","text":"Epoch 102/20, Training Loss: 1.7904, validation Loss: 1.9027\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 103/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74aa810466684f088e37c14b6e835587"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch 104/20, Training Loss: 1.7899, validation Loss: 1.9029\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 105/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d4ad9a8f5c4a60b6fd67374694317a"}},"metadata":{}},{"name":"stdout","text":"Epoch 127/20, Training Loss: 1.7867, validation Loss: 1.9057\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 128/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd93407086ae4d2e908ded16b3fc2344"}},"metadata":{}},{"name":"stdout","text":"Epoch 128/20, Training Loss: 1.7865, validation Loss: 1.9058\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 129/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbe31567ed1483ab9ddd5c966187888"}},"metadata":{}},{"name":"stdout","text":"Epoch 129/20, Training Loss: 1.7865, validation Loss: 1.9059\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 130/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f70df25a2bdf4401ad985c8bceafa6d5"}},"metadata":{}},{"name":"stdout","text":"Epoch 131/20, Training Loss: 1.7864, validation Loss: 1.9061\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 132/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d6fdd226c44c78bae9e431eb0e79b7"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch 132/20, Training Loss: 1.7863, validation Loss: 1.9062\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 133/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e696fda057740eb9f97c1fd21af886b"}},"metadata":{}},{"name":"stdout","text":"Epoch 134/20, Training Loss: 1.7862, validation Loss: 1.9064\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 135/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4284aefdfcf84fa780d294dfa8bde465"}},"metadata":{}},{"name":"stdout","text":"Epoch 136/20, Training Loss: 1.7861, validation Loss: 1.9066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 137/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7fd52d466424fad98d763c2765cb64a"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch 137/20, Training Loss: 1.7860, validation Loss: 1.9067\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 138/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6dc16ad29ef4d46853116f28cdd975f"}},"metadata":{}},{"name":"stdout","text":"Epoch 138/20, Training Loss: 1.7860, validation Loss: 1.9068\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 139/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c316851a5aa5431480ccaa0ecc690075"}},"metadata":{}},{"name":"stdout","text":"Epoch 139/20, Training Loss: 1.7859, validation Loss: 1.9068\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 140/20 LR = 5e-05:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d900b5a140245f9b8b0aeff6a6bf740"}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"}]},{"cell_type":"code","source":"torch.argmax(outputs, dim = 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T06:43:47.321023Z","iopub.execute_input":"2024-08-08T06:43:47.321407Z","iopub.status.idle":"2024-08-08T06:43:47.329839Z","shell.execute_reply.started":"2024-08-08T06:43:47.32138Z","shell.execute_reply":"2024-08-08T06:43:47.329022Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testing new feature extractor","metadata":{}},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state,","metadata":{"execution":{"iopub.status.busy":"2024-08-07T04:46:47.151621Z","iopub.execute_input":"2024-08-07T04:46:47.151977Z","iopub.status.idle":"2024-08-07T04:46:50.533433Z","shell.execute_reply.started":"2024-08-07T04:46:47.151948Z","shell.execute_reply":"2024-08-07T04:46:50.532129Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040b2960c7154fdf8086db4fb9589a34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b6398e5f14449d87f66db83a2ac8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc93215a91c542ec9c4258d1bb48275e"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimages = [image for i in range(1,32)]\nlabels = [i for i in range(1,32)]\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True)\n\ninputs = feature_extractor(images=images, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\n# predicted_class_idx = logits.argmax(-1).item()\n# print(\"Predicted class:\", model.config.id2label[predicted_class_idx]),","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:15:41.071334Z","iopub.execute_input":"2024-08-07T05:15:41.072339Z","iopub.status.idle":"2024-08-07T05:15:48.509017Z","shell.execute_reply.started":"2024-08-07T05:15:41.072296Z","shell.execute_reply":"2024-08-07T05:15:48.508199Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# outputs.hidden_states[-1][:,0,:]\nimport torch\nt = torch.column_stack([outputs.hidden_states[-1][:,0,:], torch.tensor(labels)])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:16:50.498825Z","iopub.execute_input":"2024-08-07T05:16:50.499185Z","iopub.status.idle":"2024-08-07T05:16:50.505383Z","shell.execute_reply.started":"2024-08-07T05:16:50.499156Z","shell.execute_reply":"2024-08-07T05:16:50.504416Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"t[0,:]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:17:22.258295Z","iopub.execute_input":"2024-08-07T05:17:22.258632Z","iopub.status.idle":"2024-08-07T05:17:22.273578Z","shell.execute_reply.started":"2024-08-07T05:17:22.258608Z","shell.execute_reply":"2024-08-07T05:17:22.272686Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"tensor([ 2.3126e+00,  5.5116e+00,  1.1788e+01,  5.7725e-01,  6.5475e+00,\n        -2.9125e+00,  4.5668e+00, -1.3786e+00,  6.1539e+00, -5.1831e+00,\n         4.9711e+00,  8.4007e-01,  7.7080e+00, -3.0897e+00, -3.2443e+00,\n         9.7258e+00,  1.1146e+00, -2.1286e+00,  8.7621e+00,  1.6315e+00,\n        -8.4574e+00,  1.8422e+00,  1.4254e+00,  6.5619e+00, -1.0730e+01,\n         6.0743e+00,  4.2650e+00,  6.0531e+00,  8.9479e+00,  2.5177e+00,\n         5.4446e-01,  1.4944e+00,  4.0779e+00,  1.0520e+01, -2.9379e+00,\n         6.8438e+00, -4.6463e+00, -3.0405e+00,  1.0135e+00,  8.2927e+00,\n         9.9011e+00, -3.0263e+00,  7.5373e-01, -4.1869e+00,  2.0160e+00,\n         7.0067e+00,  7.9857e-02, -2.2949e+00,  2.2206e+00, -4.4853e+00,\n         1.8516e+00, -7.8394e-01,  4.4800e+00, -3.8795e+00, -1.1654e+01,\n        -2.1962e+00,  2.5946e-01,  1.0626e+01, -5.4844e-01, -2.7896e+00,\n         1.3051e+01,  8.1810e+00,  4.4351e+00, -6.2068e+00,  1.0071e+00,\n        -4.3451e+00,  2.8967e+00, -1.7342e+00,  5.2872e+00, -4.1522e+00,\n        -9.2688e+00,  9.9553e+00, -3.6386e+00, -2.5019e+00, -8.4062e+00,\n         4.9192e-01, -9.5148e+00, -6.4460e+00, -3.4781e+00, -1.1284e+00,\n         8.1581e+00, -1.0863e+01, -1.0140e+01,  8.8340e+00,  7.9968e+00,\n         5.1645e+00, -3.4996e+00,  1.1152e+01,  4.4518e+00, -1.2614e+00,\n         1.7204e+01, -4.9011e-01,  5.3941e+00,  7.6976e+00,  1.5465e-01,\n        -6.2431e+00, -3.6648e+00, -4.9716e-01,  1.6759e+00, -1.0212e+01,\n        -4.2299e+00,  4.3840e+00, -7.6967e-01,  6.4818e-01, -1.2815e+00,\n         4.0404e+00, -2.7555e+00,  8.0193e+00, -4.0383e+00, -2.7090e+00,\n        -3.0092e+00, -2.2504e+00,  1.4036e+00, -1.9877e+00, -5.8852e-01,\n         4.5092e+00, -9.2577e+00,  8.3078e+00, -2.5480e+00, -2.3309e+00,\n         8.1919e+00,  1.7041e+00, -6.2756e+00, -5.1414e-01,  2.3397e+00,\n         1.4869e+00, -3.0239e+00, -1.0958e+01,  2.1754e+00, -1.1177e-01,\n         5.2057e+00, -4.3006e+00,  5.6387e+00, -1.9813e+00,  5.1607e+00,\n         7.4258e-01,  5.1081e+00, -4.8063e+00, -3.0374e+00,  5.3849e+00,\n        -2.8684e+00, -3.7519e+00,  5.6947e+00,  4.2382e+00, -1.0009e+00,\n        -2.1228e-01,  3.3212e+00, -8.0935e+00,  5.2201e+00, -6.6542e+00,\n        -8.2925e+00, -7.8512e+00,  4.4826e+00,  2.8323e+00, -2.7104e+00,\n         1.2681e+01,  4.5946e+00,  1.5467e+00, -8.7506e+00, -1.1305e+01,\n         3.3003e+00, -7.4826e+00,  5.0542e+00, -1.7443e+00, -1.0060e+00,\n         5.6709e+00, -3.6740e+00,  1.5832e+01,  1.2297e+01,  3.3019e+00,\n         1.9885e+00, -9.0401e-01,  5.0012e-01,  1.2189e+01, -1.2169e+00,\n         2.1518e+00,  3.6889e+00,  1.9564e+00,  7.4761e+00, -8.1639e-01,\n         3.7544e+00, -4.9893e+00, -8.8977e+00, -1.5014e-01, -1.5335e-01,\n        -7.6970e+00, -5.8783e+00,  2.3438e+00, -1.0910e+01, -7.3558e+00,\n        -1.3294e-01,  1.1221e+00,  1.8191e+00,  4.9105e+00, -9.0446e+00,\n         1.1171e+01,  2.4723e+00, -6.4088e+00, -4.9586e+00,  1.7104e+01,\n        -7.0072e-01, -9.1785e+00,  9.9637e-01, -8.4077e+00,  4.5781e-01,\n         1.4394e+00,  4.4015e+00,  7.7832e+00,  1.5674e+01,  9.7669e+00,\n         9.5864e+00,  3.4548e-01, -9.5140e+00, -3.6262e+00, -3.3511e-02,\n         9.5605e-01,  2.7343e+00, -7.1423e+00, -4.8976e+00,  8.2165e+00,\n        -1.0892e+01,  5.3325e+00, -6.9168e-01, -6.0497e+00, -3.9966e+00,\n         2.9761e+00, -4.5020e+00, -4.4989e+00, -2.4625e+00, -2.3799e+00,\n         3.4785e+00, -6.4768e-02, -1.4270e+01,  7.9958e-01, -8.5005e+00,\n        -6.1762e+00,  1.3880e+00,  4.5403e-01,  1.8790e+00, -5.1056e+00,\n        -9.7338e+00, -8.4601e+00,  2.7580e+00,  5.5546e+00, -3.5314e+00,\n         6.0557e+00, -2.3313e+00,  9.2037e+00,  4.2733e+00, -1.5292e+01,\n        -3.3109e-01, -2.5005e-01,  1.5867e+00, -4.6539e+00, -1.3627e+00,\n        -2.7924e-01, -1.0277e+00,  2.2768e+00,  7.8121e+00, -5.0762e+00,\n        -4.3243e+00,  6.6483e-01,  2.3120e+00, -1.0160e+00,  3.1079e+00,\n         4.0078e+00,  7.6681e+00,  2.7114e+00,  2.2636e+00,  6.3449e+00,\n         1.2057e+00,  9.2436e+00, -3.8165e+00, -4.0865e+00, -5.5988e+00,\n        -9.9585e+00,  5.4400e+00,  4.3910e-01,  4.3431e+00, -6.1047e+00,\n        -3.3372e+00, -5.3751e+00,  5.7061e+00,  5.4730e+00,  8.4846e+00,\n        -5.4949e+00, -9.1157e-01,  6.9689e-01,  7.5254e-02,  7.1937e+00,\n        -3.3917e+00, -5.1809e+00, -8.3977e-01,  6.8584e+00, -8.4331e+00,\n         5.7394e+00, -3.7869e+00,  4.9977e+00,  3.0823e+00,  7.9278e+00,\n        -4.2151e+00, -1.0965e+00,  2.5793e+00, -2.4594e+00,  5.3281e+00,\n        -1.2227e+01, -6.5901e-01,  1.7196e+01,  8.4178e+00, -5.5776e+00,\n         8.9550e+00, -9.1851e-01,  1.3255e+00,  1.1535e+01,  4.6121e+00,\n        -1.1908e+00,  4.8861e+00,  3.8148e+00,  3.0933e+00, -3.4191e+00,\n         8.8850e+00,  6.3427e+00, -8.7499e-01, -5.0558e+00, -3.7724e+00,\n        -2.2231e+00,  6.4751e+00, -6.2604e+00, -8.7503e+00,  3.7453e+00,\n        -4.1096e+00,  5.5612e+00, -7.2538e+00, -8.7126e+00,  2.3858e+00,\n        -7.6081e+00, -4.0343e+00,  1.6607e+01,  3.2290e+00, -1.1992e+01,\n         1.6804e+00,  1.0575e+01,  1.2953e+01,  5.3291e+00,  4.4644e+00,\n        -1.1186e+01,  7.1904e-02,  6.4786e+00,  1.3749e+01, -1.2863e+00,\n         2.1389e-01,  5.9342e+00,  4.6504e-01,  1.7982e-01, -8.1117e+00,\n        -7.9560e+00, -6.3646e+00, -4.2088e+00,  6.7474e+00, -8.9446e-01,\n        -6.3456e-01,  2.3697e+00,  1.0983e+01, -1.0096e+01, -1.4992e+00,\n        -1.3370e-01, -1.0659e+01,  1.0049e+01, -4.2879e+00, -2.5807e+00,\n         6.9250e+00, -5.1344e+00, -7.8552e+00,  2.5728e+00, -4.0974e+00,\n        -1.0583e+01,  4.3317e+00,  1.5247e+00, -7.4624e-01,  8.0111e-03,\n        -8.0616e+00,  3.6085e+00,  4.4804e+00,  2.6864e+00, -3.3090e+00,\n         3.8922e+00, -4.6573e+00,  6.0626e-02,  9.8770e+00,  5.9841e+00,\n         4.6795e+00, -9.9704e+00, -1.1974e+01,  1.3528e+01, -1.0259e+01,\n         2.6449e+00,  2.7114e+00,  3.6057e+00,  7.8294e-01,  7.4036e+00,\n        -4.2393e+00, -9.4118e+00, -2.5294e+00,  4.4294e+00,  2.2434e+00,\n        -8.6981e+00, -4.1438e+00,  6.2667e+00, -2.0602e+00,  3.7619e+00,\n        -3.0744e+00, -9.6640e+00, -2.5041e+00,  6.3875e+00, -1.0970e+01,\n         3.7260e+00,  5.1450e+00,  7.0782e+00, -6.6857e-01,  1.0461e+01,\n        -1.5351e+01,  1.4029e+01, -2.8778e+00,  3.1665e+00,  9.0058e-01,\n        -1.2030e+00, -7.5055e+00, -1.5109e+00,  4.1435e-01,  6.8360e+00,\n        -2.4361e+00,  4.7261e+00, -1.6767e+00,  3.2715e+00, -1.3578e-01,\n        -3.1394e+00, -2.9984e+00,  6.4536e+00, -5.1250e+00,  1.6784e+00,\n        -4.7196e+00, -2.6147e+00,  7.6760e-01, -8.7019e+00, -8.0888e-02,\n        -6.9515e+00, -1.5935e+00,  1.5916e+00, -3.4638e+00,  1.1822e+00,\n        -8.6731e+00,  3.6638e+00, -1.4270e+00,  8.3378e+00, -7.6860e+00,\n        -4.1551e+00, -9.2020e+00, -7.6699e-01, -3.3690e+00,  5.0199e+00,\n        -9.9882e+00,  4.2354e+00,  6.6258e+00,  9.0864e-01,  1.8286e+00,\n         2.9860e+00,  4.2116e+00, -1.8121e+00, -4.0877e+00, -2.8210e+00,\n         5.7568e+00, -4.4066e+00, -3.5520e+00, -2.6152e+00,  4.3140e+00,\n        -3.8031e+00,  9.6800e+00,  8.8169e-01, -3.6339e+00,  2.3514e+00,\n        -5.7335e+00, -4.2699e+00, -7.8389e+00, -5.9741e+00, -6.1156e-01,\n        -4.1624e+00,  1.0402e+01, -7.2951e+00,  2.6416e+00, -8.3029e+00,\n        -6.5166e-02, -4.8866e+00,  2.4596e+00,  3.9736e+00,  5.4901e+00,\n         4.2202e+00, -3.8306e+00, -6.2605e+00,  2.4231e+00, -4.0871e+00,\n        -6.3616e+00, -1.9237e+00,  2.6201e-01,  3.9231e+00, -4.2554e+00,\n         3.0737e+00, -7.9324e+00,  7.4980e-01,  6.2263e+00, -6.8387e+00,\n         6.5169e+00,  9.6261e+00,  3.0533e+00,  5.7031e+00,  5.2647e-01,\n         1.1886e+01,  4.2957e+00, -5.9036e+00,  5.3113e+00, -4.7593e+00,\n         2.3902e+00,  1.1530e+01,  1.1453e+00,  7.0914e+00,  4.4221e+00,\n         3.8341e+00,  1.3872e+00, -1.1134e+01, -1.0338e+01,  4.0840e-01,\n        -6.0602e+00,  4.3299e-01,  9.8048e-01, -2.1421e+00,  1.0620e+00,\n        -4.7048e+00, -3.7180e+00,  2.4049e+00,  1.8676e-01,  6.6197e-01,\n        -1.0010e+01, -4.6739e+00, -1.0513e+01, -2.6404e+00, -2.6510e+00,\n         7.2716e+00,  1.0749e+01,  6.6575e+00, -2.3899e+00, -7.3637e+00,\n        -3.9341e+00,  7.1931e+00, -2.3082e+00,  3.2126e+00, -2.4925e+00,\n         3.2683e+00, -5.0204e-01,  2.8221e+00, -8.1911e+00, -3.4407e+00,\n        -3.3204e+00,  4.8306e+00, -1.5854e-01,  2.6516e+00,  8.1704e-01,\n        -8.7983e+00,  4.0943e+00, -5.6544e+00,  4.1259e+00, -7.5912e-01,\n         2.2961e+00, -7.4419e+00, -6.0864e+00, -5.4809e-01, -8.6358e+00,\n         5.7299e+00, -5.8645e+00, -2.8637e+00,  4.1994e+00, -1.2684e+00,\n         4.6839e+00,  6.0358e+00, -4.8966e+00,  5.9101e+00, -1.6126e+00,\n        -1.4668e+01, -1.3364e+00, -8.6564e+00,  4.0443e+00,  8.1136e+00,\n         4.7079e+00, -9.9475e+00,  2.6451e+00,  2.7019e+00,  9.4108e+00,\n         4.5954e+00,  2.6015e+00, -1.4042e+00, -4.1166e+00, -9.4354e+00,\n         5.7500e+00, -6.5840e+00,  1.6345e+00, -3.9349e+00,  9.7327e+00,\n        -6.1233e+00,  3.4738e+00, -4.5577e+00, -1.3380e+00,  1.8434e+00,\n         8.1936e+00, -8.6092e+00, -3.4719e+00,  5.0703e+00, -4.4664e+00,\n         6.4704e+00, -5.7986e+00,  8.3599e+00, -2.7207e+00, -3.5988e+00,\n         1.0794e+01,  4.5068e+00,  1.0046e+01, -2.2723e+00, -6.5263e+00,\n        -2.2766e-01, -8.6315e-01, -6.4219e+00, -6.8218e+00, -3.5682e+00,\n        -4.7119e+00, -1.8907e-01,  5.3918e+00,  1.3922e+00, -3.0035e+00,\n         8.9866e+00, -2.6661e+00, -6.6728e+00,  1.1000e+01,  2.2653e+00,\n        -7.9431e+00,  1.8294e+00,  3.0557e+00,  9.4934e+00, -3.0099e+00,\n        -1.1694e+00, -2.3632e+00,  1.2139e+01, -4.4225e-01, -5.4353e+00,\n         6.4613e+00, -5.7222e+00, -8.5285e+00, -4.6763e+00,  4.3144e+00,\n         1.4013e-01, -2.5612e+00,  7.9967e-01,  7.7650e+00, -3.3219e+00,\n         5.2308e+00,  6.5145e-01,  8.7892e-01, -7.4683e+00,  1.0657e+01,\n         4.7196e+00, -1.6202e+00, -1.1049e+01, -3.5860e+00, -1.7874e+00,\n        -4.9695e-01,  2.7570e-01,  1.9470e+00, -2.2283e+00, -6.6657e-01,\n         2.4241e+00,  9.1253e+00,  4.5713e+00,  1.9809e+01, -4.0868e-01,\n        -7.1518e+00, -4.3420e+00, -1.2206e+00, -4.8121e+00, -3.5091e+00,\n         1.4510e+00, -4.8544e+00,  4.0994e+00, -2.3534e+00, -2.9900e+00,\n        -9.6720e+00, -2.9801e+00, -1.8269e+00, -1.0490e+01, -2.8494e+00,\n         3.2904e+00, -3.4714e+00,  6.3171e+00, -6.0031e-01,  6.4215e+00,\n         7.7318e+00, -1.1628e+01, -2.2580e+00,  6.5713e+00, -1.3225e+01,\n         1.8486e+00,  5.2005e-01, -1.1946e+00,  3.4093e-01,  6.6873e+00,\n         3.8414e+00, -6.7605e+00, -3.6807e+00,  6.5493e+00, -7.2647e+00,\n         5.5535e+00, -3.5462e+00,  4.3100e+00, -1.9080e+00,  4.4815e+00,\n         1.4344e+00, -9.2792e+00,  5.1683e-01,  2.5537e+00, -3.7238e+00,\n        -3.5990e+00, -9.6038e-01,  2.6059e+00,  3.2556e+00, -3.9208e+00,\n        -1.1759e+00, -2.2442e+00, -5.0637e+00,  5.1637e+00,  8.1804e+00,\n         6.5776e+00,  6.8021e+00, -7.5181e+00,  1.6102e+00, -9.6251e+00,\n         1.0529e+01,  6.9914e+00,  2.0125e+00,  5.1722e+00, -3.1221e+00,\n         1.7856e+00, -6.8784e+00, -1.0140e+00,  4.2255e+00,  4.5046e+00,\n        -6.5289e+00, -3.6468e+00, -5.1887e-01,  6.8367e+00,  1.8442e+00,\n         1.9149e+00, -9.1361e+00, -2.6483e+00, -4.3092e+00, -3.8572e+00,\n         4.2038e+00, -4.3649e+00,  6.4979e-01,  3.2503e+00,  7.7257e-01,\n        -9.5746e+00, -5.9892e+00,  1.3186e+00,  1.0000e+00],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:47:38.230894Z","iopub.execute_input":"2024-08-07T05:47:38.231475Z","iopub.status.idle":"2024-08-07T05:47:38.533303Z","shell.execute_reply.started":"2024-08-07T05:47:38.231444Z","shell.execute_reply":"2024-08-07T05:47:38.532165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Training with custom feature loader\n\n# Hyperparameters\nfeature_dim = 2048\nnum_classes = 8\nbatch_size = 128\nnum_epochs = 10\nlearning_rate = 1e-4\nsequence_length = 64\n\n# Model, loss function, optimizer\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to('cuda')\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n# try SGD Optimizer \n \nloss_list = []\n\nimage_features = image_features.cuda()\nlabel_tensor = label_tensor.cuda()\n\n# Training loop\nfor epoch in range(1, num_epochs):\n    model.train()\n    running_loss = 0.0\n    seq_start = 0\n    seq_end = sequence_length\n    progress_bar = tqdm(total = len(labels))\n    while seq_end < label_tensor.shape[0]:\n        optimizer.zero_grad()\n        out = []\n        targets = []\n        for i in range(batch_size):\n            if seq_end >= label_tensor.shape[0]:\n                break\n            inputs = image_features[seq_start:seq_start + sequence_length, : ]\n            target = label_tensor[seq_end]\n#             print('hello',inputs.shape)\n#             print(targets.shape)\n            # Forward pass\n            outputs = model(inputs)\n\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            out.append(outputs)\n            target = target.view(-1)\n            targets.append(target)\n            seq_start += 1\n            seq_end += 1\n            if seq_end % 100 == 0:\n                progress_bar.update(100)\n                    # Calculate loss\n        pred = torch.row_stack(out)\n        truth = torch.tensor(targets).cuda()\n        loss = criterion(pred, truth)\n        # Backward pass and optimize\n#             print(loss)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        loss_list.append(loss.item())\n        # Update progress bar with current loss\n#         data_loader.set_postfix(loss=loss.item())\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n    torch.save(model.state_dict(), f'transformer_on_resnet_50_e{epoch}.pth')\n    progress_bar.close()\n\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:26:21.751251Z","iopub.execute_input":"2024-07-31T15:26:21.752237Z","iopub.status.idle":"2024-07-31T15:26:22.676823Z","shell.execute_reply.started":"2024-07-31T15:26:21.752196Z","shell.execute_reply":"2024-07-31T15:26:22.675248Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae97e605a7b4dfcb46d2886d109f878"}},"metadata":{}},{"name":"stdout","text":"After transformer encoding:  torch.Size([64, 2048])\nTake mean:  torch.Size([64])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m             target \u001b[38;5;241m=\u001b[39m label_tensor[seq_end]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             print('hello',inputs.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#             print(targets.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;66;03m# Reshape outputs and targets to be compatible with the loss function\u001b[39;00m\n\u001b[1;32m     44\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mImageTransformer.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTake mean: \u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)","output_type":"error"}]},{"cell_type":"code","source":"# Validation\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth'))\nmodel.eval()\nseq_start = 0\nseq_end = sequence_length\nprogress_bar = tqdm(total = len(labels))\npreds = []\nwith torch.no_grad():\n    while seq_end < label_tensor.shape[0]:\n\n        inputs = image_features[seq_start:seq_start + sequence_length, : ].cuda()\n        target = label_tensor[seq_end]\n    #             print('hello',inputs.shape)\n    #             print(targets.shape)\n        # Forward pass\n        outputs = model(inputs)\n        preds.append(torch.argmax(outputs).item())\n        seq_start += 1\n        seq_end += 1\n        progress_bar.update(1)\n\nprint(len(preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-01T05:14:38.69926Z","iopub.execute_input":"2024-08-01T05:14:38.699659Z","iopub.status.idle":"2024-08-01T05:15:28.416199Z","shell.execute_reply.started":"2024-08-01T05:14:38.699629Z","shell.execute_reply":"2024-08-01T05:15:28.415411Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Validation\n# feature_dim = 2048\n# num_classes = 8\n# batch_size = 128\n# num_epochs = 10\n# learning_rate = 1e-4\n# sequence_length = 64\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth', map_location=torch.device('cpu')))\ncriterion = nn.CrossEntropyLoss()\n\nmodel.eval()\n\nall_preds = []\nall_targets = []\nrunning_loss = 0.0\nwith torch.no_grad():\n    progress_bar = tqdm(total=len(data_loader_val), desc='Validation', unit='batch')\n    for features, targets in data_loader_val:\n#         features, targets = features.cuda(), targets.cuda() \n        outputs = model(features)\n        # Reshape outputs and targets to be compatible with the loss function\n        outputs = outputs.view(-1, num_classes)\n        # Calculate loss\n        validation_loss = criterion(outputs, targets)\n        running_loss += validation_loss.item() * features.size(0)\n        # Collect predictions and true labels\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(loss=validation_loss.item())\n    \n    progress_bar.close()\n\nvalidation_loss = running_loss / val_length\n\n# Calculate metrics\nall_preds = np.array(all_preds)\nall_targets = np.array(all_targets)\n\nf1 = f1_score(all_targets, all_preds, average='weighted')\nprecision = precision_score(all_targets, all_preds, average='weighted')\nrecall = recall_score(all_targets, all_preds, average='weighted')\naccuracy = accuracy_score(all_targets, all_preds)\n\nprint(f'Validation Loss: {validation_loss:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:05:01.05667Z","iopub.execute_input":"2024-08-09T15:05:01.057648Z","iopub.status.idle":"2024-08-09T15:10:22.121773Z","shell.execute_reply.started":"2024-08-09T15:05:01.057602Z","shell.execute_reply":"2024-08-09T15:10:22.120535Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/120 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3df2c7cad4f3424c989f59f982efc119"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.9009\nF1 Score: 0.1363\nPrecision: 0.0884\nRecall: 0.2974\nAccuracy: 0.2974\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from scipy.ndimage import gaussian_filter1d\nimport numpy as np\n\npreds = torch.tensor(preds)\npreds = preds.to('cpu')\n\nsigma = 1.0  # Standard deviation of the Gaussian kernel\nfiltered_array = gaussian_filter1d(preds, sigma=sigma)\n\n# Discretize the filtered values\ndiscretized_array = np.round(filtered_array).astype(int)  # Round and convert to integers\n\n# Convert the result back to a PyTorch tensor\nfiltered_tensor = torch.tensor(discretized_array, dtype=torch.int)\n\n\nfiltered_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:37.309357Z","iopub.execute_input":"2024-08-05T10:29:37.31004Z","iopub.status.idle":"2024-08-05T10:29:37.320912Z","shell.execute_reply.started":"2024-08-05T10:29:37.31001Z","shell.execute_reply":"2024-08-05T10:29:37.320062Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/172670838.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  preds = torch.tensor(preds)\n","output_type":"stream"},{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"torch.Size([64])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Example ground truth and predictions\ny_true = label_tensor[sequence_length:]  # True labels\ny_pred = preds  # Predicted labels\n\n# Calculate precision, recall, and F1 score for different averaging methods\nprecision_macro = precision_score(y_true, y_pred, average='macro')\nrecall_macro = recall_score(y_true, y_pred, average='macro')\nf1_macro = f1_score(y_true, y_pred, average='macro')\n\nprecision_micro = precision_score(y_true, y_pred, average='micro')\nrecall_micro = recall_score(y_true, y_pred, average='micro')\nf1_micro = f1_score(y_true, y_pred, average='micro')\n\nprecision_weighted = precision_score(y_true, y_pred, average='weighted')\nrecall_weighted = recall_score(y_true, y_pred, average='weighted')\nf1_weighted = f1_score(y_true, y_pred, average='weighted')\n\nprint(f'Precision (macro): {precision_macro:.2f}')\nprint(f'Recall (macro): {recall_macro:.2f}')\nprint(f'F1 Score (macro): {f1_macro:.2f}')\n\nprint(f'Precision (micro): {precision_micro:.2f}')\nprint(f'Recall (micro): {recall_micro:.2f}')\nprint(f'F1 Score (micro): {f1_micro:.2f}')\n\nprint(f'Precision (weighted): {precision_weighted:.2f}')\nprint(f'Recall (weighted): {recall_weighted:.2f}')\nprint(f'F1 Score (weighted): {f1_weighted:.2f}')\n\naccuracy = accuracy_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:44.908057Z","iopub.execute_input":"2024-08-05T10:29:44.908952Z","iopub.status.idle":"2024-08-05T10:29:45.274898Z","shell.execute_reply.started":"2024-08-05T10:29:44.9089Z","shell.execute_reply":"2024-08-05T10:29:45.273544Z"},"trusted":true},"execution_count":117,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m preds  \u001b[38;5;66;03m# Predicted labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, and F1 score for different averaging methods\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m precision_macro \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m recall_macro \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     y_true,\n\u001b[1;32m   1827\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15408, 64]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [15408, 64]","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor(float(outputs.argmax().item()))","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.131308Z","iopub.status.idle":"2024-07-31T05:24:11.131763Z","shell.execute_reply.started":"2024-07-31T05:24:11.131523Z","shell.execute_reply":"2024-07-31T05:24:11.131541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/kaggle/input/abaw-7-dataset/cropped_aligned/1-30-1280x720/00001.jpg').convert('RGB')\ntransform = transforms.Compose([\n            transforms.Resize([224, 224])\n        ])\n\ntransform(img)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.133633Z","iopub.status.idle":"2024-07-31T05:24:11.13411Z","shell.execute_reply.started":"2024-07-31T05:24:11.1339Z","shell.execute_reply":"2024-07-31T05:24:11.133924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}