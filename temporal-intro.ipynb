{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8606881,"sourceType":"datasetVersion","datasetId":5122606},{"sourceId":9144423,"sourceType":"datasetVersion","datasetId":5473430}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riturajpradhan/temporal-intro?scriptVersionId=192618237\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.init as init\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom timm.scheduler.scheduler import Scheduler\n\nfrom torchvision import transforms\n# from transformers import AutoImageProcessor, ResNetModel\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n# from transformers.image_processing_base import BatchFeature","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T08:48:13.336543Z","iopub.execute_input":"2024-08-13T08:48:13.336915Z","iopub.status.idle":"2024-08-13T08:48:36.225993Z","shell.execute_reply.started":"2024-08-13T08:48:13.336886Z","shell.execute_reply":"2024-08-13T08:48:36.224681Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-13 08:48:19.943763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 08:48:19.943924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 08:48:20.095896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"image_paths = []\nlabels = []\nlabel_path = '/kaggle/input/abaw-7-dataset/training_set_annotations.txt'\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nwith open(label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_name = line[0]\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    image_path = os.path.join(data_path, image_name)\n    image_paths.append(image_path)\n    labels.append(image_label)\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-13T08:50:34.582844Z","iopub.execute_input":"2024-08-13T08:50:34.583323Z","iopub.status.idle":"2024-08-13T08:50:35.158107Z","shell.execute_reply.started":"2024-08-13T08:50:34.583291Z","shell.execute_reply":"2024-08-13T08:50:35.156732Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"i = 0\ntop = len(data)\ndata_dict = {}\nemo = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness','Surprise', 'Other']\npath = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nwhile i < top:\n    line = data[i].split(',')\n    current_folder = line[0].split('/')[0]\n    data_dict[current_folder] = {}\n    while current_folder == data[i].split(',')[0].split('/')[0]:\n        print(f'in data')\n        if line[3] != '-1':\n            current_exp = line[3]\n            data_dict[current_folder][emo[int(current_exp)]] = []\n            while current_exp == data[i].split(',')[3]:\n                data_dict[current_folder][emo[int(current_exp)]].append((os.path.join(path,data[i].split(',')[0]), data[i].split(',')[3]))\n                i+= 1\n        else:\n            i+= 1\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2024-08-13T09:15:55.804937Z","iopub.execute_input":"2024-08-13T09:15:55.805372Z","iopub.status.idle":"2024-08-13T09:16:12.180751Z","shell.execute_reply.started":"2024-08-13T09:15:55.805341Z","shell.execute_reply":"2024-08-13T09:16:12.179361Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m current_folder \u001b[38;5;241m=\u001b[39m line[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m data_dict[current_folder] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_folder \u001b[38;5;241m==\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m         current_exp \u001b[38;5;241m=\u001b[39m line[\u001b[38;5;241m3\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for file in os.listdir('/kaggle/working/SGD_Kaiming'):\n    if file != 'transformer_on_ViT_e80.pth':\n        os.remove(f'/kaggle/working/SGD_Kaiming/{file}')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:16:26.372401Z","iopub.execute_input":"2024-08-09T15:16:26.372834Z","iopub.status.idle":"2024-08-09T15:16:26.499443Z","shell.execute_reply.started":"2024-08-09T15:16:26.372803Z","shell.execute_reply":"2024-08-09T15:16:26.498206Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# run cell to extract image features and store them\nimage_preprocessor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nfeature_extractor = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True).to(device)\n    \n# Batch size for processing images\nbatch_size = 100\n\nbatch_count = 0\n\n# Initialize lists to store extracted features\nall_features = []\n\n# Process images in batches\nfor batch_start in tqdm(range(0, len(image_paths), batch_size), desc='Extracting Features'):\n    batch_end = min(batch_start + batch_size, len(image_paths))\n    batch_images = [Image.open(image_path) for image_path in image_paths[batch_start:batch_end]]\n    batch_labels = torch.tensor([float(label) for label in labels[batch_start:batch_end]]).to(device)\n    # Tokenize and extract features\n    inputs = image_preprocessor(images = batch_images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = feature_extractor(**inputs)\n    batch_features = output.hidden_states[-1][:,0,:].clone().detach()\n    t = torch.column_stack([batch_features, batch_labels])\n\n    # Append features to the list\n    all_features.extend(t)\n\n# Save features to a file (e.g., as a PyTorch tensor)\n    # Save features every 1000 images\n    if batch_count%50 == 0:\n        output_file = f\"./validation_features/image_features_val_{batch_count}.pt\"\n        torch.save(torch.stack(all_features).cpu(), output_file)\n        print(f\"Features saved to {output_file}\")\n        \n        # Clear memory by resetting the list\n        all_features = []\n    batch_count += 1\n    with open('counter.json', 'w') as f:\n        json.dump({'batch_count' : batch_count}, f)\n\n# Save any remaining features\nif all_features:\n    output_file = f\"./validation_features/image_features_val_{batch_count}.pt\"\n    torch.save(torch.stack(all_features).cpu(), output_file)\n    print(f\"Remaining features saved to {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:14:43.228169Z","iopub.execute_input":"2024-08-07T06:14:43.228569Z","iopub.status.idle":"2024-08-07T06:17:27.290458Z","shell.execute_reply.started":"2024-08-07T06:14:43.228541Z","shell.execute_reply":"2024-08-07T06:17:27.289385Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting Features:   0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5971aedbbd0141cb8b45a2575d139781"}},"metadata":{}},{"name":"stdout","text":"Features saved to ./validation_features/image_features_val_0.pt\nFeatures saved to ./validation_features/image_features_val_50.pt\nFeatures saved to ./validation_features/image_features_val_100.pt\nFeatures saved to ./validation_features/image_features_val_150.pt\nRemaining features saved to ./validation_features/image_features_val_155.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"path = '/kaggle/working/validation_features'\nfeat_arr = []\nfor file in os.listdir(path):\n    feat_arr.append(torch.load(os.path.join(path, file)))\n\ntraining_features = torch.row_stack(feat_arr)\ntraining_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:22:16.906892Z","iopub.execute_input":"2024-08-07T06:22:16.907268Z","iopub.status.idle":"2024-08-07T06:22:16.967469Z","shell.execute_reply.started":"2024-08-07T06:22:16.90724Z","shell.execute_reply":"2024-08-07T06:22:16.966451Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([15440, 769])"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(training_features, 'ABAW_validation_features_ViT.pt')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:22:28.725937Z","iopub.execute_input":"2024-08-07T06:22:28.726803Z","iopub.status.idle":"2024-08-07T06:22:28.784512Z","shell.execute_reply.started":"2024-08-07T06:22:28.726768Z","shell.execute_reply":"2024-08-07T06:22:28.783235Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class ImageTransformer(nn.Module):\n    def __init__(self, feature_dim, num_classes, num_heads=4, num_layers=6, dropout=0.1, sequence_length=64):\n        super(ImageTransformer, self).__init__()\n        self.feature_dim = feature_dim\n        self.num_classes = num_classes\n        \n        # Positional encoding\n        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, feature_dim))\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Classification head\n        self.fc1 = nn.Linear(feature_dim, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, num_classes)\n        \n         # Initialize weights\n        self._initialize_weights()\n\n\n    def _initialize_weights(self):\n        # Apply Kaiming initialization to all linear layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n\n    def forward(self, x):\n        \n        # Add positional encoding\n        x = x + self.positional_encoding[:, :x.size(1), :]\n        \n        # Pass through the transformer encoder\n        x = self.transformer_encoder(x)\n        \n        x = x.mean(dim=1)\n        \n        # Pass through the classification head\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        \n        return x\n\nclass ABAWFeatureDataset(Dataset):\n    def __init__(self, features, labels, sequence_length):\n        self.features = features\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.seq_start = 0\n        self.seq_end = sequence_length\n        self.length = labels.shape[0]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if idx > (self.length - self.sequence_length):\n            feature = self.features[self.length - self.sequence_length:, : ]\n            label = self.labels[self.length - 1]\n        else:\n            feature = self.features[idx:idx + self.sequence_length, :]\n            label = self.labels[idx + self.sequence_length - 1]\n        return feature, label","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:57:34.453446Z","iopub.execute_input":"2024-08-10T05:57:34.453933Z","iopub.status.idle":"2024-08-10T05:57:34.928416Z","shell.execute_reply.started":"2024-08-10T05:57:34.453889Z","shell.execute_reply":"2024-08-10T05:57:34.926902Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nsequence_length = 10\nfeature_dim = 768\nnum_classes = 8\nnum_epochs = 100\nlearning_rate = 0.01\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nval_label_path = '/kaggle/input/abaw7-extracted-features/validation_set_annotations.txt'\nval_labels = []\nwith open(val_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    val_labels.append(image_label)\n    \ntrain_label_path = '/kaggle/input/abaw7-extracted-features/training_set_annotations.txt'\ntrain_labels = []\nwith open(train_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    train_labels.append(image_label)\n    \n# Create dataset and dataloader\ntrain_image_features = torch.load('/kaggle/input/abaw7-extracted-features/ABAW_training_features_ViT.pt')\ntemp = train_image_features.detach().numpy()\ntrain_image_features = torch.tensor(temp[:,:768])\n\ntrain_labels = [int(x) for x in train_labels]\ntrain_label_tensor = torch.tensor(train_labels)\ntrain_dataset = ABAWFeatureDataset(train_image_features, train_label_tensor, sequence_length = sequence_length)\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, drop_last = True, pin_memory=True)\n\ntrain_length = len(train_dataset)\n    \nval_image_features = torch.load('/kaggle/input/abaw7-extracted-features/ABAW_validation_features_ViT.pt')\ntemp = val_image_features.detach().numpy()\nval_image_features = torch.tensor(temp[:,:768])\nval_labels = [int(x) for x in val_labels]\nval_label_tensor = torch.tensor(val_labels)\nval_dataset = ABAWFeatureDataset(val_image_features, val_label_tensor, sequence_length = sequence_length)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, drop_last = True, pin_memory=True)#, collate_fn=custom_collate_fn)\n\nval_length = len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:58:06.151999Z","iopub.execute_input":"2024-08-10T05:58:06.152487Z","iopub.status.idle":"2024-08-10T05:58:09.790213Z","shell.execute_reply.started":"2024-08-10T05:58:06.152452Z","shell.execute_reply":"2024-08-10T05:58:09.788842Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class LinearLRScheduler(Scheduler):\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 lr_min_rate: float,\n                 warmup_t=0,\n                 warmup_lr_init=0.,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True,\n                 ) -> None:\n        super().__init__(\n            optimizer, param_group_field=\"lr\",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        self.t_initial = t_initial\n        self.lr_min_rate = lr_min_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            t = t - self.warmup_t\n            total_t = self.t_initial - self.warmup_t\n            lrs = [v - ((v - v * self.lr_min_rate) * (t / total_t)) for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:58:16.437205Z","iopub.execute_input":"2024-08-10T05:58:16.437658Z","iopub.status.idle":"2024-08-10T05:58:16.452955Z","shell.execute_reply.started":"2024-08-10T05:58:16.437625Z","shell.execute_reply":"2024-08-10T05:58:16.451183Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Increase the message rate limit for printing training progress\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager().update('notebook', {\n    \"NotebookApp\": {\n        \"iopub_msg_rate_limit\": 100000,  # Increase to 10000 messages/sec\n        \"rate_limit_window\": 10.0,      # Increase the rate limit window to 10 seconds\n    }\n})\n\nos.mkdir('SGD_Kaiming_monitor')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:36:16.316961Z","iopub.execute_input":"2024-08-09T15:36:16.317325Z","iopub.status.idle":"2024-08-09T15:36:16.34012Z","shell.execute_reply.started":"2024-08-09T15:36:16.317285Z","shell.execute_reply":"2024-08-09T15:36:16.33925Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:32:23.882497Z","iopub.execute_input":"2024-08-09T15:32:23.883109Z","iopub.status.idle":"2024-08-09T15:32:23.891159Z","shell.execute_reply.started":"2024-08-09T15:32:23.883031Z","shell.execute_reply":"2024-08-09T15:32:23.889922Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"90645"},"metadata":{}}]},{"cell_type":"code","source":"# Model, loss function, optimizer\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\n# model.load_state_dict(torch.load('/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth'))\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0.1, weight_decay = 1e-4)\nscaler = GradScaler()\nwriter = SummaryWriter('SGD_Kaiming_monitor')\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n# scheduler = LinearLRScheduler(\n#             optimizer,\n#             t_initial=100,\n#             lr_min_rate=0.01,\n#             warmup_lr_init=5e-5,\n#             warmup_t=3,\n#             t_in_epochs=False,\n#         )\n# training_loss_list = []\n# validation_loss_list = []\n# loss_list = []\n# Training loop\nfor epoch in range(1,num_epochs + 1):\n    model.train()\n    running_loss = 0.0\n    progress_bar = tqdm(total=len(data_loader_train), desc=f'Epoch {epoch+1}/{num_epochs} LR = {optimizer.param_groups[0][\"lr\"]}', unit='batch')\n    \n    for i, (features, targets) in enumerate(data_loader_train):\n        features, targets = features.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(features)\n            # Calculate loss\n            loss = criterion(outputs, targets)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * features.size(0)\n#         loss_list.append(loss.item())\n#         writer.add_scalar('training_loss', loss.item(), epoch * train_length + i)\n        # Update progress bar with current loss\n        progress_bar.set_postfix(loss=loss.item())\n        progress_bar.update(1)\n        \n    epoch_loss = running_loss / train_length\n    writer.add_scalar('training_epoch_loss', epoch_loss, epoch)\n    if epoch % 10 == 0:\n        torch.save(model.state_dict(), f'/kaggle/working/SGD_Kaiming/transformer_mean_on_ViT_e{epoch}.pth')\n    model.eval()\n    running_loss = 0.0\n    all_preds = []\n    all_targets = [] \n    with torch.no_grad():\n#         val_progress_bar = tqdm(total=len(data_loader_val), desc=f'Validation', unit='batch')\n        for features, targets in data_loader_val:\n            features, targets = features.cuda(), targets.cuda() \n            outputs = model(features)\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            # Calculate loss\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            validation_loss = criterion(outputs, targets)\n            running_loss += validation_loss.item() * features.size(0)\n#             val_progress_bar.set_postfix(validation_loss=validation_loss.item())\n#             val_progress_bar.update(1)\n    \n    validation_loss = running_loss / val_length\n    scheduler.step(validation_loss)\n    \n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    f1 = f1_score(all_targets, all_preds, average='weighted')\n    precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='weighted')\n    accuracy = accuracy_score(all_targets, all_preds)\n\n    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n    writer.add_scalar('validation_epoch_loss', validation_loss, epoch)\n    writer.add_scalar('f1_score', f1, epoch)\n    writer.add_scalar('precision', precision, epoch)\n    writer.add_scalar('recall', recall, epoch)\n    writer.add_scalar('accuracy', accuracy, epoch)\n\n#     training_loss_list.append(epoch_loss)\n#     validation_loss_list.append(validation_loss)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, validation Loss: {validation_loss:.4f}')\n    progress_bar.close()\n\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:37:24.0094Z","iopub.execute_input":"2024-08-09T15:37:24.00979Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 2/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09b861d7e67d447da65b292b76cf02ba"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/100, Training Loss: 1.9089, validation Loss: 1.9912\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6550d0d16ab434c86409442138213e0"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/100, Training Loss: 1.8313, validation Loss: 1.9902\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638c4033eca94f7ca4345b0e8004651b"}},"metadata":{}},{"name":"stdout","text":"Epoch 4/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef9037efbc3447288adaaeac456b7b7"}},"metadata":{}},{"name":"stdout","text":"Epoch 5/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3687e662ded3451ea641b0312433248a"}},"metadata":{}},{"name":"stdout","text":"Epoch 6/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f4a624e130745f6a9495f8c5a0decb3"}},"metadata":{}},{"name":"stdout","text":"Epoch 7/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6948faf2ce4ed6b16900ac6b341f44"}},"metadata":{}},{"name":"stdout","text":"Epoch 8/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e46c711f3aa24ca4888fc981e0b15b67"}},"metadata":{}},{"name":"stdout","text":"Epoch 9/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ba3db1a2afb42729ba90e2fa4b5c10b"}},"metadata":{}},{"name":"stdout","text":"Epoch 10/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04127bd2d7984d3aab5f78a29e35d258"}},"metadata":{}},{"name":"stdout","text":"Epoch 11/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d879208527904e3682c3d3aa681a2f63"}},"metadata":{}},{"name":"stdout","text":"Epoch 12/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38e6439014348b4a5b7d1f06acdd158"}},"metadata":{}},{"name":"stdout","text":"Epoch 13/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/100 LR = 0.01:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620b899eace147f3b7d259ecffe4f458"}},"metadata":{}},{"name":"stdout","text":"Epoch 14/100, Training Loss: 1.8310, validation Loss: 1.9901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/100 LR = 0.001:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77161437bdaf41008ce68ee65d9e6d2a"}},"metadata":{}},{"name":"stdout","text":"Epoch 15/100, Training Loss: 1.8236, validation Loss: 1.9720\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/100 LR = 0.001:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384dedf4990049cdb3c7aebee547ef20"}},"metadata":{}},{"name":"stdout","text":"Epoch 16/100, Training Loss: 1.8061, validation Loss: 1.9577\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/100 LR = 0.001:   0%|          | 0/708 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35089178b1014ad18d03147e99b97d59"}},"metadata":{}}]},{"cell_type":"code","source":"torch.argmax(outputs, dim = 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T06:43:47.321023Z","iopub.execute_input":"2024-08-08T06:43:47.321407Z","iopub.status.idle":"2024-08-08T06:43:47.329839Z","shell.execute_reply.started":"2024-08-08T06:43:47.32138Z","shell.execute_reply":"2024-08-08T06:43:47.329022Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testing new feature extractor","metadata":{}},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state,","metadata":{"execution":{"iopub.status.busy":"2024-08-07T04:46:47.151621Z","iopub.execute_input":"2024-08-07T04:46:47.151977Z","iopub.status.idle":"2024-08-07T04:46:50.533433Z","shell.execute_reply.started":"2024-08-07T04:46:47.151948Z","shell.execute_reply":"2024-08-07T04:46:50.532129Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040b2960c7154fdf8086db4fb9589a34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b6398e5f14449d87f66db83a2ac8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc93215a91c542ec9c4258d1bb48275e"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimages = [image for i in range(1,32)]\nlabels = [i for i in range(1,32)]\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True)\n\ninputs = feature_extractor(images=images, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\n# predicted_class_idx = logits.argmax(-1).item()\n# print(\"Predicted class:\", model.config.id2label[predicted_class_idx]),","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:15:41.071334Z","iopub.execute_input":"2024-08-07T05:15:41.072339Z","iopub.status.idle":"2024-08-07T05:15:48.509017Z","shell.execute_reply.started":"2024-08-07T05:15:41.072296Z","shell.execute_reply":"2024-08-07T05:15:48.508199Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# outputs.hidden_states[-1][:,0,:]\nimport torch\nt = torch.column_stack([outputs.hidden_states[-1][:,0,:], torch.tensor(labels)])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:16:50.498825Z","iopub.execute_input":"2024-08-07T05:16:50.499185Z","iopub.status.idle":"2024-08-07T05:16:50.505383Z","shell.execute_reply.started":"2024-08-07T05:16:50.499156Z","shell.execute_reply":"2024-08-07T05:16:50.504416Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"t[0,:]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:17:22.258295Z","iopub.execute_input":"2024-08-07T05:17:22.258632Z","iopub.status.idle":"2024-08-07T05:17:22.273578Z","shell.execute_reply.started":"2024-08-07T05:17:22.258608Z","shell.execute_reply":"2024-08-07T05:17:22.272686Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"tensor([ 2.3126e+00,  5.5116e+00,  1.1788e+01,  5.7725e-01,  6.5475e+00,\n        -2.9125e+00,  4.5668e+00, -1.3786e+00,  6.1539e+00, -5.1831e+00,\n         4.9711e+00,  8.4007e-01,  7.7080e+00, -3.0897e+00, -3.2443e+00,\n         9.7258e+00,  1.1146e+00, -2.1286e+00,  8.7621e+00,  1.6315e+00,\n        -8.4574e+00,  1.8422e+00,  1.4254e+00,  6.5619e+00, -1.0730e+01,\n         6.0743e+00,  4.2650e+00,  6.0531e+00,  8.9479e+00,  2.5177e+00,\n         5.4446e-01,  1.4944e+00,  4.0779e+00,  1.0520e+01, -2.9379e+00,\n         6.8438e+00, -4.6463e+00, -3.0405e+00,  1.0135e+00,  8.2927e+00,\n         9.9011e+00, -3.0263e+00,  7.5373e-01, -4.1869e+00,  2.0160e+00,\n         7.0067e+00,  7.9857e-02, -2.2949e+00,  2.2206e+00, -4.4853e+00,\n         1.8516e+00, -7.8394e-01,  4.4800e+00, -3.8795e+00, -1.1654e+01,\n        -2.1962e+00,  2.5946e-01,  1.0626e+01, -5.4844e-01, -2.7896e+00,\n         1.3051e+01,  8.1810e+00,  4.4351e+00, -6.2068e+00,  1.0071e+00,\n        -4.3451e+00,  2.8967e+00, -1.7342e+00,  5.2872e+00, -4.1522e+00,\n        -9.2688e+00,  9.9553e+00, -3.6386e+00, -2.5019e+00, -8.4062e+00,\n         4.9192e-01, -9.5148e+00, -6.4460e+00, -3.4781e+00, -1.1284e+00,\n         8.1581e+00, -1.0863e+01, -1.0140e+01,  8.8340e+00,  7.9968e+00,\n         5.1645e+00, -3.4996e+00,  1.1152e+01,  4.4518e+00, -1.2614e+00,\n         1.7204e+01, -4.9011e-01,  5.3941e+00,  7.6976e+00,  1.5465e-01,\n        -6.2431e+00, -3.6648e+00, -4.9716e-01,  1.6759e+00, -1.0212e+01,\n        -4.2299e+00,  4.3840e+00, -7.6967e-01,  6.4818e-01, -1.2815e+00,\n         4.0404e+00, -2.7555e+00,  8.0193e+00, -4.0383e+00, -2.7090e+00,\n        -3.0092e+00, -2.2504e+00,  1.4036e+00, -1.9877e+00, -5.8852e-01,\n         4.5092e+00, -9.2577e+00,  8.3078e+00, -2.5480e+00, -2.3309e+00,\n         8.1919e+00,  1.7041e+00, -6.2756e+00, -5.1414e-01,  2.3397e+00,\n         1.4869e+00, -3.0239e+00, -1.0958e+01,  2.1754e+00, -1.1177e-01,\n         5.2057e+00, -4.3006e+00,  5.6387e+00, -1.9813e+00,  5.1607e+00,\n         7.4258e-01,  5.1081e+00, -4.8063e+00, -3.0374e+00,  5.3849e+00,\n        -2.8684e+00, -3.7519e+00,  5.6947e+00,  4.2382e+00, -1.0009e+00,\n        -2.1228e-01,  3.3212e+00, -8.0935e+00,  5.2201e+00, -6.6542e+00,\n        -8.2925e+00, -7.8512e+00,  4.4826e+00,  2.8323e+00, -2.7104e+00,\n         1.2681e+01,  4.5946e+00,  1.5467e+00, -8.7506e+00, -1.1305e+01,\n         3.3003e+00, -7.4826e+00,  5.0542e+00, -1.7443e+00, -1.0060e+00,\n         5.6709e+00, -3.6740e+00,  1.5832e+01,  1.2297e+01,  3.3019e+00,\n         1.9885e+00, -9.0401e-01,  5.0012e-01,  1.2189e+01, -1.2169e+00,\n         2.1518e+00,  3.6889e+00,  1.9564e+00,  7.4761e+00, -8.1639e-01,\n         3.7544e+00, -4.9893e+00, -8.8977e+00, -1.5014e-01, -1.5335e-01,\n        -7.6970e+00, -5.8783e+00,  2.3438e+00, -1.0910e+01, -7.3558e+00,\n        -1.3294e-01,  1.1221e+00,  1.8191e+00,  4.9105e+00, -9.0446e+00,\n         1.1171e+01,  2.4723e+00, -6.4088e+00, -4.9586e+00,  1.7104e+01,\n        -7.0072e-01, -9.1785e+00,  9.9637e-01, -8.4077e+00,  4.5781e-01,\n         1.4394e+00,  4.4015e+00,  7.7832e+00,  1.5674e+01,  9.7669e+00,\n         9.5864e+00,  3.4548e-01, -9.5140e+00, -3.6262e+00, -3.3511e-02,\n         9.5605e-01,  2.7343e+00, -7.1423e+00, -4.8976e+00,  8.2165e+00,\n        -1.0892e+01,  5.3325e+00, -6.9168e-01, -6.0497e+00, -3.9966e+00,\n         2.9761e+00, -4.5020e+00, -4.4989e+00, -2.4625e+00, -2.3799e+00,\n         3.4785e+00, -6.4768e-02, -1.4270e+01,  7.9958e-01, -8.5005e+00,\n        -6.1762e+00,  1.3880e+00,  4.5403e-01,  1.8790e+00, -5.1056e+00,\n        -9.7338e+00, -8.4601e+00,  2.7580e+00,  5.5546e+00, -3.5314e+00,\n         6.0557e+00, -2.3313e+00,  9.2037e+00,  4.2733e+00, -1.5292e+01,\n        -3.3109e-01, -2.5005e-01,  1.5867e+00, -4.6539e+00, -1.3627e+00,\n        -2.7924e-01, -1.0277e+00,  2.2768e+00,  7.8121e+00, -5.0762e+00,\n        -4.3243e+00,  6.6483e-01,  2.3120e+00, -1.0160e+00,  3.1079e+00,\n         4.0078e+00,  7.6681e+00,  2.7114e+00,  2.2636e+00,  6.3449e+00,\n         1.2057e+00,  9.2436e+00, -3.8165e+00, -4.0865e+00, -5.5988e+00,\n        -9.9585e+00,  5.4400e+00,  4.3910e-01,  4.3431e+00, -6.1047e+00,\n        -3.3372e+00, -5.3751e+00,  5.7061e+00,  5.4730e+00,  8.4846e+00,\n        -5.4949e+00, -9.1157e-01,  6.9689e-01,  7.5254e-02,  7.1937e+00,\n        -3.3917e+00, -5.1809e+00, -8.3977e-01,  6.8584e+00, -8.4331e+00,\n         5.7394e+00, -3.7869e+00,  4.9977e+00,  3.0823e+00,  7.9278e+00,\n        -4.2151e+00, -1.0965e+00,  2.5793e+00, -2.4594e+00,  5.3281e+00,\n        -1.2227e+01, -6.5901e-01,  1.7196e+01,  8.4178e+00, -5.5776e+00,\n         8.9550e+00, -9.1851e-01,  1.3255e+00,  1.1535e+01,  4.6121e+00,\n        -1.1908e+00,  4.8861e+00,  3.8148e+00,  3.0933e+00, -3.4191e+00,\n         8.8850e+00,  6.3427e+00, -8.7499e-01, -5.0558e+00, -3.7724e+00,\n        -2.2231e+00,  6.4751e+00, -6.2604e+00, -8.7503e+00,  3.7453e+00,\n        -4.1096e+00,  5.5612e+00, -7.2538e+00, -8.7126e+00,  2.3858e+00,\n        -7.6081e+00, -4.0343e+00,  1.6607e+01,  3.2290e+00, -1.1992e+01,\n         1.6804e+00,  1.0575e+01,  1.2953e+01,  5.3291e+00,  4.4644e+00,\n        -1.1186e+01,  7.1904e-02,  6.4786e+00,  1.3749e+01, -1.2863e+00,\n         2.1389e-01,  5.9342e+00,  4.6504e-01,  1.7982e-01, -8.1117e+00,\n        -7.9560e+00, -6.3646e+00, -4.2088e+00,  6.7474e+00, -8.9446e-01,\n        -6.3456e-01,  2.3697e+00,  1.0983e+01, -1.0096e+01, -1.4992e+00,\n        -1.3370e-01, -1.0659e+01,  1.0049e+01, -4.2879e+00, -2.5807e+00,\n         6.9250e+00, -5.1344e+00, -7.8552e+00,  2.5728e+00, -4.0974e+00,\n        -1.0583e+01,  4.3317e+00,  1.5247e+00, -7.4624e-01,  8.0111e-03,\n        -8.0616e+00,  3.6085e+00,  4.4804e+00,  2.6864e+00, -3.3090e+00,\n         3.8922e+00, -4.6573e+00,  6.0626e-02,  9.8770e+00,  5.9841e+00,\n         4.6795e+00, -9.9704e+00, -1.1974e+01,  1.3528e+01, -1.0259e+01,\n         2.6449e+00,  2.7114e+00,  3.6057e+00,  7.8294e-01,  7.4036e+00,\n        -4.2393e+00, -9.4118e+00, -2.5294e+00,  4.4294e+00,  2.2434e+00,\n        -8.6981e+00, -4.1438e+00,  6.2667e+00, -2.0602e+00,  3.7619e+00,\n        -3.0744e+00, -9.6640e+00, -2.5041e+00,  6.3875e+00, -1.0970e+01,\n         3.7260e+00,  5.1450e+00,  7.0782e+00, -6.6857e-01,  1.0461e+01,\n        -1.5351e+01,  1.4029e+01, -2.8778e+00,  3.1665e+00,  9.0058e-01,\n        -1.2030e+00, -7.5055e+00, -1.5109e+00,  4.1435e-01,  6.8360e+00,\n        -2.4361e+00,  4.7261e+00, -1.6767e+00,  3.2715e+00, -1.3578e-01,\n        -3.1394e+00, -2.9984e+00,  6.4536e+00, -5.1250e+00,  1.6784e+00,\n        -4.7196e+00, -2.6147e+00,  7.6760e-01, -8.7019e+00, -8.0888e-02,\n        -6.9515e+00, -1.5935e+00,  1.5916e+00, -3.4638e+00,  1.1822e+00,\n        -8.6731e+00,  3.6638e+00, -1.4270e+00,  8.3378e+00, -7.6860e+00,\n        -4.1551e+00, -9.2020e+00, -7.6699e-01, -3.3690e+00,  5.0199e+00,\n        -9.9882e+00,  4.2354e+00,  6.6258e+00,  9.0864e-01,  1.8286e+00,\n         2.9860e+00,  4.2116e+00, -1.8121e+00, -4.0877e+00, -2.8210e+00,\n         5.7568e+00, -4.4066e+00, -3.5520e+00, -2.6152e+00,  4.3140e+00,\n        -3.8031e+00,  9.6800e+00,  8.8169e-01, -3.6339e+00,  2.3514e+00,\n        -5.7335e+00, -4.2699e+00, -7.8389e+00, -5.9741e+00, -6.1156e-01,\n        -4.1624e+00,  1.0402e+01, -7.2951e+00,  2.6416e+00, -8.3029e+00,\n        -6.5166e-02, -4.8866e+00,  2.4596e+00,  3.9736e+00,  5.4901e+00,\n         4.2202e+00, -3.8306e+00, -6.2605e+00,  2.4231e+00, -4.0871e+00,\n        -6.3616e+00, -1.9237e+00,  2.6201e-01,  3.9231e+00, -4.2554e+00,\n         3.0737e+00, -7.9324e+00,  7.4980e-01,  6.2263e+00, -6.8387e+00,\n         6.5169e+00,  9.6261e+00,  3.0533e+00,  5.7031e+00,  5.2647e-01,\n         1.1886e+01,  4.2957e+00, -5.9036e+00,  5.3113e+00, -4.7593e+00,\n         2.3902e+00,  1.1530e+01,  1.1453e+00,  7.0914e+00,  4.4221e+00,\n         3.8341e+00,  1.3872e+00, -1.1134e+01, -1.0338e+01,  4.0840e-01,\n        -6.0602e+00,  4.3299e-01,  9.8048e-01, -2.1421e+00,  1.0620e+00,\n        -4.7048e+00, -3.7180e+00,  2.4049e+00,  1.8676e-01,  6.6197e-01,\n        -1.0010e+01, -4.6739e+00, -1.0513e+01, -2.6404e+00, -2.6510e+00,\n         7.2716e+00,  1.0749e+01,  6.6575e+00, -2.3899e+00, -7.3637e+00,\n        -3.9341e+00,  7.1931e+00, -2.3082e+00,  3.2126e+00, -2.4925e+00,\n         3.2683e+00, -5.0204e-01,  2.8221e+00, -8.1911e+00, -3.4407e+00,\n        -3.3204e+00,  4.8306e+00, -1.5854e-01,  2.6516e+00,  8.1704e-01,\n        -8.7983e+00,  4.0943e+00, -5.6544e+00,  4.1259e+00, -7.5912e-01,\n         2.2961e+00, -7.4419e+00, -6.0864e+00, -5.4809e-01, -8.6358e+00,\n         5.7299e+00, -5.8645e+00, -2.8637e+00,  4.1994e+00, -1.2684e+00,\n         4.6839e+00,  6.0358e+00, -4.8966e+00,  5.9101e+00, -1.6126e+00,\n        -1.4668e+01, -1.3364e+00, -8.6564e+00,  4.0443e+00,  8.1136e+00,\n         4.7079e+00, -9.9475e+00,  2.6451e+00,  2.7019e+00,  9.4108e+00,\n         4.5954e+00,  2.6015e+00, -1.4042e+00, -4.1166e+00, -9.4354e+00,\n         5.7500e+00, -6.5840e+00,  1.6345e+00, -3.9349e+00,  9.7327e+00,\n        -6.1233e+00,  3.4738e+00, -4.5577e+00, -1.3380e+00,  1.8434e+00,\n         8.1936e+00, -8.6092e+00, -3.4719e+00,  5.0703e+00, -4.4664e+00,\n         6.4704e+00, -5.7986e+00,  8.3599e+00, -2.7207e+00, -3.5988e+00,\n         1.0794e+01,  4.5068e+00,  1.0046e+01, -2.2723e+00, -6.5263e+00,\n        -2.2766e-01, -8.6315e-01, -6.4219e+00, -6.8218e+00, -3.5682e+00,\n        -4.7119e+00, -1.8907e-01,  5.3918e+00,  1.3922e+00, -3.0035e+00,\n         8.9866e+00, -2.6661e+00, -6.6728e+00,  1.1000e+01,  2.2653e+00,\n        -7.9431e+00,  1.8294e+00,  3.0557e+00,  9.4934e+00, -3.0099e+00,\n        -1.1694e+00, -2.3632e+00,  1.2139e+01, -4.4225e-01, -5.4353e+00,\n         6.4613e+00, -5.7222e+00, -8.5285e+00, -4.6763e+00,  4.3144e+00,\n         1.4013e-01, -2.5612e+00,  7.9967e-01,  7.7650e+00, -3.3219e+00,\n         5.2308e+00,  6.5145e-01,  8.7892e-01, -7.4683e+00,  1.0657e+01,\n         4.7196e+00, -1.6202e+00, -1.1049e+01, -3.5860e+00, -1.7874e+00,\n        -4.9695e-01,  2.7570e-01,  1.9470e+00, -2.2283e+00, -6.6657e-01,\n         2.4241e+00,  9.1253e+00,  4.5713e+00,  1.9809e+01, -4.0868e-01,\n        -7.1518e+00, -4.3420e+00, -1.2206e+00, -4.8121e+00, -3.5091e+00,\n         1.4510e+00, -4.8544e+00,  4.0994e+00, -2.3534e+00, -2.9900e+00,\n        -9.6720e+00, -2.9801e+00, -1.8269e+00, -1.0490e+01, -2.8494e+00,\n         3.2904e+00, -3.4714e+00,  6.3171e+00, -6.0031e-01,  6.4215e+00,\n         7.7318e+00, -1.1628e+01, -2.2580e+00,  6.5713e+00, -1.3225e+01,\n         1.8486e+00,  5.2005e-01, -1.1946e+00,  3.4093e-01,  6.6873e+00,\n         3.8414e+00, -6.7605e+00, -3.6807e+00,  6.5493e+00, -7.2647e+00,\n         5.5535e+00, -3.5462e+00,  4.3100e+00, -1.9080e+00,  4.4815e+00,\n         1.4344e+00, -9.2792e+00,  5.1683e-01,  2.5537e+00, -3.7238e+00,\n        -3.5990e+00, -9.6038e-01,  2.6059e+00,  3.2556e+00, -3.9208e+00,\n        -1.1759e+00, -2.2442e+00, -5.0637e+00,  5.1637e+00,  8.1804e+00,\n         6.5776e+00,  6.8021e+00, -7.5181e+00,  1.6102e+00, -9.6251e+00,\n         1.0529e+01,  6.9914e+00,  2.0125e+00,  5.1722e+00, -3.1221e+00,\n         1.7856e+00, -6.8784e+00, -1.0140e+00,  4.2255e+00,  4.5046e+00,\n        -6.5289e+00, -3.6468e+00, -5.1887e-01,  6.8367e+00,  1.8442e+00,\n         1.9149e+00, -9.1361e+00, -2.6483e+00, -4.3092e+00, -3.8572e+00,\n         4.2038e+00, -4.3649e+00,  6.4979e-01,  3.2503e+00,  7.7257e-01,\n        -9.5746e+00, -5.9892e+00,  1.3186e+00,  1.0000e+00],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:47:38.230894Z","iopub.execute_input":"2024-08-07T05:47:38.231475Z","iopub.status.idle":"2024-08-07T05:47:38.533303Z","shell.execute_reply.started":"2024-08-07T05:47:38.231444Z","shell.execute_reply":"2024-08-07T05:47:38.532165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Training with custom feature loader\n\n# Hyperparameters\nfeature_dim = 2048\nnum_classes = 8\nbatch_size = 128\nnum_epochs = 10\nlearning_rate = 1e-4\nsequence_length = 64\n\n# Model, loss function, optimizer\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to('cuda')\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n# try SGD Optimizer \n \nloss_list = []\n\nimage_features = image_features.cuda()\nlabel_tensor = label_tensor.cuda()\n\n# Training loop\nfor epoch in range(1, num_epochs):\n    model.train()\n    running_loss = 0.0\n    seq_start = 0\n    seq_end = sequence_length\n    progress_bar = tqdm(total = len(labels))\n    while seq_end < label_tensor.shape[0]:\n        optimizer.zero_grad()\n        out = []\n        targets = []\n        for i in range(batch_size):\n            if seq_end >= label_tensor.shape[0]:\n                break\n            inputs = image_features[seq_start:seq_start + sequence_length, : ]\n            target = label_tensor[seq_end]\n#             print('hello',inputs.shape)\n#             print(targets.shape)\n            # Forward pass\n            outputs = model(inputs)\n\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            out.append(outputs)\n            target = target.view(-1)\n            targets.append(target)\n            seq_start += 1\n            seq_end += 1\n            if seq_end % 100 == 0:\n                progress_bar.update(100)\n                    # Calculate loss\n        pred = torch.row_stack(out)\n        truth = torch.tensor(targets).cuda()\n        loss = criterion(pred, truth)\n        # Backward pass and optimize\n#             print(loss)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        loss_list.append(loss.item())\n        # Update progress bar with current loss\n#         data_loader.set_postfix(loss=loss.item())\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n    torch.save(model.state_dict(), f'transformer_on_resnet_50_e{epoch}.pth')\n    progress_bar.close()\n\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:26:21.751251Z","iopub.execute_input":"2024-07-31T15:26:21.752237Z","iopub.status.idle":"2024-07-31T15:26:22.676823Z","shell.execute_reply.started":"2024-07-31T15:26:21.752196Z","shell.execute_reply":"2024-07-31T15:26:22.675248Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae97e605a7b4dfcb46d2886d109f878"}},"metadata":{}},{"name":"stdout","text":"After transformer encoding:  torch.Size([64, 2048])\nTake mean:  torch.Size([64])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m             target \u001b[38;5;241m=\u001b[39m label_tensor[seq_end]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             print('hello',inputs.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#             print(targets.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;66;03m# Reshape outputs and targets to be compatible with the loss function\u001b[39;00m\n\u001b[1;32m     44\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mImageTransformer.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTake mean: \u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)","output_type":"error"}]},{"cell_type":"code","source":"# Validation\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth'))\nmodel.eval()\nseq_start = 0\nseq_end = sequence_length\nprogress_bar = tqdm(total = len(labels))\npreds = []\nwith torch.no_grad():\n    while seq_end < label_tensor.shape[0]:\n\n        inputs = image_features[seq_start:seq_start + sequence_length, : ].cuda()\n        target = label_tensor[seq_end]\n    #             print('hello',inputs.shape)\n    #             print(targets.shape)\n        # Forward pass\n        outputs = model(inputs)\n        preds.append(torch.argmax(outputs).item())\n        seq_start += 1\n        seq_end += 1\n        progress_bar.update(1)\n\nprint(len(preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:58:31.795782Z","iopub.execute_input":"2024-08-10T05:58:31.796297Z","iopub.status.idle":"2024-08-10T05:58:33.40848Z","shell.execute_reply.started":"2024-08-10T05:58:31.79626Z","shell.execute_reply":"2024-08-10T05:58:33.406961Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageTransformer(feature_dim\u001b[38;5;241m=\u001b[39mfeature_dim, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, sequence_length \u001b[38;5;241m=\u001b[39m sequence_length)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m seq_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:993\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    991\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    992\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    995\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:447\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"],"ename":"RuntimeError","evalue":"PytorchStreamReader failed reading zip archive: failed finding central directory","output_type":"error"}]},{"cell_type":"code","source":"# Validation\n# feature_dim = 2048\n# num_classes = 8\n# batch_size = 128\n# num_epochs = 10\n# learning_rate = 1e-4\n# sequence_length = 64\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/SGD_Kaiming/transformer_on_ViT_e80.pth', map_location=torch.device('cpu')))\ncriterion = nn.CrossEntropyLoss()\n\nmodel.eval()\n\nall_preds = []\nall_targets = []\nrunning_loss = 0.0\nwith torch.no_grad():\n    progress_bar = tqdm(total=len(data_loader_val), desc='Validation', unit='batch')\n    for features, targets in data_loader_val:\n#         features, targets = features.cuda(), targets.cuda() \n        outputs = model(features)\n        # Reshape outputs and targets to be compatible with the loss function\n        outputs = outputs.view(-1, num_classes)\n        # Calculate loss\n        validation_loss = criterion(outputs, targets)\n        running_loss += validation_loss.item() * features.size(0)\n        # Collect predictions and true labels\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(loss=validation_loss.item())\n    \n    progress_bar.close()\n\nvalidation_loss = running_loss / val_length\n\n# Calculate metrics\nall_preds = np.array(all_preds)\nall_targets = np.array(all_targets)\n\nf1 = f1_score(all_targets, all_preds, average='weighted')\nprecision = precision_score(all_targets, all_preds, average='weighted')\nrecall = recall_score(all_targets, all_preds, average='weighted')\naccuracy = accuracy_score(all_targets, all_preds)\n\nprint(f'Validation Loss: {validation_loss:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-09T15:05:01.05667Z","iopub.execute_input":"2024-08-09T15:05:01.057648Z","iopub.status.idle":"2024-08-09T15:10:22.121773Z","shell.execute_reply.started":"2024-08-09T15:05:01.057602Z","shell.execute_reply":"2024-08-09T15:10:22.120535Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/120 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3df2c7cad4f3424c989f59f982efc119"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.9009\nF1 Score: 0.1363\nPrecision: 0.0884\nRecall: 0.2974\nAccuracy: 0.2974\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from scipy.ndimage import gaussian_filter1d\nimport numpy as np\n\npreds = torch.tensor(preds)\npreds = preds.to('cpu')\n\nsigma = 1.0  # Standard deviation of the Gaussian kernel\nfiltered_array = gaussian_filter1d(preds, sigma=sigma)\n\n# Discretize the filtered values\ndiscretized_array = np.round(filtered_array).astype(int)  # Round and convert to integers\n\n# Convert the result back to a PyTorch tensor\nfiltered_tensor = torch.tensor(discretized_array, dtype=torch.int)\n\n\nfiltered_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:37.309357Z","iopub.execute_input":"2024-08-05T10:29:37.31004Z","iopub.status.idle":"2024-08-05T10:29:37.320912Z","shell.execute_reply.started":"2024-08-05T10:29:37.31001Z","shell.execute_reply":"2024-08-05T10:29:37.320062Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/172670838.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  preds = torch.tensor(preds)\n","output_type":"stream"},{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"torch.Size([64])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Example ground truth and predictions\ny_true = label_tensor[sequence_length:]  # True labels\ny_pred = preds  # Predicted labels\n\n# Calculate precision, recall, and F1 score for different averaging methods\nprecision_macro = precision_score(y_true, y_pred, average='macro')\nrecall_macro = recall_score(y_true, y_pred, average='macro')\nf1_macro = f1_score(y_true, y_pred, average='macro')\n\nprecision_micro = precision_score(y_true, y_pred, average='micro')\nrecall_micro = recall_score(y_true, y_pred, average='micro')\nf1_micro = f1_score(y_true, y_pred, average='micro')\n\nprecision_weighted = precision_score(y_true, y_pred, average='weighted')\nrecall_weighted = recall_score(y_true, y_pred, average='weighted')\nf1_weighted = f1_score(y_true, y_pred, average='weighted')\n\nprint(f'Precision (macro): {precision_macro:.2f}')\nprint(f'Recall (macro): {recall_macro:.2f}')\nprint(f'F1 Score (macro): {f1_macro:.2f}')\n\nprint(f'Precision (micro): {precision_micro:.2f}')\nprint(f'Recall (micro): {recall_micro:.2f}')\nprint(f'F1 Score (micro): {f1_micro:.2f}')\n\nprint(f'Precision (weighted): {precision_weighted:.2f}')\nprint(f'Recall (weighted): {recall_weighted:.2f}')\nprint(f'F1 Score (weighted): {f1_weighted:.2f}')\n\naccuracy = accuracy_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:44.908057Z","iopub.execute_input":"2024-08-05T10:29:44.908952Z","iopub.status.idle":"2024-08-05T10:29:45.274898Z","shell.execute_reply.started":"2024-08-05T10:29:44.9089Z","shell.execute_reply":"2024-08-05T10:29:45.273544Z"},"trusted":true},"execution_count":117,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m preds  \u001b[38;5;66;03m# Predicted labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, and F1 score for different averaging methods\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m precision_macro \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m recall_macro \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     y_true,\n\u001b[1;32m   1827\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15408, 64]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [15408, 64]","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor(float(outputs.argmax().item()))","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.131308Z","iopub.status.idle":"2024-07-31T05:24:11.131763Z","shell.execute_reply.started":"2024-07-31T05:24:11.131523Z","shell.execute_reply":"2024-07-31T05:24:11.131541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/kaggle/input/abaw-7-dataset/cropped_aligned/1-30-1280x720/00001.jpg').convert('RGB')\ntransform = transforms.Compose([\n            transforms.Resize([224, 224])\n        ])\n\ntransform(img)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.133633Z","iopub.status.idle":"2024-07-31T05:24:11.13411Z","shell.execute_reply.started":"2024-07-31T05:24:11.1339Z","shell.execute_reply":"2024-07-31T05:24:11.133924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}