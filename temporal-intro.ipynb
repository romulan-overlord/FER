{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9108739,"sourceType":"datasetVersion","datasetId":5473430},{"sourceId":8606881,"sourceType":"datasetVersion","datasetId":5122606}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riturajpradhan/temporal-intro?scriptVersionId=191485207\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n\nfrom torchvision import transforms\n# from transformers import AutoImageProcessor, ResNetModel\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n# from transformers.image_processing_base import BatchFeature","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T05:24:07.02577Z","iopub.execute_input":"2024-08-07T05:24:07.026488Z","iopub.status.idle":"2024-08-07T05:24:07.03277Z","shell.execute_reply.started":"2024-08-07T05:24:07.026455Z","shell.execute_reply":"2024-08-07T05:24:07.031826Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"image_paths = []\nlabels = []\nlabel_path = '/kaggle/input/abaw-7-dataset/training_set_annotations.txt'\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nwith open(label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_name = line[0]\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    image_path = os.path.join(data_path, image_name)\n    image_paths.append(image_path)\n    labels.append(image_label)\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:24:10.016916Z","iopub.execute_input":"2024-08-07T05:24:10.017286Z","iopub.status.idle":"2024-08-07T05:24:10.504672Z","shell.execute_reply.started":"2024-08-07T05:24:10.017256Z","shell.execute_reply":"2024-08-07T05:24:10.503885Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"os.rmdir('val')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:38:23.064579Z","iopub.execute_input":"2024-08-07T05:38:23.065289Z","iopub.status.idle":"2024-08-07T05:38:23.069052Z","shell.execute_reply.started":"2024-08-07T05:38:23.06526Z","shell.execute_reply":"2024-08-07T05:38:23.068198Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# run cell to extract image features and store them\nimage_preprocessor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nfeature_extractor = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True).to(device)\n    \n# Batch size for processing images\nbatch_size = 500\n\nbatch_count = 0\n\n# Initialize lists to store extracted features\nall_features = []\n\n# Process images in batches\nfor batch_start in tqdm(range(0, len(image_paths), batch_size), desc='Extracting Features'):\n    batch_end = min(batch_start + batch_size, len(image_paths))\n    batch_images = [Image.open(image_path) for image_path in image_paths[batch_start:batch_end]]\n    batch_labels = torch.tensor([float(label) for label in labels[batch_start:batch_end]]).to(device)\n    # Tokenize and extract features\n    inputs = image_preprocessor(images = batch_images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = feature_extractor(**inputs)\n    t = torch.column_stack([output.hidden_states[-1][:,0,:], torch.tensor(batch_labels)])\n\n    # Append features to the list\n    all_features.extend(t)\n    break\n\n# Save features to a file (e.g., as a PyTorch tensor)\n    # Save features every 1000 images\n    if batch_count%50 == 0:\n        output_file = f\"./training_features/image_features_val_{batch_count}.pt\"\n        torch.save(torch.stack(all_features).cpu(), output_file)\n        print(f\"Features saved to {output_file}\")\n        \n        # Clear memory by resetting the list\n        all_features = []\n    batch_count += 1\n    with open('counter.json', 'w') as f:\n        json.dump({'batch_count' : batch_count}, f)\n\n# Save any remaining features\nif all_features:\n    output_file = f\"./training_features/image_features_val_{batch_count}.pt\"\n    torch.save(torch.stack(all_features).cpu(), output_file)\n    print(f\"Remaining features saved to {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:32:59.426313Z","iopub.execute_input":"2024-08-07T05:32:59.4273Z","iopub.status.idle":"2024-08-07T05:33:02.078164Z","shell.execute_reply.started":"2024-08-07T05:32:59.427266Z","shell.execute_reply":"2024-08-07T05:33:02.076483Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting Features:   0%|          | 0/182 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c70615aa4fec4b45bae9a5e61099b9f3"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[79], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m image_preprocessor(images \u001b[38;5;241m=\u001b[39m batch_images, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcolumn_stack([output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;241m0\u001b[39m,:], torch\u001b[38;5;241m.\u001b[39mtensor(batch_labels)])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Append features to the list\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:831\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 831\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    842\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:614\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m    610\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    611\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    612\u001b[0m )\n\u001b[0;32m--> 614\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:443\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    436\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    437\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    438\u001b[0m         hidden_states,\n\u001b[1;32m    439\u001b[0m         layer_head_mask,\n\u001b[1;32m    440\u001b[0m         output_attentions,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:401\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m    400\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m--> 401\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[1;32m    404\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(layer_output, hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:343\u001b[0m, in \u001b[0;36mViTIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    342\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 343\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 15.89 GiB of which 511.12 MiB is free. Process 2424 has 15.39 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 925.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 15.89 GiB of which 511.12 MiB is free. Process 2424 has 15.39 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 925.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"len(all_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:33:20.178659Z","iopub.execute_input":"2024-08-07T05:33:20.179422Z","iopub.status.idle":"2024-08-07T05:33:20.185251Z","shell.execute_reply.started":"2024-08-07T05:33:20.179388Z","shell.execute_reply":"2024-08-07T05:33:20.184208Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# # loading image features to RAM\n# # just load it from ABAW_training_features.pt\n# t = []\n\n# # Define the file indices\n# file_indices = [0, 50, 100, 150, 182]\n\n# # Load the .pt files\n# for i in file_indices:\n#     file_path = f'/kaggle/working/val/image_features_val_{str(i)}.pt'\n#     try:\n#         t.append(torch.load(file_path, map_location=torch.device('cpu')))  # Use map_location if necessary\n#     except RuntimeError as e:\n#         print(f\"Failed to load {file_path}: {e}\")\n\n# # Stack the tensors if they were loaded successfully\n# if t:\n#     image_features = torch.row_stack(t)\n#     image_features.requires_grad = True\n# else:\n#     print(\"No tensors were loaded successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:29:50.499121Z","iopub.execute_input":"2024-07-31T09:29:50.499803Z","iopub.status.idle":"2024-07-31T09:29:51.175065Z","shell.execute_reply.started":"2024-07-31T09:29:50.499772Z","shell.execute_reply":"2024-07-31T09:29:51.174221Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class ImageTransformer(nn.Module):\n    def __init__(self, feature_dim, num_classes, num_heads=4, num_layers=6, dropout=0.1, sequence_length=64):\n        super(ImageTransformer, self).__init__()\n        self.feature_dim = feature_dim\n        self.num_classes = num_classes\n        \n        # Positional encoding\n        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, feature_dim))\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_classes, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Classification head\n        self.fc1 = nn.Linear(feature_dim, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, num_classes)\n    \n    def forward(self, pixel_values):\n        # Add positional encoding\n        pixel_values = pixel_values + self.positional_encoding\n        \n        # Transformer encoder\n        x = self.transformer_encoder(pixel_values)\n        # Pooling: take the mean of the sequence\n        x = x.mean(dim=1)\n        # Classification\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        \n        return x\n\nclass ABAWFeatureDataset(Dataset):\n    def __init__(self, features, labels, sequence_length):\n        self.features = features\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.seq_start = 0\n        self.seq_end = sequence_length\n        self.length = labels.shape[0]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if idx > (self.length - self.sequence_length):\n            feature = self.features[self.length - self.sequence_length:, : ]\n            label = self.labels[self.length - 1]\n        else:\n            feature = self.features[idx:idx + self.sequence_length, :]\n            label = self.labels[idx + self.sequence_length - 1]\n        return feature, label","metadata":{"execution":{"iopub.status.busy":"2024-08-05T09:08:53.927024Z","iopub.execute_input":"2024-08-05T09:08:53.927842Z","iopub.status.idle":"2024-08-05T09:08:53.940703Z","shell.execute_reply.started":"2024-08-05T09:08:53.927805Z","shell.execute_reply":"2024-08-05T09:08:53.939655Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nsequence_length = 32\nfeature_dim = 2048\nnum_classes = 8\nnum_epochs = 10\nlearning_rate = 1e-7\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nval_label_path = '/kaggle/input/abaw7-extracted-features/validation_set_annotations.txt'\nval_labels = []\nwith open(val_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    val_labels.append(image_label)\n    \ntrain_label_path = '/kaggle/input/abaw7-extracted-features/training_set_annotations.txt'\ntrain_labels = []\nwith open(train_label_path, 'r') as f:\n    f.readline()\n    data = f.readlines()\n\nfor d in data:\n    line = d.split(',')\n    image_label = line[3]\n    if int(image_label) == -1:\n        continue\n    train_labels.append(image_label)\n    \n# Create dataset and dataloader\ntrain_image_features = torch.load('/kaggle/input/abaw7-extracted-features/ABAW_training_features.pt')\ntemp = train_image_features.detach().numpy()\ntrain_image_features = torch.tensor(temp)\n\ntrain_labels = [int(x) for x in train_labels]\ntrain_label_tensor = torch.tensor(train_labels)\ntrain_dataset = ABAWFeatureDataset(train_image_features, train_label_tensor, sequence_length = sequence_length)\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, drop_last = True, pin_memory=True)\n\ntrain_length = len(train_dataset)\n    \nval_image_features = torch.load('/kaggle/input/abaw7-extracted-features/ABAW_validation_features.pt')\ntemp = val_image_features.detach().numpy()\nval_image_features = torch.tensor(temp)\nval_labels = [int(x) for x in val_labels]\nval_label_tensor = torch.tensor(val_labels)\nval_dataset = ABAWFeatureDataset(val_image_features, val_label_tensor, sequence_length = sequence_length)\ndata_loader_val = DataLoader(dataset, batch_size=batch_size, drop_last = True, pin_memory=True)#, collate_fn=custom_collate_fn)\n\nval_length = len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:31:11.254767Z","iopub.execute_input":"2024-08-05T10:31:11.255134Z","iopub.status.idle":"2024-08-05T10:31:15.831974Z","shell.execute_reply.started":"2024-08-05T10:31:11.255104Z","shell.execute_reply":"2024-08-05T10:31:15.831194Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# Model, loss function, optimizer\n# model = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\n# model.load_state_dict(torch.load('/kaggle/working/transformer_on_resnet_50_e0.pth'))\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nscaler = GradScaler()\n\n# training_loss_list = []\n# validation_loss_list = []\n# loss_list = []\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    progress_bar = tqdm(total=len(data_loader_train), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n    \n    for features, targets in data_loader_train:\n        features, targets = features.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(features)\n            # Reshape outputs and targets to be compatible with the loss function\n#             outputs = outputs.view(-1, num_classes)\n#             targets = targets.view(-1)\n            # Calculate loss\n            loss = criterion(outputs, targets)\n        \n        \n#         print('before back')\n        scaler.scale(loss).backward()\n#         print('after back')\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * features.size(0)\n        loss_list.append(loss.item())\n        # Update progress bar with current loss\n        progress_bar.set_postfix(loss=loss.item())\n        progress_bar.update(1)\n        \n    epoch_loss = running_loss / train_length\n    torch.save(model.state_dict(), f'transformer_on_resnet_50_e{epoch}.pth')\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for features, targets in data_loader_val:\n            features, targets = features.cuda(), targets.cuda() \n            outputs = model(features)\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            # Calculate loss\n            validation_loss = criterion(outputs, targets)\n            running_loss += validation_loss.item() * features.size(0)\n    \n    validation_loss = running_loss / val_length\n    \n    training_loss_list.append(epoch_loss)\n    validation_loss_list.append(validation_loss)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, validation Loss: {validation_loss:.4f}')\n    progress_bar.close()\n\nprint(\"Training complete!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing new feature extractor","metadata":{}},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state,","metadata":{"execution":{"iopub.status.busy":"2024-08-07T04:46:47.151621Z","iopub.execute_input":"2024-08-07T04:46:47.151977Z","iopub.status.idle":"2024-08-07T04:46:50.533433Z","shell.execute_reply.started":"2024-08-07T04:46:47.151948Z","shell.execute_reply":"2024-08-07T04:46:50.532129Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040b2960c7154fdf8086db4fb9589a34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b6398e5f14449d87f66db83a2ac8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc93215a91c542ec9c4258d1bb48275e"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimages = [image for i in range(1,32)]\nlabels = [i for i in range(1,32)]\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states = True, return_dict = True)\n\ninputs = feature_extractor(images=images, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\n# predicted_class_idx = logits.argmax(-1).item()\n# print(\"Predicted class:\", model.config.id2label[predicted_class_idx]),","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:15:41.071334Z","iopub.execute_input":"2024-08-07T05:15:41.072339Z","iopub.status.idle":"2024-08-07T05:15:48.509017Z","shell.execute_reply.started":"2024-08-07T05:15:41.072296Z","shell.execute_reply":"2024-08-07T05:15:48.508199Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# outputs.hidden_states[-1][:,0,:]\nimport torch\nt = torch.column_stack([outputs.hidden_states[-1][:,0,:], torch.tensor(labels)])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:16:50.498825Z","iopub.execute_input":"2024-08-07T05:16:50.499185Z","iopub.status.idle":"2024-08-07T05:16:50.505383Z","shell.execute_reply.started":"2024-08-07T05:16:50.499156Z","shell.execute_reply":"2024-08-07T05:16:50.504416Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"t[0,:]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:17:22.258295Z","iopub.execute_input":"2024-08-07T05:17:22.258632Z","iopub.status.idle":"2024-08-07T05:17:22.273578Z","shell.execute_reply.started":"2024-08-07T05:17:22.258608Z","shell.execute_reply":"2024-08-07T05:17:22.272686Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"tensor([ 2.3126e+00,  5.5116e+00,  1.1788e+01,  5.7725e-01,  6.5475e+00,\n        -2.9125e+00,  4.5668e+00, -1.3786e+00,  6.1539e+00, -5.1831e+00,\n         4.9711e+00,  8.4007e-01,  7.7080e+00, -3.0897e+00, -3.2443e+00,\n         9.7258e+00,  1.1146e+00, -2.1286e+00,  8.7621e+00,  1.6315e+00,\n        -8.4574e+00,  1.8422e+00,  1.4254e+00,  6.5619e+00, -1.0730e+01,\n         6.0743e+00,  4.2650e+00,  6.0531e+00,  8.9479e+00,  2.5177e+00,\n         5.4446e-01,  1.4944e+00,  4.0779e+00,  1.0520e+01, -2.9379e+00,\n         6.8438e+00, -4.6463e+00, -3.0405e+00,  1.0135e+00,  8.2927e+00,\n         9.9011e+00, -3.0263e+00,  7.5373e-01, -4.1869e+00,  2.0160e+00,\n         7.0067e+00,  7.9857e-02, -2.2949e+00,  2.2206e+00, -4.4853e+00,\n         1.8516e+00, -7.8394e-01,  4.4800e+00, -3.8795e+00, -1.1654e+01,\n        -2.1962e+00,  2.5946e-01,  1.0626e+01, -5.4844e-01, -2.7896e+00,\n         1.3051e+01,  8.1810e+00,  4.4351e+00, -6.2068e+00,  1.0071e+00,\n        -4.3451e+00,  2.8967e+00, -1.7342e+00,  5.2872e+00, -4.1522e+00,\n        -9.2688e+00,  9.9553e+00, -3.6386e+00, -2.5019e+00, -8.4062e+00,\n         4.9192e-01, -9.5148e+00, -6.4460e+00, -3.4781e+00, -1.1284e+00,\n         8.1581e+00, -1.0863e+01, -1.0140e+01,  8.8340e+00,  7.9968e+00,\n         5.1645e+00, -3.4996e+00,  1.1152e+01,  4.4518e+00, -1.2614e+00,\n         1.7204e+01, -4.9011e-01,  5.3941e+00,  7.6976e+00,  1.5465e-01,\n        -6.2431e+00, -3.6648e+00, -4.9716e-01,  1.6759e+00, -1.0212e+01,\n        -4.2299e+00,  4.3840e+00, -7.6967e-01,  6.4818e-01, -1.2815e+00,\n         4.0404e+00, -2.7555e+00,  8.0193e+00, -4.0383e+00, -2.7090e+00,\n        -3.0092e+00, -2.2504e+00,  1.4036e+00, -1.9877e+00, -5.8852e-01,\n         4.5092e+00, -9.2577e+00,  8.3078e+00, -2.5480e+00, -2.3309e+00,\n         8.1919e+00,  1.7041e+00, -6.2756e+00, -5.1414e-01,  2.3397e+00,\n         1.4869e+00, -3.0239e+00, -1.0958e+01,  2.1754e+00, -1.1177e-01,\n         5.2057e+00, -4.3006e+00,  5.6387e+00, -1.9813e+00,  5.1607e+00,\n         7.4258e-01,  5.1081e+00, -4.8063e+00, -3.0374e+00,  5.3849e+00,\n        -2.8684e+00, -3.7519e+00,  5.6947e+00,  4.2382e+00, -1.0009e+00,\n        -2.1228e-01,  3.3212e+00, -8.0935e+00,  5.2201e+00, -6.6542e+00,\n        -8.2925e+00, -7.8512e+00,  4.4826e+00,  2.8323e+00, -2.7104e+00,\n         1.2681e+01,  4.5946e+00,  1.5467e+00, -8.7506e+00, -1.1305e+01,\n         3.3003e+00, -7.4826e+00,  5.0542e+00, -1.7443e+00, -1.0060e+00,\n         5.6709e+00, -3.6740e+00,  1.5832e+01,  1.2297e+01,  3.3019e+00,\n         1.9885e+00, -9.0401e-01,  5.0012e-01,  1.2189e+01, -1.2169e+00,\n         2.1518e+00,  3.6889e+00,  1.9564e+00,  7.4761e+00, -8.1639e-01,\n         3.7544e+00, -4.9893e+00, -8.8977e+00, -1.5014e-01, -1.5335e-01,\n        -7.6970e+00, -5.8783e+00,  2.3438e+00, -1.0910e+01, -7.3558e+00,\n        -1.3294e-01,  1.1221e+00,  1.8191e+00,  4.9105e+00, -9.0446e+00,\n         1.1171e+01,  2.4723e+00, -6.4088e+00, -4.9586e+00,  1.7104e+01,\n        -7.0072e-01, -9.1785e+00,  9.9637e-01, -8.4077e+00,  4.5781e-01,\n         1.4394e+00,  4.4015e+00,  7.7832e+00,  1.5674e+01,  9.7669e+00,\n         9.5864e+00,  3.4548e-01, -9.5140e+00, -3.6262e+00, -3.3511e-02,\n         9.5605e-01,  2.7343e+00, -7.1423e+00, -4.8976e+00,  8.2165e+00,\n        -1.0892e+01,  5.3325e+00, -6.9168e-01, -6.0497e+00, -3.9966e+00,\n         2.9761e+00, -4.5020e+00, -4.4989e+00, -2.4625e+00, -2.3799e+00,\n         3.4785e+00, -6.4768e-02, -1.4270e+01,  7.9958e-01, -8.5005e+00,\n        -6.1762e+00,  1.3880e+00,  4.5403e-01,  1.8790e+00, -5.1056e+00,\n        -9.7338e+00, -8.4601e+00,  2.7580e+00,  5.5546e+00, -3.5314e+00,\n         6.0557e+00, -2.3313e+00,  9.2037e+00,  4.2733e+00, -1.5292e+01,\n        -3.3109e-01, -2.5005e-01,  1.5867e+00, -4.6539e+00, -1.3627e+00,\n        -2.7924e-01, -1.0277e+00,  2.2768e+00,  7.8121e+00, -5.0762e+00,\n        -4.3243e+00,  6.6483e-01,  2.3120e+00, -1.0160e+00,  3.1079e+00,\n         4.0078e+00,  7.6681e+00,  2.7114e+00,  2.2636e+00,  6.3449e+00,\n         1.2057e+00,  9.2436e+00, -3.8165e+00, -4.0865e+00, -5.5988e+00,\n        -9.9585e+00,  5.4400e+00,  4.3910e-01,  4.3431e+00, -6.1047e+00,\n        -3.3372e+00, -5.3751e+00,  5.7061e+00,  5.4730e+00,  8.4846e+00,\n        -5.4949e+00, -9.1157e-01,  6.9689e-01,  7.5254e-02,  7.1937e+00,\n        -3.3917e+00, -5.1809e+00, -8.3977e-01,  6.8584e+00, -8.4331e+00,\n         5.7394e+00, -3.7869e+00,  4.9977e+00,  3.0823e+00,  7.9278e+00,\n        -4.2151e+00, -1.0965e+00,  2.5793e+00, -2.4594e+00,  5.3281e+00,\n        -1.2227e+01, -6.5901e-01,  1.7196e+01,  8.4178e+00, -5.5776e+00,\n         8.9550e+00, -9.1851e-01,  1.3255e+00,  1.1535e+01,  4.6121e+00,\n        -1.1908e+00,  4.8861e+00,  3.8148e+00,  3.0933e+00, -3.4191e+00,\n         8.8850e+00,  6.3427e+00, -8.7499e-01, -5.0558e+00, -3.7724e+00,\n        -2.2231e+00,  6.4751e+00, -6.2604e+00, -8.7503e+00,  3.7453e+00,\n        -4.1096e+00,  5.5612e+00, -7.2538e+00, -8.7126e+00,  2.3858e+00,\n        -7.6081e+00, -4.0343e+00,  1.6607e+01,  3.2290e+00, -1.1992e+01,\n         1.6804e+00,  1.0575e+01,  1.2953e+01,  5.3291e+00,  4.4644e+00,\n        -1.1186e+01,  7.1904e-02,  6.4786e+00,  1.3749e+01, -1.2863e+00,\n         2.1389e-01,  5.9342e+00,  4.6504e-01,  1.7982e-01, -8.1117e+00,\n        -7.9560e+00, -6.3646e+00, -4.2088e+00,  6.7474e+00, -8.9446e-01,\n        -6.3456e-01,  2.3697e+00,  1.0983e+01, -1.0096e+01, -1.4992e+00,\n        -1.3370e-01, -1.0659e+01,  1.0049e+01, -4.2879e+00, -2.5807e+00,\n         6.9250e+00, -5.1344e+00, -7.8552e+00,  2.5728e+00, -4.0974e+00,\n        -1.0583e+01,  4.3317e+00,  1.5247e+00, -7.4624e-01,  8.0111e-03,\n        -8.0616e+00,  3.6085e+00,  4.4804e+00,  2.6864e+00, -3.3090e+00,\n         3.8922e+00, -4.6573e+00,  6.0626e-02,  9.8770e+00,  5.9841e+00,\n         4.6795e+00, -9.9704e+00, -1.1974e+01,  1.3528e+01, -1.0259e+01,\n         2.6449e+00,  2.7114e+00,  3.6057e+00,  7.8294e-01,  7.4036e+00,\n        -4.2393e+00, -9.4118e+00, -2.5294e+00,  4.4294e+00,  2.2434e+00,\n        -8.6981e+00, -4.1438e+00,  6.2667e+00, -2.0602e+00,  3.7619e+00,\n        -3.0744e+00, -9.6640e+00, -2.5041e+00,  6.3875e+00, -1.0970e+01,\n         3.7260e+00,  5.1450e+00,  7.0782e+00, -6.6857e-01,  1.0461e+01,\n        -1.5351e+01,  1.4029e+01, -2.8778e+00,  3.1665e+00,  9.0058e-01,\n        -1.2030e+00, -7.5055e+00, -1.5109e+00,  4.1435e-01,  6.8360e+00,\n        -2.4361e+00,  4.7261e+00, -1.6767e+00,  3.2715e+00, -1.3578e-01,\n        -3.1394e+00, -2.9984e+00,  6.4536e+00, -5.1250e+00,  1.6784e+00,\n        -4.7196e+00, -2.6147e+00,  7.6760e-01, -8.7019e+00, -8.0888e-02,\n        -6.9515e+00, -1.5935e+00,  1.5916e+00, -3.4638e+00,  1.1822e+00,\n        -8.6731e+00,  3.6638e+00, -1.4270e+00,  8.3378e+00, -7.6860e+00,\n        -4.1551e+00, -9.2020e+00, -7.6699e-01, -3.3690e+00,  5.0199e+00,\n        -9.9882e+00,  4.2354e+00,  6.6258e+00,  9.0864e-01,  1.8286e+00,\n         2.9860e+00,  4.2116e+00, -1.8121e+00, -4.0877e+00, -2.8210e+00,\n         5.7568e+00, -4.4066e+00, -3.5520e+00, -2.6152e+00,  4.3140e+00,\n        -3.8031e+00,  9.6800e+00,  8.8169e-01, -3.6339e+00,  2.3514e+00,\n        -5.7335e+00, -4.2699e+00, -7.8389e+00, -5.9741e+00, -6.1156e-01,\n        -4.1624e+00,  1.0402e+01, -7.2951e+00,  2.6416e+00, -8.3029e+00,\n        -6.5166e-02, -4.8866e+00,  2.4596e+00,  3.9736e+00,  5.4901e+00,\n         4.2202e+00, -3.8306e+00, -6.2605e+00,  2.4231e+00, -4.0871e+00,\n        -6.3616e+00, -1.9237e+00,  2.6201e-01,  3.9231e+00, -4.2554e+00,\n         3.0737e+00, -7.9324e+00,  7.4980e-01,  6.2263e+00, -6.8387e+00,\n         6.5169e+00,  9.6261e+00,  3.0533e+00,  5.7031e+00,  5.2647e-01,\n         1.1886e+01,  4.2957e+00, -5.9036e+00,  5.3113e+00, -4.7593e+00,\n         2.3902e+00,  1.1530e+01,  1.1453e+00,  7.0914e+00,  4.4221e+00,\n         3.8341e+00,  1.3872e+00, -1.1134e+01, -1.0338e+01,  4.0840e-01,\n        -6.0602e+00,  4.3299e-01,  9.8048e-01, -2.1421e+00,  1.0620e+00,\n        -4.7048e+00, -3.7180e+00,  2.4049e+00,  1.8676e-01,  6.6197e-01,\n        -1.0010e+01, -4.6739e+00, -1.0513e+01, -2.6404e+00, -2.6510e+00,\n         7.2716e+00,  1.0749e+01,  6.6575e+00, -2.3899e+00, -7.3637e+00,\n        -3.9341e+00,  7.1931e+00, -2.3082e+00,  3.2126e+00, -2.4925e+00,\n         3.2683e+00, -5.0204e-01,  2.8221e+00, -8.1911e+00, -3.4407e+00,\n        -3.3204e+00,  4.8306e+00, -1.5854e-01,  2.6516e+00,  8.1704e-01,\n        -8.7983e+00,  4.0943e+00, -5.6544e+00,  4.1259e+00, -7.5912e-01,\n         2.2961e+00, -7.4419e+00, -6.0864e+00, -5.4809e-01, -8.6358e+00,\n         5.7299e+00, -5.8645e+00, -2.8637e+00,  4.1994e+00, -1.2684e+00,\n         4.6839e+00,  6.0358e+00, -4.8966e+00,  5.9101e+00, -1.6126e+00,\n        -1.4668e+01, -1.3364e+00, -8.6564e+00,  4.0443e+00,  8.1136e+00,\n         4.7079e+00, -9.9475e+00,  2.6451e+00,  2.7019e+00,  9.4108e+00,\n         4.5954e+00,  2.6015e+00, -1.4042e+00, -4.1166e+00, -9.4354e+00,\n         5.7500e+00, -6.5840e+00,  1.6345e+00, -3.9349e+00,  9.7327e+00,\n        -6.1233e+00,  3.4738e+00, -4.5577e+00, -1.3380e+00,  1.8434e+00,\n         8.1936e+00, -8.6092e+00, -3.4719e+00,  5.0703e+00, -4.4664e+00,\n         6.4704e+00, -5.7986e+00,  8.3599e+00, -2.7207e+00, -3.5988e+00,\n         1.0794e+01,  4.5068e+00,  1.0046e+01, -2.2723e+00, -6.5263e+00,\n        -2.2766e-01, -8.6315e-01, -6.4219e+00, -6.8218e+00, -3.5682e+00,\n        -4.7119e+00, -1.8907e-01,  5.3918e+00,  1.3922e+00, -3.0035e+00,\n         8.9866e+00, -2.6661e+00, -6.6728e+00,  1.1000e+01,  2.2653e+00,\n        -7.9431e+00,  1.8294e+00,  3.0557e+00,  9.4934e+00, -3.0099e+00,\n        -1.1694e+00, -2.3632e+00,  1.2139e+01, -4.4225e-01, -5.4353e+00,\n         6.4613e+00, -5.7222e+00, -8.5285e+00, -4.6763e+00,  4.3144e+00,\n         1.4013e-01, -2.5612e+00,  7.9967e-01,  7.7650e+00, -3.3219e+00,\n         5.2308e+00,  6.5145e-01,  8.7892e-01, -7.4683e+00,  1.0657e+01,\n         4.7196e+00, -1.6202e+00, -1.1049e+01, -3.5860e+00, -1.7874e+00,\n        -4.9695e-01,  2.7570e-01,  1.9470e+00, -2.2283e+00, -6.6657e-01,\n         2.4241e+00,  9.1253e+00,  4.5713e+00,  1.9809e+01, -4.0868e-01,\n        -7.1518e+00, -4.3420e+00, -1.2206e+00, -4.8121e+00, -3.5091e+00,\n         1.4510e+00, -4.8544e+00,  4.0994e+00, -2.3534e+00, -2.9900e+00,\n        -9.6720e+00, -2.9801e+00, -1.8269e+00, -1.0490e+01, -2.8494e+00,\n         3.2904e+00, -3.4714e+00,  6.3171e+00, -6.0031e-01,  6.4215e+00,\n         7.7318e+00, -1.1628e+01, -2.2580e+00,  6.5713e+00, -1.3225e+01,\n         1.8486e+00,  5.2005e-01, -1.1946e+00,  3.4093e-01,  6.6873e+00,\n         3.8414e+00, -6.7605e+00, -3.6807e+00,  6.5493e+00, -7.2647e+00,\n         5.5535e+00, -3.5462e+00,  4.3100e+00, -1.9080e+00,  4.4815e+00,\n         1.4344e+00, -9.2792e+00,  5.1683e-01,  2.5537e+00, -3.7238e+00,\n        -3.5990e+00, -9.6038e-01,  2.6059e+00,  3.2556e+00, -3.9208e+00,\n        -1.1759e+00, -2.2442e+00, -5.0637e+00,  5.1637e+00,  8.1804e+00,\n         6.5776e+00,  6.8021e+00, -7.5181e+00,  1.6102e+00, -9.6251e+00,\n         1.0529e+01,  6.9914e+00,  2.0125e+00,  5.1722e+00, -3.1221e+00,\n         1.7856e+00, -6.8784e+00, -1.0140e+00,  4.2255e+00,  4.5046e+00,\n        -6.5289e+00, -3.6468e+00, -5.1887e-01,  6.8367e+00,  1.8442e+00,\n         1.9149e+00, -9.1361e+00, -2.6483e+00, -4.3092e+00, -3.8572e+00,\n         4.2038e+00, -4.3649e+00,  6.4979e-01,  3.2503e+00,  7.7257e-01,\n        -9.5746e+00, -5.9892e+00,  1.3186e+00,  1.0000e+00],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:32:45.155203Z","iopub.execute_input":"2024-08-07T05:32:45.155589Z","iopub.status.idle":"2024-08-07T05:32:46.142253Z","shell.execute_reply.started":"2024-08-07T05:32:45.155558Z","shell.execute_reply":"2024-08-07T05:32:46.141087Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Training with custom feature loader\n\n# Hyperparameters\nfeature_dim = 2048\nnum_classes = 8\nbatch_size = 128\nnum_epochs = 10\nlearning_rate = 1e-4\nsequence_length = 64\n\n# Model, loss function, optimizer\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to('cuda')\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n# try SGD Optimizer \n \nloss_list = []\n\nimage_features = image_features.cuda()\nlabel_tensor = label_tensor.cuda()\n\n# Training loop\nfor epoch in range(1, num_epochs):\n    model.train()\n    running_loss = 0.0\n    seq_start = 0\n    seq_end = sequence_length\n    progress_bar = tqdm(total = len(labels))\n    while seq_end < label_tensor.shape[0]:\n        optimizer.zero_grad()\n        out = []\n        targets = []\n        for i in range(batch_size):\n            if seq_end >= label_tensor.shape[0]:\n                break\n            inputs = image_features[seq_start:seq_start + sequence_length, : ]\n            target = label_tensor[seq_end]\n#             print('hello',inputs.shape)\n#             print(targets.shape)\n            # Forward pass\n            outputs = model(inputs)\n\n            # Reshape outputs and targets to be compatible with the loss function\n            outputs = outputs.view(-1, num_classes)\n            out.append(outputs)\n            target = target.view(-1)\n            targets.append(target)\n            seq_start += 1\n            seq_end += 1\n            if seq_end % 100 == 0:\n                progress_bar.update(100)\n                    # Calculate loss\n        pred = torch.row_stack(out)\n        truth = torch.tensor(targets).cuda()\n        loss = criterion(pred, truth)\n        # Backward pass and optimize\n#             print(loss)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        loss_list.append(loss.item())\n        # Update progress bar with current loss\n#         data_loader.set_postfix(loss=loss.item())\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n    torch.save(model.state_dict(), f'transformer_on_resnet_50_e{epoch}.pth')\n    progress_bar.close()\n\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:26:21.751251Z","iopub.execute_input":"2024-07-31T15:26:21.752237Z","iopub.status.idle":"2024-07-31T15:26:22.676823Z","shell.execute_reply.started":"2024-07-31T15:26:21.752196Z","shell.execute_reply":"2024-07-31T15:26:22.675248Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae97e605a7b4dfcb46d2886d109f878"}},"metadata":{}},{"name":"stdout","text":"After transformer encoding:  torch.Size([64, 2048])\nTake mean:  torch.Size([64])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m             target \u001b[38;5;241m=\u001b[39m label_tensor[seq_end]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             print('hello',inputs.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#             print(targets.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;66;03m# Reshape outputs and targets to be compatible with the loss function\u001b[39;00m\n\u001b[1;32m     44\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mImageTransformer.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTake mean: \u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x64 and 2048x256)","output_type":"error"}]},{"cell_type":"code","source":"len(labels) - sequence_length","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.121094Z","iopub.status.idle":"2024-07-31T05:24:11.121525Z","shell.execute_reply.started":"2024-07-31T05:24:11.121325Z","shell.execute_reply":"2024-07-31T05:24:11.121342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to('cuda')\nmodel.load_state_dict(torch.load('/kaggle/working/transformer_on_resnet_50_e0.pth'))\nmodel.eval()\nseq_start = 0\nseq_end = sequence_length\nprogress_bar = tqdm(total = len(labels))\npreds = []\nwith torch.no_grad():\n    while seq_end < label_tensor.shape[0]:\n\n        inputs = image_features[seq_start:seq_start + sequence_length, : ].cuda()\n        target = label_tensor[seq_end]\n    #             print('hello',inputs.shape)\n    #             print(targets.shape)\n        # Forward pass\n        outputs = model(inputs)\n        preds.append(torch.argmax(outputs).item())\n        seq_start += 1\n        seq_end += 1\n        progress_bar.update(1)\n\nprint(len(preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-01T05:14:38.69926Z","iopub.execute_input":"2024-08-01T05:14:38.699659Z","iopub.status.idle":"2024-08-01T05:15:28.416199Z","shell.execute_reply.started":"2024-08-01T05:14:38.699629Z","shell.execute_reply":"2024-08-01T05:15:28.415411Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Validation\n# feature_dim = 2048\n# num_classes = 8\n# batch_size = 128\n# num_epochs = 10\n# learning_rate = 1e-4\n# sequence_length = 64\n\nmodel = ImageTransformer(feature_dim=feature_dim, num_classes=num_classes, sequence_length = sequence_length).to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/transformer_on_resnet_50_e9.pth'))\ncriterion = nn.CrossEntropyLoss()\n\nmodel.eval()\n\nall_preds = []\nall_targets = []\nrunning_loss = 0.0\nwith torch.no_grad():\n    progress_bar = tqdm(total=len(data_loader_val), desc='Validation', unit='batch')\n    for features, targets in data_loader_val:\n        features, targets = features.cuda(), targets.cuda() \n        outputs = model(features)\n        # Reshape outputs and targets to be compatible with the loss function\n        outputs = outputs.view(-1, num_classes)\n        # Calculate loss\n        validation_loss = criterion(outputs, targets)\n        running_loss += validation_loss.item() * features.size(0)\n        # Collect predictions and true labels\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(loss=validation_loss.item())\n    \n    progress_bar.close()\n\nvalidation_loss = running_loss / val_length\n\n# Calculate metrics\nall_preds = np.array(all_preds)\nall_targets = np.array(all_targets)\n\nf1 = f1_score(all_targets, all_preds, average='weighted')\nprecision = precision_score(all_targets, all_preds, average='weighted')\nrecall = recall_score(all_targets, all_preds, average='weighted')\naccuracy = accuracy_score(all_targets, all_preds)\n\nprint(f'Validation Loss: {validation_loss:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:26:43.849945Z","iopub.execute_input":"2024-08-05T10:26:43.850324Z","iopub.status.idle":"2024-08-05T10:27:08.892349Z","shell.execute_reply.started":"2024-08-05T10:26:43.850293Z","shell.execute_reply":"2024-08-05T10:27:08.891219Z"},"trusted":true},"execution_count":114,"outputs":[{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/241 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37410ff0c6ad4ed7b04a5a4887fe09f3"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.9743\nF1 Score: 0.1377\nPrecision: 0.0894\nRecall: 0.2991\nAccuracy: 0.2991\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from scipy.ndimage import gaussian_filter1d\nimport numpy as np\n\npreds = torch.tensor(preds)\npreds = preds.to('cpu')\n\nsigma = 1.0  # Standard deviation of the Gaussian kernel\nfiltered_array = gaussian_filter1d(preds, sigma=sigma)\n\n# Discretize the filtered values\ndiscretized_array = np.round(filtered_array).astype(int)  # Round and convert to integers\n\n# Convert the result back to a PyTorch tensor\nfiltered_tensor = torch.tensor(discretized_array, dtype=torch.int)\n\n\nfiltered_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:37.309357Z","iopub.execute_input":"2024-08-05T10:29:37.31004Z","iopub.status.idle":"2024-08-05T10:29:37.320912Z","shell.execute_reply.started":"2024-08-05T10:29:37.31001Z","shell.execute_reply":"2024-08-05T10:29:37.320062Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/172670838.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  preds = torch.tensor(preds)\n","output_type":"stream"},{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"torch.Size([64])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Example ground truth and predictions\ny_true = label_tensor[sequence_length:]  # True labels\ny_pred = preds  # Predicted labels\n\n# Calculate precision, recall, and F1 score for different averaging methods\nprecision_macro = precision_score(y_true, y_pred, average='macro')\nrecall_macro = recall_score(y_true, y_pred, average='macro')\nf1_macro = f1_score(y_true, y_pred, average='macro')\n\nprecision_micro = precision_score(y_true, y_pred, average='micro')\nrecall_micro = recall_score(y_true, y_pred, average='micro')\nf1_micro = f1_score(y_true, y_pred, average='micro')\n\nprecision_weighted = precision_score(y_true, y_pred, average='weighted')\nrecall_weighted = recall_score(y_true, y_pred, average='weighted')\nf1_weighted = f1_score(y_true, y_pred, average='weighted')\n\nprint(f'Precision (macro): {precision_macro:.2f}')\nprint(f'Recall (macro): {recall_macro:.2f}')\nprint(f'F1 Score (macro): {f1_macro:.2f}')\n\nprint(f'Precision (micro): {precision_micro:.2f}')\nprint(f'Recall (micro): {recall_micro:.2f}')\nprint(f'F1 Score (micro): {f1_micro:.2f}')\n\nprint(f'Precision (weighted): {precision_weighted:.2f}')\nprint(f'Recall (weighted): {recall_weighted:.2f}')\nprint(f'F1 Score (weighted): {f1_weighted:.2f}')\n\naccuracy = accuracy_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:29:44.908057Z","iopub.execute_input":"2024-08-05T10:29:44.908952Z","iopub.status.idle":"2024-08-05T10:29:45.274898Z","shell.execute_reply.started":"2024-08-05T10:29:44.9089Z","shell.execute_reply":"2024-08-05T10:29:45.273544Z"},"trusted":true},"execution_count":117,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m preds  \u001b[38;5;66;03m# Predicted labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, and F1 score for different averaging methods\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m precision_macro \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m recall_macro \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     y_true,\n\u001b[1;32m   1827\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15408, 64]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [15408, 64]","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor(float(outputs.argmax().item()))","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.131308Z","iopub.status.idle":"2024-07-31T05:24:11.131763Z","shell.execute_reply.started":"2024-07-31T05:24:11.131523Z","shell.execute_reply":"2024-07-31T05:24:11.131541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/kaggle/input/abaw-7-dataset/cropped_aligned/1-30-1280x720/00001.jpg').convert('RGB')\ntransform = transforms.Compose([\n            transforms.Resize([224, 224])\n        ])\n\ntransform(img)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:24:11.133633Z","iopub.status.idle":"2024-07-31T05:24:11.13411Z","shell.execute_reply.started":"2024-07-31T05:24:11.1339Z","shell.execute_reply":"2024-07-31T05:24:11.133924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}