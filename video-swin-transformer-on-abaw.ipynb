{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8606881,"sourceType":"datasetVersion","datasetId":5122606}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T15:05:58.668324Z","iopub.execute_input":"2024-08-14T15:05:58.668788Z","iopub.status.idle":"2024-08-14T15:06:16.224684Z","shell.execute_reply.started":"2024-08-14T15:05:58.668752Z","shell.execute_reply":"2024-08-14T15:06:16.222928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport gc\nimport numpy as np\nimport pandas as pd\nimport time\nimport copy\nfrom collections import Counter\n\n# from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.init as init\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n\nfrom timm.scheduler.scheduler import Scheduler\nfrom timm.loss import LabelSmoothingCrossEntropy\nimport timm\n\nfrom torchvision import transforms as T\nimport torchvision\n# from transformers import AutoImageProcessor, ResNetModel\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n# from transformers.image_processing_base import BatchFeature","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:47:11.043920Z","iopub.execute_input":"2024-08-21T14:47:11.044266Z","iopub.status.idle":"2024-08-21T14:47:30.253503Z","shell.execute_reply.started":"2024-08-21T14:47:11.044238Z","shell.execute_reply":"2024-08-21T14:47:30.252245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"def data_builder(label_path, train = True):\n    # read annotations\n    df = pd.read_csv(label_path, names = ['image', 'val', 'arousal', 'expr', 'au1', 'au2', 'au3', 'au4', 'au5', 'au6', 'au7', 'au8', 'au9', 'au10', 'au11', 'au12'] )\n    # Remove the first row\n    df = df.drop(index=0)\n\n    # reset index\n    df = df.reset_index(drop=True)\n\n    df[['folder', 'image']] = df['image'].str.split('/', expand=True)\n\n    grouped = df.groupby(['folder'], as_index= False).apply(lambda x: x.sort_values('image'))\n    grouped['expr'] = grouped['expr'].astype(int)\n    grouped = grouped.reset_index(drop=True)\n\n    # generating data dictionary\n    data_dict = {}\n    emo = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness','Surprise', 'Other']\n    seq = 0\n    current = -1\n    for index, row in grouped.iterrows():\n        if row['expr'] != current:\n            current = row['expr']\n            if train:\n                seq += 1\n\n        if row['expr'] == -1:\n            continue\n\n        if row['folder'] in data_dict:\n            data_dict[row['folder']].append(\n                (os.path.join(row['folder'], row['image']), row['expr'])\n            )\n        else:\n            data_dict[row['folder']] = [(os.path.join(row['folder'], row['image']), row['expr']), ]\n            \n    return data_dict\n\ndef most_common_element(lst):\n    if not lst:\n        return None  # Return None if the list is empty\n    counter = Counter(lst)\n    most_common = counter.most_common(1)[0]  # Get the most common element\n    return most_common[0]\n\ndef sequence_extractor(data_dict, data_path, min_stride = 5, sequence_length = 5, train = True):\n    train_seqs = []\n    train_labels = []\n    for folder in data_dict.keys():\n        s_len = len(data_dict[folder])\n        max_start_index = s_len - (sequence_length - 1) * min_stride\n        for i in range(0, max_start_index, sequence_length * min_stride):\n            sublist = [os.path.join(data_path, data_dict[folder][i + j * min_stride][0]) for j in range(5)]\n            train_seqs.append(sublist)\n            t_lab = [data_dict[folder][i + j * min_stride][1] for j in range(5)]\n\n            train_labels.append(most_common_element(t_lab))\n    return train_seqs, train_labels\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:47:33.585186Z","iopub.execute_input":"2024-08-21T14:47:33.585520Z","iopub.status.idle":"2024-08-21T14:47:33.600631Z","shell.execute_reply.started":"2024-08-21T14:47:33.585495Z","shell.execute_reply":"2024-08-21T14:47:33.599729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(data_dict['107']['Other0'])\nval_seq = []\nval_labels = []\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\n\n\ndef most_common_element(lst):\n    if not lst:\n        return None  # Return None if the list is empty\n    counter = Counter(lst)\n    most_common = counter.most_common(1)[0]  # Get the most common element\n    return most_common[0]\n\nstride = 4\nfor key in data_dict.keys():\n    for exp in data_dict[key].keys():\n        n = len(data_dict[key][exp])\n        # Calculate the maximum starting index that allows creating a sublist of length 5\n        max_start_index = n - (5 - 1) * stride\n        for i in range(0, max_start_index, 5 * stride):\n            sublist = [os.path.join(data_path, data_dict[key][exp][i + j * stride][0]) for j in range(5)]\n            val_seq.append(sublist)\n            t_lab = [data_dict[key][exp][i + j * stride][1] for j in range(5)]\n            \n            val_labels.append(most_common_element(t_lab))\n\n\nlen(val_seq)\n# data_dict['107']['Other0'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-19T08:57:08.319205Z","iopub.execute_input":"2024-08-19T08:57:08.319734Z","iopub.status.idle":"2024-08-19T08:57:08.355043Z","shell.execute_reply.started":"2024-08-19T08:57:08.319694Z","shell.execute_reply":"2024-08-19T08:57:08.353873Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ABAWFeatureDataset(Dataset):\n    def __init__(self, features, labels, transform = None):\n        self.features = features\n        self.labels = labels\n        self.length = len(labels)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        frames = [Image.open(i).convert('RGB') for i in self.features[idx]]\n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n        frames = torch.stack(frames)        # Stack frames to form a 4D tensor (T, C, H, W)\n        frames = frames.permute(1, 0, 2, 3)  # Change order to (C, T, H, W)\n        label = self.labels[idx]\n        return frames, label","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:47:35.445506Z","iopub.execute_input":"2024-08-21T14:47:35.446320Z","iopub.status.idle":"2024-08-21T14:47:35.453181Z","shell.execute_reply.started":"2024-08-21T14:47:35.446284Z","shell.execute_reply":"2024-08-21T14:47:35.452261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label_path = '/kaggle/input/abaw-7-dataset/training_set_annotations.txt'\nval_label_path = '/kaggle/input/abaw-7-dataset/validation_set_annotations.txt'\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\n\nbatch_size = 16\nstride = 5\nsequence_length = 10\n\ntrain_seqs, train_labels = sequence_extractor(\n    data_dict = data_builder(train_label_path, train = False),\n    data_path = data_path,\n    min_stride = stride,\n    sequence_length = sequence_length\n)\n\nval_seqs, val_labels = sequence_extractor(\n    data_dict = data_builder(val_label_path, train = False),\n    data_path = data_path,\n    min_stride = stride,\n    sequence_length = sequence_length,\n    train = False\n)\n\ntrain_dataset = ABAWFeatureDataset(train_seqs, train_labels, transform = T.Compose([\n        T.RandomHorizontalFlip(p=0.5),  # Flip the frame with a probability of 0.5\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),  # Converts the image to a tensor and normalizes to [0, 1]\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD),  # Normalization using ImageNet mean and std\n    ])\n)\nval_dataset = ABAWFeatureDataset(val_seqs, val_labels, transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n    ])\n)\n\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle = True)\n\ndataloaders = {\n    \"train\": data_loader_train,\n    \"val\": data_loader_val\n}\n\ndataset_sizes = {\n    \"train\": len(train_dataset),\n    \"val\": len(val_dataset)\n}\n\ndataset_sizes","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:47:40.459597Z","iopub.execute_input":"2024-08-21T14:47:40.459977Z","iopub.status.idle":"2024-08-21T14:47:54.073780Z","shell.execute_reply.started":"2024-08-21T14:47:40.459949Z","shell.execute_reply":"2024-08-21T14:47:54.072863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.video.swin3d_b(weights=\"KINETICS400_IMAGENET22K_V1\")\n\n# model = torchvision.models.video.swin3d_b(weights=None)\n\n# state_dict = torch.load('/kaggle/working/trained_model/freezing_epoch_10.pth', map_location=torch.device('cpu'))\n\n# Step 3: Apply the loaded weights to the model\n# model.load_state_dict(state_dict)\n\nnum_ftrs = model.head.in_features # Get the input features of the current head\n\n# Step 2: Create a new head\nnew_head = nn.Sequential(\n nn.Linear(num_ftrs, 8) # Final layer with 3 output units (for 3 classes)\n)\n\nmodel.head = new_head\n\n# Freeze all layers first\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last few layers\nfor name, param in model.named_parameters():\n    if any(layer_name in name for layer_name in [\"features.6\", \"norm\", \"avgpool\", \"head\"]):\n        param.requires_grad = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)\n\n\nrun = 7\n\nlabels_np = np.array(train_labels)\nclass_counts = np.bincount(labels_np)          # Generate class counts\nclass_weights = 1.0 / class_counts             # Calculate inverse of class counts\nclass_weights = class_weights / class_weights.sum()            # Normalize the weights to sum to 1 (optional)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n# criterion = LabelSmoothingCrossEntropy()\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\ncriterion = criterion.to(device)\noptimizer = optim.AdamW(model.head.parameters(), lr=0.001)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)\nwriter = SummaryWriter(f'run{run}')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:47:56.756679Z","iopub.execute_input":"2024-08-21T14:47:56.757807Z","iopub.status.idle":"2024-08-21T14:48:01.819520Z","shell.execute_reply.started":"2024-08-21T14:47:56.757764Z","shell.execute_reply":"2024-08-21T14:48:01.818538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T14:48:36.390541Z","iopub.execute_input":"2024-08-17T14:48:36.390925Z","iopub.status.idle":"2024-08-17T14:48:36.399794Z","shell.execute_reply.started":"2024-08-17T14:48:36.390894Z","shell.execute_reply":"2024-08-17T14:48:36.398874Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device, writer, run, num_epochs=20):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            all_preds = []\n            all_labels = []\n            \n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n            \n            if phase == 'train' and (epoch+1) % 5 == 0:\n                scheduler.step()\n                torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase6_freezing_epoch_{epoch}.pth')\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            # Calculate F1 score, precision, and recall\n            epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n            epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n            \n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            \n            # Log the metrics\n            writer.add_scalar(f'{phase}{run}/Loss', epoch_loss, epoch)\n            writer.add_scalar(f'{phase}{run}/Accuracy', epoch_acc, epoch)\n            writer.add_scalar(f'{phase}{run}/F1', epoch_f1, epoch)\n            writer.add_scalar(f'{phase}{run}/Precision', epoch_precision, epoch)\n            writer.add_scalar(f'{phase}{run}/Recall', epoch_recall, epoch)\n            \n            # Log the learning rate as a scalar\n            current_lr = optimizer.param_groups[0]['lr']\n            writer.add_scalar(f'{phase}6/Learning_Rate', current_lr, epoch)\n        \n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    torch.save(model.state_dict(), f'/kaggle/working/trained_model/phase6{run}freezing_final_epoch_{num_epochs}.pth')  # Save the model\n    best_model_wts = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:48:05.113650Z","iopub.execute_input":"2024-08-21T14:48:05.114028Z","iopub.status.idle":"2024-08-21T14:48:05.130009Z","shell.execute_reply.started":"2024-08-21T14:48:05.113996Z","shell.execute_reply":"2024-08-21T14:48:05.129043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Increase the message rate limit for printing training progress\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager().update('notebook', {\n    \"NotebookApp\": {\n        \"iopub_msg_rate_limit\": 10000,  # Increase to 10000 messages/sec\n        \"rate_limit_window\": 10.0,      # Increase the rate limit window to 10 seconds\n    }\n})","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:48:07.064429Z","iopub.execute_input":"2024-08-21T14:48:07.064796Z","iopub.status.idle":"2024-08-21T14:48:07.081941Z","shell.execute_reply.started":"2024-08-21T14:48:07.064767Z","shell.execute_reply":"2024-08-21T14:48:07.081206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\n\ndescription = (\n    f\"Training parameters:\\n\"\n    f\"Model: {model.__class__.__name__}\\n\"\n    f\"Criterion: {criterion.__class__.__name__}\\n\"\n    f\"Optimizer: {optimizer.__class__.__name__}\\n\"\n    f\"Scheduler: {exp_lr_scheduler.__class__.__name__}\\n\"\n    f\"Device: {device}\\n\"\n    f\"Number of epochs: {num_epochs}\\n\"\n    f\"Batch Size: {batch_size}\\n\"\n    f\"Stride: {stride}\\n\"\n    f\"Sequence Length: {sequence_length}\"\n)\nwriter.add_text(\"Training Parameters\", description, global_step=0)\nmodel_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders, dataset_sizes, device, writer = writer, run = run, num_epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:48:09.621457Z","iopub.execute_input":"2024-08-21T14:48:09.621909Z","iopub.status.idle":"2024-08-21T15:31:06.251411Z","shell.execute_reply.started":"2024-08-21T14:48:09.621874Z","shell.execute_reply":"2024-08-21T15:31:06.250388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Independent Validation","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:46:00.378681Z","iopub.execute_input":"2024-08-21T14:46:00.379354Z","iopub.status.idle":"2024-08-21T14:46:00.885329Z","shell.execute_reply.started":"2024-08-21T14:46:00.379320Z","shell.execute_reply":"2024-08-21T14:46:00.884209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_seqs, val_labels = sequence_extractor(\n    data_dict = data_builder(val_label_path, train = False),\n    data_path = data_path,\n    min_stride = 1,\n    sequence_length = 5,\n    train = False\n)\n\nval_dataset = ABAWFeatureDataset(val_seqs, val_labels, transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n    ])\n)\n\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle = True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:05.277394Z","iopub.execute_input":"2024-08-19T09:28:05.277779Z","iopub.status.idle":"2024-08-19T09:28:07.581753Z","shell.execute_reply.started":"2024-08-19T09:28:05.277749Z","shell.execute_reply":"2024-08-19T09:28:07.580948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"running_loss = 0.0\nrunning_corrects = 0.0\nall_preds = []\nall_labels = []\n            \n    \nfor inputs, labels in tqdm(data_loader_val):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n#     optimizer.zero_grad()\n\n    with torch.no_grad():\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n\n    running_loss += loss.item() * inputs.size(0)\n    running_corrects += torch.sum(preds == labels.data)\n\n    all_preds.extend(preds.cpu().numpy())\n    all_labels.extend(labels.cpu().numpy())\n\nepoch_loss = running_loss / dataset_sizes['val']\nepoch_acc = running_corrects.double() / dataset_sizes['val']\n\n# Calculate F1 score, precision, and recall\nepoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\nepoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n\nprint(f\"{'val'} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f}\")\n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:28:11.406099Z","iopub.execute_input":"2024-08-19T09:28:11.406430Z","iopub.status.idle":"2024-08-19T09:30:41.886717Z","shell.execute_reply.started":"2024-08-19T09:28:11.406405Z","shell.execute_reply":"2024-08-19T09:30:41.885835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Example list of image paths\ndata_path = '/kaggle/input/abaw-7-dataset/cropped_aligned'\nimage_paths = [os.path.join(data_path, x) for x in train_seqs[0]]\nlabels = [train_labels[0] for x in train_seqs[0]]\n# Number of images\nn_images = len(image_paths)\n\n# Calculate grid size (assuming a square grid)\ngrid_size = int(n_images**0.5) + (n_images**0.5 != int(n_images**0.5))\n\n# Create a figure with a grid of subplots\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n\n# Flatten the axes array for easy iteration\naxes = axes.flatten()\n\n# Iterate over images and plot them with labels\nfor i, (image_path, label) in enumerate(zip(image_paths, labels)):\n    img = mpimg.imread(image_path)\n    axes[i].imshow(img)\n    axes[i].axis('off')  # Hide axes\n    axes[i].set_title(label, fontsize=12)  # Add label\n\n# Hide any remaining empty subplots\nfor j in range(i+1, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:12:32.517175Z","iopub.execute_input":"2024-08-14T10:12:32.518098Z","iopub.status.idle":"2024-08-14T10:12:33.938161Z","shell.execute_reply.started":"2024-08-14T10:12:32.518062Z","shell.execute_reply":"2024-08-14T10:12:33.937147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:12:09.990894Z","iopub.execute_input":"2024-08-14T10:12:09.991652Z","iopub.status.idle":"2024-08-14T10:12:09.998000Z","shell.execute_reply.started":"2024-08-14T10:12:09.991618Z","shell.execute_reply":"2024-08-14T10:12:09.996809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build a dynamic FER model which takes video ","metadata":{}}]}